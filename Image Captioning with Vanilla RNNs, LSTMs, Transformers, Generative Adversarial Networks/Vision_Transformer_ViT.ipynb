{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b7ee23c",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "\n",
    "Prepared by comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a910e",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3cdc4fd",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6152858f",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840763c9",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d502cffe",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787434ed",
   "metadata": {},
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "217f44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        #32,64,4\n",
    "\n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_]) # shape 64-4-16-64\n",
    "       \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ## Compute attention logits. Note that this operation is conducted as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2,-1))#/torch.sqrt(torch.tensor(self.head_dims, dtype=torch.float)) # shape 64-4-16-64\n",
    "        scaled_attention_scores = attention_scores / torch.sqrt(torch.tensor(self.head_dims,dtype=torch.float))\n",
    "        #print(attention_scores.shape)\n",
    "        #print(scaled_attention_scores.shape)\n",
    "\n",
    "        ## Compute attention Weights. Note that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "\n",
    "        attention_weights = torch.softmax(scaled_attention_scores, dim=-1)\n",
    "        #print(attention_weights.shape)\n",
    "\n",
    "        ## Compute output values. Note that this operation is conducted as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "\n",
    "        #print(attention_weights.shape) # [B, H, N, N]\n",
    "        #print(v.shape) # [B, H, N, D//H]\n",
    "        output = torch.matmul(attention_weights, v) \n",
    "        #print(output.shape) # [B, H, N, D//H]\n",
    "        output = output.permute(0,2,1,3) # [B, N, H, D//H]\n",
    "        #print(output.shape)\n",
    "        attn_out = output.reshape(output.shape[0], output.shape[1], -1) # [B,N,D]\n",
    "        #print(attn_out.shape)\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "    \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039f68f",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "edf1a8a8",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
    "    layer = SelfAttention(32,64,4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c2a5a",
   "metadata": {},
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b9ef2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.elu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8a7409ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "## input --> layernorm --> SelfAttention --> skip connection \n",
    "##       --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "    ###############################################################\n",
    "    # TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "    ###############################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dims)\n",
    "        self.mlp = MLP(hidden_dims,hidden_dims,hidden_dims,bias)\n",
    "        self.self_attention = SelfAttention(hidden_dims, head_dims=hidden_dims//num_heads, num_heads=num_heads, bias=bias)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###################################################################\n",
    "    #                                 END OF YOUR CODE                #             \n",
    "    ###################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "    ##############################################################\n",
    "    # TODO: Complete the forward of TransformerBlock module      #\n",
    "    ##############################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        x_norm_1= self.layer_norm(x)\n",
    "        x_attention = self.self_attention(x_norm_1)\n",
    "        x_skip = x + x_attention\n",
    "        x_norm_2 = self.layer_norm(x_skip)\n",
    "        x_mlp = self.mlp(x_norm_2)\n",
    "        x_out = x_mlp + x_skip\n",
    "        return x_out\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ###################################################################\n",
    "    #                                 END OF YOUR CODE                #             \n",
    "    ###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74208d7",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "648ab1d4",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,128]\n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c1132",
   "metadata": {},
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aca38dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4cdab",
   "metadata": {},
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2ec9176e",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size())  # you should see [64,10]\n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833736a1",
   "metadata": {},
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33f7f1",
   "metadata": {},
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c50f0b22",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, np.float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6403f",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c758b6b0",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7414b",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "23c264e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea5a24",
   "metadata": {},
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eb34b985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc187ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 4.822 | Acc: 8.000% (2/25)\n",
      "Loss: 7.069 | Acc: 8.000% (4/50)\n",
      "Loss: 6.719 | Acc: 10.667% (8/75)\n",
      "Loss: 6.840 | Acc: 11.000% (11/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 11.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 5.843 | Acc: 20.000% (5/25)\n",
      "Loss: 5.997 | Acc: 14.000% (7/50)\n",
      "Loss: 6.049 | Acc: 10.667% (8/75)\n",
      "Loss: 5.966 | Acc: 9.000% (9/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 9.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 6.394 | Acc: 8.000% (2/25)\n",
      "Loss: 5.012 | Acc: 8.000% (4/50)\n",
      "Loss: 4.464 | Acc: 9.333% (7/75)\n",
      "Loss: 4.032 | Acc: 12.000% (12/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 12.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.476 | Acc: 12.000% (3/25)\n",
      "Loss: 3.399 | Acc: 22.000% (11/50)\n",
      "Loss: 3.317 | Acc: 20.000% (15/75)\n",
      "Loss: 3.332 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 2.863 | Acc: 24.000% (6/25)\n",
      "Loss: 2.704 | Acc: 28.000% (14/50)\n",
      "Loss: 2.572 | Acc: 22.667% (17/75)\n",
      "Loss: 2.646 | Acc: 18.000% (18/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 18.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.645 | Acc: 28.000% (7/25)\n",
      "Loss: 2.922 | Acc: 22.000% (11/50)\n",
      "Loss: 2.763 | Acc: 21.333% (16/75)\n",
      "Loss: 2.825 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 2.088 | Acc: 36.000% (9/25)\n",
      "Loss: 2.195 | Acc: 32.000% (16/50)\n",
      "Loss: 2.093 | Acc: 34.667% (26/75)\n",
      "Loss: 2.011 | Acc: 35.000% (35/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 35.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.280 | Acc: 28.000% (7/25)\n",
      "Loss: 2.270 | Acc: 18.000% (9/50)\n",
      "Loss: 2.252 | Acc: 17.333% (13/75)\n",
      "Loss: 2.269 | Acc: 17.000% (17/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 17.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1.738 | Acc: 36.000% (9/25)\n",
      "Loss: 1.736 | Acc: 40.000% (20/50)\n",
      "Loss: 1.767 | Acc: 40.000% (30/75)\n",
      "Loss: 1.739 | Acc: 40.000% (40/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 40.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.427 | Acc: 20.000% (5/25)\n",
      "Loss: 2.402 | Acc: 18.000% (9/50)\n",
      "Loss: 2.272 | Acc: 24.000% (18/75)\n",
      "Loss: 2.248 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 1.600 | Acc: 40.000% (10/25)\n",
      "Loss: 1.430 | Acc: 50.000% (25/50)\n",
      "Loss: 1.430 | Acc: 48.000% (36/75)\n",
      "Loss: 1.441 | Acc: 47.000% (47/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 47.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.572 | Acc: 24.000% (6/25)\n",
      "Loss: 2.519 | Acc: 24.000% (12/50)\n",
      "Loss: 2.320 | Acc: 25.333% (19/75)\n",
      "Loss: 2.218 | Acc: 28.000% (28/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 28.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.264 | Acc: 44.000% (11/25)\n",
      "Loss: 1.289 | Acc: 44.000% (22/50)\n",
      "Loss: 1.175 | Acc: 52.000% (39/75)\n",
      "Loss: 1.188 | Acc: 52.000% (52/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 52.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.400 | Acc: 12.000% (3/25)\n",
      "Loss: 2.240 | Acc: 12.000% (6/50)\n",
      "Loss: 2.108 | Acc: 20.000% (15/75)\n",
      "Loss: 2.104 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 0.833 | Acc: 72.000% (18/25)\n",
      "Loss: 0.957 | Acc: 70.000% (35/50)\n",
      "Loss: 1.014 | Acc: 66.667% (50/75)\n",
      "Loss: 0.948 | Acc: 67.000% (67/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 67.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.464 | Acc: 20.000% (5/25)\n",
      "Loss: 2.215 | Acc: 26.000% (13/50)\n",
      "Loss: 2.150 | Acc: 28.000% (21/75)\n",
      "Loss: 2.058 | Acc: 31.000% (31/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 31.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 0.734 | Acc: 80.000% (20/25)\n",
      "Loss: 0.701 | Acc: 84.000% (42/50)\n",
      "Loss: 0.710 | Acc: 81.333% (61/75)\n",
      "Loss: 0.681 | Acc: 82.000% (82/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 82.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.541 | Acc: 16.000% (4/25)\n",
      "Loss: 2.310 | Acc: 18.000% (9/50)\n",
      "Loss: 2.271 | Acc: 20.000% (15/75)\n",
      "Loss: 2.325 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 0.494 | Acc: 80.000% (20/25)\n",
      "Loss: 0.497 | Acc: 86.000% (43/50)\n",
      "Loss: 0.474 | Acc: 89.333% (67/75)\n",
      "Loss: 0.467 | Acc: 88.000% (88/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 88.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.589 | Acc: 24.000% (6/25)\n",
      "Loss: 2.423 | Acc: 22.000% (11/50)\n",
      "Loss: 2.264 | Acc: 25.333% (19/75)\n",
      "Loss: 2.180 | Acc: 27.000% (27/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 27.0\n",
      "\n",
      "Final train set accuracy is 88.0\n",
      "Final val set accuracy is 27.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims=hidden_dims, input_dims=input_dims, output_dims=output_dims, num_trans_layers = num_trans_layers, num_heads=num_heads, image_k=image_k, patch_k=patch_k)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(10):\n",
    "    tr_acc = train(network, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(network, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588330f1",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2ee3dabf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 4.197 | Acc: 9.375% (6/64)\n",
      "Loss: 4.616 | Acc: 12.500% (16/128)\n",
      "Loss: 5.525 | Acc: 11.979% (23/192)\n",
      "Loss: 5.231 | Acc: 12.500% (32/256)\n",
      "Loss: 5.249 | Acc: 13.438% (43/320)\n",
      "Loss: 5.137 | Acc: 13.542% (52/384)\n",
      "Loss: 4.971 | Acc: 13.170% (59/448)\n",
      "Loss: 4.838 | Acc: 12.891% (66/512)\n",
      "Loss: 4.627 | Acc: 13.715% (79/576)\n",
      "Loss: 4.455 | Acc: 14.688% (94/640)\n",
      "Loss: 4.379 | Acc: 14.062% (99/704)\n",
      "Loss: 4.232 | Acc: 14.583% (112/768)\n",
      "Loss: 4.142 | Acc: 14.423% (120/832)\n",
      "Loss: 4.025 | Acc: 15.513% (139/896)\n",
      "Loss: 3.922 | Acc: 15.208% (146/960)\n",
      "Loss: 3.821 | Acc: 15.137% (155/1024)\n",
      "Loss: 3.725 | Acc: 15.717% (171/1088)\n",
      "Loss: 3.651 | Acc: 15.799% (182/1152)\n",
      "Loss: 3.576 | Acc: 16.201% (197/1216)\n",
      "Loss: 3.502 | Acc: 16.719% (214/1280)\n",
      "Loss: 3.430 | Acc: 17.708% (238/1344)\n",
      "Loss: 3.393 | Acc: 17.401% (245/1408)\n",
      "Loss: 3.348 | Acc: 17.527% (258/1472)\n",
      "Loss: 3.307 | Acc: 17.708% (272/1536)\n",
      "Loss: 3.263 | Acc: 18.000% (288/1600)\n",
      "Loss: 3.213 | Acc: 18.029% (300/1664)\n",
      "Loss: 3.173 | Acc: 18.171% (314/1728)\n",
      "Loss: 3.144 | Acc: 18.192% (326/1792)\n",
      "Loss: 3.101 | Acc: 18.912% (351/1856)\n",
      "Loss: 3.067 | Acc: 19.115% (367/1920)\n",
      "Loss: 3.038 | Acc: 19.204% (381/1984)\n",
      "Loss: 3.007 | Acc: 19.531% (400/2048)\n",
      "Loss: 2.978 | Acc: 19.697% (416/2112)\n",
      "Loss: 2.950 | Acc: 19.853% (432/2176)\n",
      "Loss: 2.924 | Acc: 20.045% (449/2240)\n",
      "Loss: 2.898 | Acc: 20.269% (467/2304)\n",
      "Loss: 2.869 | Acc: 20.608% (488/2368)\n",
      "Loss: 2.845 | Acc: 20.970% (510/2432)\n",
      "Loss: 2.820 | Acc: 21.194% (529/2496)\n",
      "Loss: 2.804 | Acc: 21.055% (539/2560)\n",
      "Loss: 2.791 | Acc: 20.922% (549/2624)\n",
      "Loss: 2.773 | Acc: 20.982% (564/2688)\n",
      "Loss: 2.756 | Acc: 21.112% (581/2752)\n",
      "Loss: 2.747 | Acc: 21.023% (592/2816)\n",
      "Loss: 2.735 | Acc: 20.972% (604/2880)\n",
      "Loss: 2.723 | Acc: 20.958% (617/2944)\n",
      "Loss: 2.708 | Acc: 21.077% (634/3008)\n",
      "Loss: 2.695 | Acc: 21.094% (648/3072)\n",
      "Loss: 2.680 | Acc: 21.110% (662/3136)\n",
      "Loss: 2.666 | Acc: 21.406% (685/3200)\n",
      "Loss: 2.655 | Acc: 21.446% (700/3264)\n",
      "Loss: 2.647 | Acc: 21.454% (714/3328)\n",
      "Loss: 2.635 | Acc: 21.492% (729/3392)\n",
      "Loss: 2.622 | Acc: 21.615% (747/3456)\n",
      "Loss: 2.611 | Acc: 21.761% (766/3520)\n",
      "Loss: 2.603 | Acc: 21.735% (779/3584)\n",
      "Loss: 2.596 | Acc: 21.820% (796/3648)\n",
      "Loss: 2.585 | Acc: 21.929% (814/3712)\n",
      "Loss: 2.575 | Acc: 21.981% (830/3776)\n",
      "Loss: 2.564 | Acc: 22.109% (849/3840)\n",
      "Loss: 2.554 | Acc: 22.208% (867/3904)\n",
      "Loss: 2.547 | Acc: 22.329% (886/3968)\n",
      "Loss: 2.536 | Acc: 22.297% (899/4032)\n",
      "Loss: 2.530 | Acc: 22.217% (910/4096)\n",
      "Loss: 2.527 | Acc: 22.043% (917/4160)\n",
      "Loss: 2.521 | Acc: 22.017% (930/4224)\n",
      "Loss: 2.515 | Acc: 22.155% (950/4288)\n",
      "Loss: 2.507 | Acc: 22.151% (964/4352)\n",
      "Loss: 2.497 | Acc: 22.328% (986/4416)\n",
      "Loss: 2.488 | Acc: 22.388% (1003/4480)\n",
      "Loss: 2.481 | Acc: 22.469% (1021/4544)\n",
      "Loss: 2.473 | Acc: 22.526% (1038/4608)\n",
      "Loss: 2.465 | Acc: 22.688% (1060/4672)\n",
      "Loss: 2.459 | Acc: 22.741% (1077/4736)\n",
      "Loss: 2.450 | Acc: 22.896% (1099/4800)\n",
      "Loss: 2.444 | Acc: 23.006% (1119/4864)\n",
      "Loss: 2.438 | Acc: 23.052% (1136/4928)\n",
      "Loss: 2.430 | Acc: 23.297% (1163/4992)\n",
      "Loss: 2.423 | Acc: 23.497% (1188/5056)\n",
      "Loss: 2.416 | Acc: 23.613% (1209/5120)\n",
      "Loss: 2.412 | Acc: 23.534% (1220/5184)\n",
      "Loss: 2.404 | Acc: 23.761% (1247/5248)\n",
      "Loss: 2.397 | Acc: 23.814% (1265/5312)\n",
      "Loss: 2.391 | Acc: 23.903% (1285/5376)\n",
      "Loss: 2.386 | Acc: 23.897% (1300/5440)\n",
      "Loss: 2.379 | Acc: 23.983% (1320/5504)\n",
      "Loss: 2.374 | Acc: 24.048% (1339/5568)\n",
      "Loss: 2.369 | Acc: 24.165% (1361/5632)\n",
      "Loss: 2.365 | Acc: 24.140% (1375/5696)\n",
      "Loss: 2.363 | Acc: 24.167% (1392/5760)\n",
      "Loss: 2.357 | Acc: 24.279% (1414/5824)\n",
      "Loss: 2.355 | Acc: 24.253% (1428/5888)\n",
      "Loss: 2.350 | Acc: 24.328% (1448/5952)\n",
      "Loss: 2.345 | Acc: 24.335% (1464/6016)\n",
      "Loss: 2.340 | Acc: 24.375% (1482/6080)\n",
      "Loss: 2.336 | Acc: 24.495% (1505/6144)\n",
      "Loss: 2.333 | Acc: 24.468% (1519/6208)\n",
      "Loss: 2.328 | Acc: 24.570% (1541/6272)\n",
      "Loss: 2.325 | Acc: 24.621% (1560/6336)\n",
      "Loss: 2.321 | Acc: 24.594% (1574/6400)\n",
      "Loss: 2.316 | Acc: 24.675% (1595/6464)\n",
      "Loss: 2.312 | Acc: 24.724% (1614/6528)\n",
      "Loss: 2.308 | Acc: 24.788% (1634/6592)\n",
      "Loss: 2.302 | Acc: 24.940% (1660/6656)\n",
      "Loss: 2.300 | Acc: 24.911% (1674/6720)\n",
      "Loss: 2.296 | Acc: 24.941% (1692/6784)\n",
      "Loss: 2.293 | Acc: 25.088% (1718/6848)\n",
      "Loss: 2.289 | Acc: 25.174% (1740/6912)\n",
      "Loss: 2.285 | Acc: 25.172% (1756/6976)\n",
      "Loss: 2.281 | Acc: 25.241% (1777/7040)\n",
      "Loss: 2.277 | Acc: 25.338% (1800/7104)\n",
      "Loss: 2.274 | Acc: 25.405% (1821/7168)\n",
      "Loss: 2.271 | Acc: 25.484% (1843/7232)\n",
      "Loss: 2.267 | Acc: 25.576% (1866/7296)\n",
      "Loss: 2.262 | Acc: 25.666% (1889/7360)\n",
      "Loss: 2.258 | Acc: 25.700% (1908/7424)\n",
      "Loss: 2.255 | Acc: 25.721% (1926/7488)\n",
      "Loss: 2.251 | Acc: 25.728% (1943/7552)\n",
      "Loss: 2.247 | Acc: 25.814% (1966/7616)\n",
      "Loss: 2.244 | Acc: 25.833% (1984/7680)\n",
      "Loss: 2.241 | Acc: 25.826% (2000/7744)\n",
      "Loss: 2.238 | Acc: 25.820% (2016/7808)\n",
      "Loss: 2.234 | Acc: 25.902% (2039/7872)\n",
      "Loss: 2.231 | Acc: 25.983% (2062/7936)\n",
      "Loss: 2.229 | Acc: 26.038% (2083/8000)\n",
      "Loss: 2.225 | Acc: 26.104% (2105/8064)\n",
      "Loss: 2.222 | Acc: 26.132% (2124/8128)\n",
      "Loss: 2.218 | Acc: 26.233% (2149/8192)\n",
      "Loss: 2.215 | Acc: 26.296% (2171/8256)\n",
      "Loss: 2.211 | Acc: 26.418% (2198/8320)\n",
      "Loss: 2.207 | Acc: 26.527% (2224/8384)\n",
      "Loss: 2.201 | Acc: 26.705% (2256/8448)\n",
      "Loss: 2.196 | Acc: 26.903% (2290/8512)\n",
      "Loss: 2.192 | Acc: 27.006% (2316/8576)\n",
      "Loss: 2.187 | Acc: 27.164% (2347/8640)\n",
      "Loss: 2.183 | Acc: 27.229% (2370/8704)\n",
      "Loss: 2.180 | Acc: 27.361% (2399/8768)\n",
      "Loss: 2.176 | Acc: 27.457% (2425/8832)\n",
      "Loss: 2.176 | Acc: 27.417% (2439/8896)\n",
      "Loss: 2.175 | Acc: 27.422% (2457/8960)\n",
      "Loss: 2.174 | Acc: 27.349% (2468/9024)\n",
      "Loss: 2.172 | Acc: 27.388% (2489/9088)\n",
      "Loss: 2.168 | Acc: 27.524% (2519/9152)\n",
      "Loss: 2.166 | Acc: 27.528% (2537/9216)\n",
      "Loss: 2.164 | Acc: 27.619% (2563/9280)\n",
      "Loss: 2.160 | Acc: 27.718% (2590/9344)\n",
      "Loss: 2.158 | Acc: 27.742% (2610/9408)\n",
      "Loss: 2.156 | Acc: 27.787% (2632/9472)\n",
      "Loss: 2.155 | Acc: 27.768% (2648/9536)\n",
      "Loss: 2.152 | Acc: 27.833% (2672/9600)\n",
      "Loss: 2.148 | Acc: 27.897% (2696/9664)\n",
      "Loss: 2.145 | Acc: 27.991% (2723/9728)\n",
      "Loss: 2.143 | Acc: 28.043% (2746/9792)\n",
      "Loss: 2.142 | Acc: 28.044% (2764/9856)\n",
      "Loss: 2.140 | Acc: 28.105% (2788/9920)\n",
      "Loss: 2.138 | Acc: 28.105% (2806/9984)\n",
      "Loss: 2.135 | Acc: 28.175% (2831/10048)\n",
      "Loss: 2.132 | Acc: 28.263% (2858/10112)\n",
      "Loss: 2.129 | Acc: 28.390% (2889/10176)\n",
      "Loss: 2.129 | Acc: 28.320% (2900/10240)\n",
      "Loss: 2.126 | Acc: 28.406% (2927/10304)\n",
      "Loss: 2.123 | Acc: 28.463% (2951/10368)\n",
      "Loss: 2.122 | Acc: 28.470% (2970/10432)\n",
      "Loss: 2.120 | Acc: 28.544% (2996/10496)\n",
      "Loss: 2.118 | Acc: 28.580% (3018/10560)\n",
      "Loss: 2.115 | Acc: 28.709% (3050/10624)\n",
      "Loss: 2.112 | Acc: 28.761% (3074/10688)\n",
      "Loss: 2.108 | Acc: 28.841% (3101/10752)\n",
      "Loss: 2.105 | Acc: 28.957% (3132/10816)\n",
      "Loss: 2.104 | Acc: 28.961% (3151/10880)\n",
      "Loss: 2.102 | Acc: 28.938% (3167/10944)\n",
      "Loss: 2.100 | Acc: 29.006% (3193/11008)\n",
      "Loss: 2.097 | Acc: 29.037% (3215/11072)\n",
      "Loss: 2.096 | Acc: 29.104% (3241/11136)\n",
      "Loss: 2.093 | Acc: 29.161% (3266/11200)\n",
      "Loss: 2.092 | Acc: 29.208% (3290/11264)\n",
      "Loss: 2.089 | Acc: 29.255% (3314/11328)\n",
      "Loss: 2.087 | Acc: 29.301% (3338/11392)\n",
      "Loss: 2.084 | Acc: 29.338% (3361/11456)\n",
      "Loss: 2.083 | Acc: 29.366% (3383/11520)\n",
      "Loss: 2.081 | Acc: 29.385% (3404/11584)\n",
      "Loss: 2.080 | Acc: 29.404% (3425/11648)\n",
      "Loss: 2.079 | Acc: 29.389% (3442/11712)\n",
      "Loss: 2.078 | Acc: 29.390% (3461/11776)\n",
      "Loss: 2.076 | Acc: 29.426% (3484/11840)\n",
      "Loss: 2.075 | Acc: 29.393% (3499/11904)\n",
      "Loss: 2.074 | Acc: 29.437% (3523/11968)\n",
      "Loss: 2.073 | Acc: 29.413% (3539/12032)\n",
      "Loss: 2.071 | Acc: 29.506% (3569/12096)\n",
      "Loss: 2.069 | Acc: 29.515% (3589/12160)\n",
      "Loss: 2.067 | Acc: 29.565% (3614/12224)\n",
      "Loss: 2.065 | Acc: 29.647% (3643/12288)\n",
      "Loss: 2.063 | Acc: 29.663% (3664/12352)\n",
      "Loss: 2.061 | Acc: 29.696% (3687/12416)\n",
      "Loss: 2.060 | Acc: 29.712% (3708/12480)\n",
      "Loss: 2.058 | Acc: 29.711% (3727/12544)\n",
      "Loss: 2.057 | Acc: 29.751% (3751/12608)\n",
      "Loss: 2.055 | Acc: 29.806% (3777/12672)\n",
      "Loss: 2.053 | Acc: 29.845% (3801/12736)\n",
      "Loss: 2.051 | Acc: 29.891% (3826/12800)\n",
      "Loss: 2.049 | Acc: 29.928% (3850/12864)\n",
      "Loss: 2.048 | Acc: 29.935% (3870/12928)\n",
      "Loss: 2.046 | Acc: 29.995% (3897/12992)\n",
      "Loss: 2.044 | Acc: 30.032% (3921/13056)\n",
      "Loss: 2.042 | Acc: 30.084% (3947/13120)\n",
      "Loss: 2.040 | Acc: 30.135% (3973/13184)\n",
      "Loss: 2.039 | Acc: 30.208% (4002/13248)\n",
      "Loss: 2.037 | Acc: 30.258% (4028/13312)\n",
      "Loss: 2.035 | Acc: 30.308% (4054/13376)\n",
      "Loss: 2.034 | Acc: 30.320% (4075/13440)\n",
      "Loss: 2.033 | Acc: 30.339% (4097/13504)\n",
      "Loss: 2.033 | Acc: 30.351% (4118/13568)\n",
      "Loss: 2.032 | Acc: 30.362% (4139/13632)\n",
      "Loss: 2.030 | Acc: 30.410% (4165/13696)\n",
      "Loss: 2.028 | Acc: 30.451% (4190/13760)\n",
      "Loss: 2.027 | Acc: 30.454% (4210/13824)\n",
      "Loss: 2.026 | Acc: 30.523% (4239/13888)\n",
      "Loss: 2.024 | Acc: 30.540% (4261/13952)\n",
      "Loss: 2.022 | Acc: 30.601% (4289/14016)\n",
      "Loss: 2.022 | Acc: 30.625% (4312/14080)\n",
      "Loss: 2.021 | Acc: 30.635% (4333/14144)\n",
      "Loss: 2.019 | Acc: 30.701% (4362/14208)\n",
      "Loss: 2.018 | Acc: 30.760% (4390/14272)\n",
      "Loss: 2.017 | Acc: 30.783% (4413/14336)\n",
      "Loss: 2.017 | Acc: 30.771% (4431/14400)\n",
      "Loss: 2.014 | Acc: 30.821% (4458/14464)\n",
      "Loss: 2.013 | Acc: 30.851% (4482/14528)\n",
      "Loss: 2.013 | Acc: 30.832% (4499/14592)\n",
      "Loss: 2.011 | Acc: 30.868% (4524/14656)\n",
      "Loss: 2.010 | Acc: 30.890% (4547/14720)\n",
      "Loss: 2.010 | Acc: 30.892% (4567/14784)\n",
      "Loss: 2.008 | Acc: 30.920% (4591/14848)\n",
      "Loss: 2.006 | Acc: 30.948% (4615/14912)\n",
      "Loss: 2.006 | Acc: 30.983% (4640/14976)\n",
      "Loss: 2.005 | Acc: 30.991% (4661/15040)\n",
      "Loss: 2.004 | Acc: 31.038% (4688/15104)\n",
      "Loss: 2.002 | Acc: 31.079% (4714/15168)\n",
      "Loss: 2.000 | Acc: 31.152% (4745/15232)\n",
      "Loss: 1.999 | Acc: 31.152% (4765/15296)\n",
      "Loss: 1.998 | Acc: 31.185% (4790/15360)\n",
      "Loss: 1.995 | Acc: 31.276% (4824/15424)\n",
      "Loss: 1.994 | Acc: 31.295% (4847/15488)\n",
      "Loss: 1.994 | Acc: 31.295% (4867/15552)\n",
      "Loss: 1.993 | Acc: 31.295% (4887/15616)\n",
      "Loss: 1.991 | Acc: 31.346% (4915/15680)\n",
      "Loss: 1.990 | Acc: 31.352% (4936/15744)\n",
      "Loss: 1.988 | Acc: 31.414% (4966/15808)\n",
      "Loss: 1.987 | Acc: 31.426% (4988/15872)\n",
      "Loss: 1.986 | Acc: 31.445% (5011/15936)\n",
      "Loss: 1.985 | Acc: 31.481% (5037/16000)\n",
      "Loss: 1.985 | Acc: 31.487% (5058/16064)\n",
      "Loss: 1.984 | Acc: 31.504% (5081/16128)\n",
      "Loss: 1.982 | Acc: 31.540% (5107/16192)\n",
      "Loss: 1.981 | Acc: 31.576% (5133/16256)\n",
      "Loss: 1.981 | Acc: 31.575% (5153/16320)\n",
      "Loss: 1.979 | Acc: 31.598% (5177/16384)\n",
      "Loss: 1.978 | Acc: 31.627% (5202/16448)\n",
      "Loss: 1.977 | Acc: 31.656% (5227/16512)\n",
      "Loss: 1.976 | Acc: 31.660% (5248/16576)\n",
      "Loss: 1.975 | Acc: 31.707% (5276/16640)\n",
      "Loss: 1.975 | Acc: 31.687% (5293/16704)\n",
      "Loss: 1.974 | Acc: 31.691% (5314/16768)\n",
      "Loss: 1.973 | Acc: 31.702% (5336/16832)\n",
      "Loss: 1.972 | Acc: 31.700% (5356/16896)\n",
      "Loss: 1.971 | Acc: 31.722% (5380/16960)\n",
      "Loss: 1.970 | Acc: 31.749% (5405/17024)\n",
      "Loss: 1.969 | Acc: 31.753% (5426/17088)\n",
      "Loss: 1.968 | Acc: 31.775% (5450/17152)\n",
      "Loss: 1.968 | Acc: 31.802% (5475/17216)\n",
      "Loss: 1.967 | Acc: 31.811% (5497/17280)\n",
      "Loss: 1.965 | Acc: 31.838% (5522/17344)\n",
      "Loss: 1.964 | Acc: 31.882% (5550/17408)\n",
      "Loss: 1.963 | Acc: 31.925% (5578/17472)\n",
      "Loss: 1.962 | Acc: 31.969% (5606/17536)\n",
      "Loss: 1.960 | Acc: 32.028% (5637/17600)\n",
      "Loss: 1.959 | Acc: 32.026% (5657/17664)\n",
      "Loss: 1.958 | Acc: 32.062% (5684/17728)\n",
      "Loss: 1.957 | Acc: 32.104% (5712/17792)\n",
      "Loss: 1.955 | Acc: 32.168% (5744/17856)\n",
      "Loss: 1.955 | Acc: 32.199% (5770/17920)\n",
      "Loss: 1.954 | Acc: 32.190% (5789/17984)\n",
      "Loss: 1.953 | Acc: 32.175% (5807/18048)\n",
      "Loss: 1.952 | Acc: 32.211% (5834/18112)\n",
      "Loss: 1.951 | Acc: 32.246% (5861/18176)\n",
      "Loss: 1.952 | Acc: 32.215% (5876/18240)\n",
      "Loss: 1.950 | Acc: 32.261% (5905/18304)\n",
      "Loss: 1.950 | Acc: 32.268% (5927/18368)\n",
      "Loss: 1.949 | Acc: 32.281% (5950/18432)\n",
      "Loss: 1.949 | Acc: 32.266% (5968/18496)\n",
      "Loss: 1.948 | Acc: 32.284% (5992/18560)\n",
      "Loss: 1.947 | Acc: 32.319% (6019/18624)\n",
      "Loss: 1.946 | Acc: 32.390% (6053/18688)\n",
      "Loss: 1.945 | Acc: 32.391% (6074/18752)\n",
      "Loss: 1.945 | Acc: 32.403% (6097/18816)\n",
      "Loss: 1.944 | Acc: 32.421% (6121/18880)\n",
      "Loss: 1.944 | Acc: 32.411% (6140/18944)\n",
      "Loss: 1.943 | Acc: 32.460% (6170/19008)\n",
      "Loss: 1.942 | Acc: 32.503% (6199/19072)\n",
      "Loss: 1.942 | Acc: 32.530% (6225/19136)\n",
      "Loss: 1.942 | Acc: 32.531% (6246/19200)\n",
      "Loss: 1.941 | Acc: 32.558% (6272/19264)\n",
      "Loss: 1.940 | Acc: 32.590% (6299/19328)\n",
      "Loss: 1.939 | Acc: 32.637% (6329/19392)\n",
      "Loss: 1.939 | Acc: 32.638% (6350/19456)\n",
      "Loss: 1.938 | Acc: 32.654% (6374/19520)\n",
      "Loss: 1.937 | Acc: 32.670% (6398/19584)\n",
      "Loss: 1.937 | Acc: 32.660% (6417/19648)\n",
      "Loss: 1.936 | Acc: 32.655% (6437/19712)\n",
      "Loss: 1.935 | Acc: 32.696% (6466/19776)\n",
      "Loss: 1.934 | Acc: 32.697% (6487/19840)\n",
      "Loss: 1.934 | Acc: 32.697% (6508/19904)\n",
      "Loss: 1.933 | Acc: 32.707% (6531/19968)\n",
      "Loss: 1.933 | Acc: 32.713% (6553/20032)\n",
      "Loss: 1.932 | Acc: 32.753% (6582/20096)\n",
      "Loss: 1.931 | Acc: 32.758% (6604/20160)\n",
      "Loss: 1.931 | Acc: 32.738% (6621/20224)\n",
      "Loss: 1.929 | Acc: 32.788% (6652/20288)\n",
      "Loss: 1.928 | Acc: 32.827% (6681/20352)\n",
      "Loss: 1.927 | Acc: 32.861% (6709/20416)\n",
      "Loss: 1.926 | Acc: 32.891% (6736/20480)\n",
      "Loss: 1.925 | Acc: 32.929% (6765/20544)\n",
      "Loss: 1.923 | Acc: 32.968% (6794/20608)\n",
      "Loss: 1.922 | Acc: 32.982% (6818/20672)\n",
      "Loss: 1.921 | Acc: 32.996% (6842/20736)\n",
      "Loss: 1.920 | Acc: 33.029% (6870/20800)\n",
      "Loss: 1.919 | Acc: 33.052% (6896/20864)\n",
      "Loss: 1.918 | Acc: 33.056% (6918/20928)\n",
      "Loss: 1.918 | Acc: 33.060% (6940/20992)\n",
      "Loss: 1.917 | Acc: 33.093% (6968/21056)\n",
      "Loss: 1.916 | Acc: 33.120% (6995/21120)\n",
      "Loss: 1.915 | Acc: 33.124% (7017/21184)\n",
      "Loss: 1.914 | Acc: 33.156% (7045/21248)\n",
      "Loss: 1.914 | Acc: 33.169% (7069/21312)\n",
      "Loss: 1.913 | Acc: 33.219% (7101/21376)\n",
      "Loss: 1.913 | Acc: 33.214% (7121/21440)\n",
      "Loss: 1.912 | Acc: 33.245% (7149/21504)\n",
      "Loss: 1.912 | Acc: 33.239% (7169/21568)\n",
      "Loss: 1.911 | Acc: 33.229% (7188/21632)\n",
      "Loss: 1.910 | Acc: 33.260% (7216/21696)\n",
      "Loss: 1.910 | Acc: 33.281% (7242/21760)\n",
      "Loss: 1.909 | Acc: 33.312% (7270/21824)\n",
      "Loss: 1.908 | Acc: 33.333% (7296/21888)\n",
      "Loss: 1.907 | Acc: 33.368% (7325/21952)\n",
      "Loss: 1.906 | Acc: 33.408% (7355/22016)\n",
      "Loss: 1.906 | Acc: 33.442% (7384/22080)\n",
      "Loss: 1.904 | Acc: 33.476% (7413/22144)\n",
      "Loss: 1.904 | Acc: 33.474% (7434/22208)\n",
      "Loss: 1.904 | Acc: 33.522% (7466/22272)\n",
      "Loss: 1.903 | Acc: 33.529% (7489/22336)\n",
      "Loss: 1.902 | Acc: 33.540% (7513/22400)\n",
      "Loss: 1.902 | Acc: 33.574% (7542/22464)\n",
      "Loss: 1.901 | Acc: 33.545% (7557/22528)\n",
      "Loss: 1.900 | Acc: 33.587% (7588/22592)\n",
      "Loss: 1.899 | Acc: 33.625% (7618/22656)\n",
      "Loss: 1.899 | Acc: 33.631% (7641/22720)\n",
      "Loss: 1.898 | Acc: 33.664% (7670/22784)\n",
      "Loss: 1.896 | Acc: 33.705% (7701/22848)\n",
      "Loss: 1.895 | Acc: 33.729% (7728/22912)\n",
      "Loss: 1.894 | Acc: 33.753% (7755/22976)\n",
      "Loss: 1.893 | Acc: 33.763% (7779/23040)\n",
      "Loss: 1.892 | Acc: 33.782% (7805/23104)\n",
      "Loss: 1.892 | Acc: 33.788% (7828/23168)\n",
      "Loss: 1.890 | Acc: 33.837% (7861/23232)\n",
      "Loss: 1.889 | Acc: 33.847% (7885/23296)\n",
      "Loss: 1.889 | Acc: 33.848% (7907/23360)\n",
      "Loss: 1.888 | Acc: 33.858% (7931/23424)\n",
      "Loss: 1.887 | Acc: 33.911% (7965/23488)\n",
      "Loss: 1.886 | Acc: 33.938% (7993/23552)\n",
      "Loss: 1.886 | Acc: 33.964% (8021/23616)\n",
      "Loss: 1.885 | Acc: 33.978% (8046/23680)\n",
      "Loss: 1.885 | Acc: 33.983% (8069/23744)\n",
      "Loss: 1.883 | Acc: 34.022% (8100/23808)\n",
      "Loss: 1.883 | Acc: 34.036% (8125/23872)\n",
      "Loss: 1.882 | Acc: 34.037% (8147/23936)\n",
      "Loss: 1.881 | Acc: 34.062% (8175/24000)\n",
      "Loss: 1.880 | Acc: 34.080% (8201/24064)\n",
      "Loss: 1.879 | Acc: 34.122% (8233/24128)\n",
      "Loss: 1.879 | Acc: 34.119% (8254/24192)\n",
      "Loss: 1.879 | Acc: 34.128% (8278/24256)\n",
      "Loss: 1.878 | Acc: 34.169% (8310/24320)\n",
      "Loss: 1.877 | Acc: 34.219% (8344/24384)\n",
      "Loss: 1.876 | Acc: 34.228% (8368/24448)\n",
      "Loss: 1.876 | Acc: 34.236% (8392/24512)\n",
      "Loss: 1.874 | Acc: 34.253% (8418/24576)\n",
      "Loss: 1.874 | Acc: 34.286% (8448/24640)\n",
      "Loss: 1.873 | Acc: 34.294% (8472/24704)\n",
      "Loss: 1.872 | Acc: 34.310% (8498/24768)\n",
      "Loss: 1.872 | Acc: 34.311% (8520/24832)\n",
      "Loss: 1.871 | Acc: 34.315% (8543/24896)\n",
      "Loss: 1.871 | Acc: 34.307% (8563/24960)\n",
      "Loss: 1.870 | Acc: 34.323% (8589/25024)\n",
      "Loss: 1.869 | Acc: 34.363% (8621/25088)\n",
      "Loss: 1.867 | Acc: 34.407% (8654/25152)\n",
      "Loss: 1.867 | Acc: 34.423% (8680/25216)\n",
      "Loss: 1.866 | Acc: 34.466% (8713/25280)\n",
      "Loss: 1.865 | Acc: 34.489% (8741/25344)\n",
      "Loss: 1.864 | Acc: 34.509% (8768/25408)\n",
      "Loss: 1.863 | Acc: 34.520% (8793/25472)\n",
      "Loss: 1.863 | Acc: 34.524% (8816/25536)\n",
      "Loss: 1.862 | Acc: 34.551% (8845/25600)\n",
      "Loss: 1.861 | Acc: 34.589% (8877/25664)\n",
      "Loss: 1.860 | Acc: 34.655% (8916/25728)\n",
      "Loss: 1.859 | Acc: 34.681% (8945/25792)\n",
      "Loss: 1.858 | Acc: 34.719% (8977/25856)\n",
      "Loss: 1.857 | Acc: 34.745% (9006/25920)\n",
      "Loss: 1.856 | Acc: 34.794% (9041/25984)\n",
      "Loss: 1.855 | Acc: 34.797% (9064/26048)\n",
      "Loss: 1.854 | Acc: 34.815% (9091/26112)\n",
      "Loss: 1.853 | Acc: 34.853% (9123/26176)\n",
      "Loss: 1.852 | Acc: 34.870% (9150/26240)\n",
      "Loss: 1.852 | Acc: 34.888% (9177/26304)\n",
      "Loss: 1.853 | Acc: 34.879% (9197/26368)\n",
      "Loss: 1.852 | Acc: 34.890% (9222/26432)\n",
      "Loss: 1.852 | Acc: 34.918% (9252/26496)\n",
      "Loss: 1.851 | Acc: 34.936% (9279/26560)\n",
      "Loss: 1.851 | Acc: 34.920% (9297/26624)\n",
      "Loss: 1.850 | Acc: 34.945% (9326/26688)\n",
      "Loss: 1.849 | Acc: 34.977% (9357/26752)\n",
      "Loss: 1.849 | Acc: 34.994% (9384/26816)\n",
      "Loss: 1.848 | Acc: 35.007% (9410/26880)\n",
      "Loss: 1.848 | Acc: 35.017% (9435/26944)\n",
      "Loss: 1.846 | Acc: 35.049% (9466/27008)\n",
      "Loss: 1.845 | Acc: 35.069% (9494/27072)\n",
      "Loss: 1.844 | Acc: 35.101% (9525/27136)\n",
      "Loss: 1.844 | Acc: 35.110% (9550/27200)\n",
      "Loss: 1.843 | Acc: 35.116% (9574/27264)\n",
      "Loss: 1.843 | Acc: 35.111% (9595/27328)\n",
      "Loss: 1.843 | Acc: 35.112% (9618/27392)\n",
      "Loss: 1.842 | Acc: 35.140% (9648/27456)\n",
      "Loss: 1.841 | Acc: 35.156% (9675/27520)\n",
      "Loss: 1.841 | Acc: 35.176% (9703/27584)\n",
      "Loss: 1.840 | Acc: 35.221% (9738/27648)\n",
      "Loss: 1.839 | Acc: 35.230% (9763/27712)\n",
      "Loss: 1.838 | Acc: 35.232% (9786/27776)\n",
      "Loss: 1.838 | Acc: 35.255% (9815/27840)\n",
      "Loss: 1.837 | Acc: 35.285% (9846/27904)\n",
      "Loss: 1.837 | Acc: 35.319% (9878/27968)\n",
      "Loss: 1.837 | Acc: 35.324% (9902/28032)\n",
      "Loss: 1.836 | Acc: 35.329% (9926/28096)\n",
      "Loss: 1.836 | Acc: 35.348% (9954/28160)\n",
      "Loss: 1.836 | Acc: 35.346% (9976/28224)\n",
      "Loss: 1.836 | Acc: 35.354% (10001/28288)\n",
      "Loss: 1.835 | Acc: 35.380% (10031/28352)\n",
      "Loss: 1.834 | Acc: 35.417% (10064/28416)\n",
      "Loss: 1.834 | Acc: 35.428% (10090/28480)\n",
      "Loss: 1.833 | Acc: 35.458% (10121/28544)\n",
      "Loss: 1.832 | Acc: 35.462% (10145/28608)\n",
      "Loss: 1.832 | Acc: 35.484% (10174/28672)\n",
      "Loss: 1.832 | Acc: 35.496% (10200/28736)\n",
      "Loss: 1.832 | Acc: 35.497% (10223/28800)\n",
      "Loss: 1.831 | Acc: 35.543% (10259/28864)\n",
      "Loss: 1.830 | Acc: 35.550% (10284/28928)\n",
      "Loss: 1.830 | Acc: 35.558% (10309/28992)\n",
      "Loss: 1.830 | Acc: 35.580% (10338/29056)\n",
      "Loss: 1.829 | Acc: 35.591% (10364/29120)\n",
      "Loss: 1.829 | Acc: 35.609% (10392/29184)\n",
      "Loss: 1.828 | Acc: 35.596% (10411/29248)\n",
      "Loss: 1.827 | Acc: 35.607% (10437/29312)\n",
      "Loss: 1.826 | Acc: 35.641% (10470/29376)\n",
      "Loss: 1.826 | Acc: 35.662% (10499/29440)\n",
      "Loss: 1.826 | Acc: 35.660% (10521/29504)\n",
      "Loss: 1.826 | Acc: 35.650% (10541/29568)\n",
      "Loss: 1.826 | Acc: 35.657% (10566/29632)\n",
      "Loss: 1.826 | Acc: 35.648% (10586/29696)\n",
      "Loss: 1.825 | Acc: 35.696% (10623/29760)\n",
      "Loss: 1.825 | Acc: 35.696% (10646/29824)\n",
      "Loss: 1.824 | Acc: 35.707% (10672/29888)\n",
      "Loss: 1.824 | Acc: 35.710% (10696/29952)\n",
      "Loss: 1.824 | Acc: 35.748% (10730/30016)\n",
      "Loss: 1.823 | Acc: 35.751% (10754/30080)\n",
      "Loss: 1.822 | Acc: 35.805% (10793/30144)\n",
      "Loss: 1.822 | Acc: 35.818% (10820/30208)\n",
      "Loss: 1.822 | Acc: 35.822% (10844/30272)\n",
      "Loss: 1.821 | Acc: 35.845% (10874/30336)\n",
      "Loss: 1.821 | Acc: 35.852% (10899/30400)\n",
      "Loss: 1.821 | Acc: 35.846% (10920/30464)\n",
      "Loss: 1.820 | Acc: 35.872% (10951/30528)\n",
      "Loss: 1.820 | Acc: 35.898% (10982/30592)\n",
      "Loss: 1.819 | Acc: 35.908% (11008/30656)\n",
      "Loss: 1.819 | Acc: 35.934% (11039/30720)\n",
      "Loss: 1.819 | Acc: 35.925% (11059/30784)\n",
      "Loss: 1.819 | Acc: 35.892% (11072/30848)\n",
      "Loss: 1.818 | Acc: 35.925% (11105/30912)\n",
      "Loss: 1.818 | Acc: 35.918% (11126/30976)\n",
      "Loss: 1.818 | Acc: 35.915% (11148/31040)\n",
      "Loss: 1.818 | Acc: 35.938% (11178/31104)\n",
      "Loss: 1.818 | Acc: 35.938% (11201/31168)\n",
      "Loss: 1.817 | Acc: 35.966% (11233/31232)\n",
      "Loss: 1.817 | Acc: 35.960% (11254/31296)\n",
      "Loss: 1.816 | Acc: 35.979% (11283/31360)\n",
      "Loss: 1.816 | Acc: 35.976% (11305/31424)\n",
      "Loss: 1.816 | Acc: 35.972% (11327/31488)\n",
      "Loss: 1.815 | Acc: 36.001% (11359/31552)\n",
      "Loss: 1.814 | Acc: 36.026% (11390/31616)\n",
      "Loss: 1.813 | Acc: 36.048% (11420/31680)\n",
      "Loss: 1.814 | Acc: 36.038% (11440/31744)\n",
      "Loss: 1.813 | Acc: 36.044% (11465/31808)\n",
      "Loss: 1.813 | Acc: 36.076% (11498/31872)\n",
      "Loss: 1.813 | Acc: 36.078% (11522/31936)\n",
      "Loss: 1.813 | Acc: 36.062% (11540/32000)\n",
      "Loss: 1.813 | Acc: 36.068% (11565/32064)\n",
      "Loss: 1.812 | Acc: 36.084% (11593/32128)\n",
      "Loss: 1.812 | Acc: 36.080% (11615/32192)\n",
      "Loss: 1.811 | Acc: 36.089% (11641/32256)\n",
      "Loss: 1.811 | Acc: 36.098% (11667/32320)\n",
      "Loss: 1.810 | Acc: 36.083% (11685/32384)\n",
      "Loss: 1.810 | Acc: 36.085% (11709/32448)\n",
      "Loss: 1.810 | Acc: 36.101% (11737/32512)\n",
      "Loss: 1.809 | Acc: 36.128% (11769/32576)\n",
      "Loss: 1.809 | Acc: 36.143% (11797/32640)\n",
      "Loss: 1.808 | Acc: 36.158% (11825/32704)\n",
      "Loss: 1.807 | Acc: 36.172% (11853/32768)\n",
      "Loss: 1.807 | Acc: 36.199% (11885/32832)\n",
      "Loss: 1.807 | Acc: 36.211% (11912/32896)\n",
      "Loss: 1.806 | Acc: 36.238% (11944/32960)\n",
      "Loss: 1.806 | Acc: 36.243% (11969/33024)\n",
      "Loss: 1.805 | Acc: 36.252% (11995/33088)\n",
      "Loss: 1.805 | Acc: 36.257% (12020/33152)\n",
      "Loss: 1.804 | Acc: 36.269% (12047/33216)\n",
      "Loss: 1.804 | Acc: 36.295% (12079/33280)\n",
      "Loss: 1.803 | Acc: 36.315% (12109/33344)\n",
      "Loss: 1.802 | Acc: 36.345% (12142/33408)\n",
      "Loss: 1.802 | Acc: 36.359% (12170/33472)\n",
      "Loss: 1.801 | Acc: 36.376% (12199/33536)\n",
      "Loss: 1.801 | Acc: 36.402% (12231/33600)\n",
      "Loss: 1.800 | Acc: 36.419% (12260/33664)\n",
      "Loss: 1.800 | Acc: 36.427% (12286/33728)\n",
      "Loss: 1.800 | Acc: 36.432% (12311/33792)\n",
      "Loss: 1.799 | Acc: 36.437% (12336/33856)\n",
      "Loss: 1.799 | Acc: 36.448% (12363/33920)\n",
      "Loss: 1.799 | Acc: 36.458% (12390/33984)\n",
      "Loss: 1.798 | Acc: 36.457% (12413/34048)\n",
      "Loss: 1.798 | Acc: 36.474% (12442/34112)\n",
      "Loss: 1.797 | Acc: 36.499% (12474/34176)\n",
      "Loss: 1.796 | Acc: 36.525% (12506/34240)\n",
      "Loss: 1.796 | Acc: 36.535% (12533/34304)\n",
      "Loss: 1.795 | Acc: 36.557% (12564/34368)\n",
      "Loss: 1.795 | Acc: 36.571% (12592/34432)\n",
      "Loss: 1.794 | Acc: 36.598% (12625/34496)\n",
      "Loss: 1.793 | Acc: 36.615% (12654/34560)\n",
      "Loss: 1.793 | Acc: 36.619% (12679/34624)\n",
      "Loss: 1.793 | Acc: 36.644% (12711/34688)\n",
      "Loss: 1.792 | Acc: 36.671% (12744/34752)\n",
      "Loss: 1.792 | Acc: 36.670% (12767/34816)\n",
      "Loss: 1.791 | Acc: 36.686% (12796/34880)\n",
      "Loss: 1.791 | Acc: 36.690% (12821/34944)\n",
      "Loss: 1.790 | Acc: 36.709% (12851/35008)\n",
      "Loss: 1.790 | Acc: 36.710% (12875/35072)\n",
      "Loss: 1.790 | Acc: 36.712% (12899/35136)\n",
      "Loss: 1.789 | Acc: 36.744% (12934/35200)\n",
      "Loss: 1.788 | Acc: 36.763% (12964/35264)\n",
      "Loss: 1.788 | Acc: 36.767% (12989/35328)\n",
      "Loss: 1.788 | Acc: 36.774% (13015/35392)\n",
      "Loss: 1.788 | Acc: 36.778% (13040/35456)\n",
      "Loss: 1.788 | Acc: 36.779% (13064/35520)\n",
      "Loss: 1.787 | Acc: 36.781% (13088/35584)\n",
      "Loss: 1.786 | Acc: 36.807% (13121/35648)\n",
      "Loss: 1.786 | Acc: 36.817% (13148/35712)\n",
      "Loss: 1.785 | Acc: 36.846% (13182/35776)\n",
      "Loss: 1.785 | Acc: 36.861% (13211/35840)\n",
      "Loss: 1.785 | Acc: 36.868% (13237/35904)\n",
      "Loss: 1.785 | Acc: 36.880% (13265/35968)\n",
      "Loss: 1.784 | Acc: 36.898% (13295/36032)\n",
      "Loss: 1.784 | Acc: 36.902% (13320/36096)\n",
      "Loss: 1.784 | Acc: 36.908% (13346/36160)\n",
      "Loss: 1.783 | Acc: 36.931% (13378/36224)\n",
      "Loss: 1.783 | Acc: 36.935% (13403/36288)\n",
      "Loss: 1.782 | Acc: 36.958% (13435/36352)\n",
      "Loss: 1.781 | Acc: 36.989% (13470/36416)\n",
      "Loss: 1.781 | Acc: 37.007% (13500/36480)\n",
      "Loss: 1.781 | Acc: 37.016% (13527/36544)\n",
      "Loss: 1.780 | Acc: 37.019% (13552/36608)\n",
      "Loss: 1.780 | Acc: 37.045% (13585/36672)\n",
      "Loss: 1.780 | Acc: 37.043% (13608/36736)\n",
      "Loss: 1.779 | Acc: 37.049% (13634/36800)\n",
      "Loss: 1.779 | Acc: 37.066% (13664/36864)\n",
      "Loss: 1.778 | Acc: 37.083% (13694/36928)\n",
      "Loss: 1.778 | Acc: 37.097% (13723/36992)\n",
      "Loss: 1.778 | Acc: 37.125% (13757/37056)\n",
      "Loss: 1.777 | Acc: 37.142% (13787/37120)\n",
      "Loss: 1.777 | Acc: 37.145% (13812/37184)\n",
      "Loss: 1.777 | Acc: 37.156% (13840/37248)\n",
      "Loss: 1.777 | Acc: 37.162% (13866/37312)\n",
      "Loss: 1.776 | Acc: 37.190% (13900/37376)\n",
      "Loss: 1.776 | Acc: 37.201% (13928/37440)\n",
      "Loss: 1.775 | Acc: 37.215% (13957/37504)\n",
      "Loss: 1.775 | Acc: 37.228% (13986/37568)\n",
      "Loss: 1.774 | Acc: 37.242% (14015/37632)\n",
      "Loss: 1.773 | Acc: 37.264% (14047/37696)\n",
      "Loss: 1.773 | Acc: 37.267% (14072/37760)\n",
      "Loss: 1.773 | Acc: 37.270% (14097/37824)\n",
      "Loss: 1.772 | Acc: 37.302% (14133/37888)\n",
      "Loss: 1.772 | Acc: 37.321% (14164/37952)\n",
      "Loss: 1.772 | Acc: 37.324% (14189/38016)\n",
      "Loss: 1.771 | Acc: 37.329% (14215/38080)\n",
      "Loss: 1.771 | Acc: 37.343% (14244/38144)\n",
      "Loss: 1.771 | Acc: 37.356% (14273/38208)\n",
      "Loss: 1.770 | Acc: 37.385% (14308/38272)\n",
      "Loss: 1.769 | Acc: 37.411% (14342/38336)\n",
      "Loss: 1.769 | Acc: 37.419% (14369/38400)\n",
      "Loss: 1.769 | Acc: 37.432% (14398/38464)\n",
      "Loss: 1.768 | Acc: 37.443% (14426/38528)\n",
      "Loss: 1.768 | Acc: 37.446% (14451/38592)\n",
      "Loss: 1.767 | Acc: 37.477% (14487/38656)\n",
      "Loss: 1.767 | Acc: 37.497% (14519/38720)\n",
      "Loss: 1.767 | Acc: 37.508% (14547/38784)\n",
      "Loss: 1.766 | Acc: 37.521% (14576/38848)\n",
      "Loss: 1.766 | Acc: 37.546% (14610/38912)\n",
      "Loss: 1.765 | Acc: 37.556% (14638/38976)\n",
      "Loss: 1.765 | Acc: 37.567% (14666/39040)\n",
      "Loss: 1.764 | Acc: 37.579% (14695/39104)\n",
      "Loss: 1.764 | Acc: 37.600% (14727/39168)\n",
      "Loss: 1.764 | Acc: 37.607% (14754/39232)\n",
      "Loss: 1.763 | Acc: 37.632% (14788/39296)\n",
      "Loss: 1.763 | Acc: 37.630% (14811/39360)\n",
      "Loss: 1.762 | Acc: 37.660% (14847/39424)\n",
      "Loss: 1.762 | Acc: 37.657% (14870/39488)\n",
      "Loss: 1.762 | Acc: 37.647% (14890/39552)\n",
      "Loss: 1.762 | Acc: 37.649% (14915/39616)\n",
      "Loss: 1.762 | Acc: 37.644% (14937/39680)\n",
      "Loss: 1.761 | Acc: 37.659% (14967/39744)\n",
      "Loss: 1.761 | Acc: 37.658% (14991/39808)\n",
      "Loss: 1.761 | Acc: 37.671% (15020/39872)\n",
      "Loss: 1.761 | Acc: 37.675% (15046/39936)\n",
      "Loss: 1.760 | Acc: 37.690% (15076/40000)\n",
      "Loss: 1.760 | Acc: 37.697% (15103/40064)\n",
      "Loss: 1.760 | Acc: 37.702% (15129/40128)\n",
      "Loss: 1.759 | Acc: 37.714% (15158/40192)\n",
      "Loss: 1.759 | Acc: 37.716% (15183/40256)\n",
      "Loss: 1.759 | Acc: 37.726% (15211/40320)\n",
      "Loss: 1.758 | Acc: 37.760% (15249/40384)\n",
      "Loss: 1.758 | Acc: 37.750% (15269/40448)\n",
      "Loss: 1.758 | Acc: 37.769% (15301/40512)\n",
      "Loss: 1.757 | Acc: 37.781% (15330/40576)\n",
      "Loss: 1.757 | Acc: 37.785% (15356/40640)\n",
      "Loss: 1.756 | Acc: 37.822% (15395/40704)\n",
      "Loss: 1.756 | Acc: 37.819% (15418/40768)\n",
      "Loss: 1.756 | Acc: 37.836% (15449/40832)\n",
      "Loss: 1.756 | Acc: 37.845% (15477/40896)\n",
      "Loss: 1.755 | Acc: 37.869% (15511/40960)\n",
      "Loss: 1.755 | Acc: 37.880% (15540/41024)\n",
      "Loss: 1.754 | Acc: 37.894% (15570/41088)\n",
      "Loss: 1.754 | Acc: 37.899% (15596/41152)\n",
      "Loss: 1.753 | Acc: 37.910% (15625/41216)\n",
      "Loss: 1.752 | Acc: 37.919% (15653/41280)\n",
      "Loss: 1.752 | Acc: 37.943% (15687/41344)\n",
      "Loss: 1.752 | Acc: 37.954% (15716/41408)\n",
      "Loss: 1.751 | Acc: 37.965% (15745/41472)\n",
      "Loss: 1.751 | Acc: 37.965% (15769/41536)\n",
      "Loss: 1.751 | Acc: 37.969% (15795/41600)\n",
      "Loss: 1.750 | Acc: 37.982% (15825/41664)\n",
      "Loss: 1.750 | Acc: 37.989% (15852/41728)\n",
      "Loss: 1.750 | Acc: 37.988% (15876/41792)\n",
      "Loss: 1.749 | Acc: 38.011% (15910/41856)\n",
      "Loss: 1.749 | Acc: 38.027% (15941/41920)\n",
      "Loss: 1.749 | Acc: 38.024% (15964/41984)\n",
      "Loss: 1.748 | Acc: 38.028% (15990/42048)\n",
      "Loss: 1.748 | Acc: 38.025% (16013/42112)\n",
      "Loss: 1.748 | Acc: 38.022% (16036/42176)\n",
      "Loss: 1.747 | Acc: 38.037% (16067/42240)\n",
      "Loss: 1.747 | Acc: 38.041% (16093/42304)\n",
      "Loss: 1.747 | Acc: 38.055% (16123/42368)\n",
      "Loss: 1.746 | Acc: 38.063% (16151/42432)\n",
      "Loss: 1.746 | Acc: 38.067% (16177/42496)\n",
      "Loss: 1.746 | Acc: 38.087% (16210/42560)\n",
      "Loss: 1.746 | Acc: 38.079% (16231/42624)\n",
      "Loss: 1.745 | Acc: 38.097% (16263/42688)\n",
      "Loss: 1.745 | Acc: 38.113% (16294/42752)\n",
      "Loss: 1.745 | Acc: 38.119% (16321/42816)\n",
      "Loss: 1.744 | Acc: 38.120% (16346/42880)\n",
      "Loss: 1.744 | Acc: 38.129% (16374/42944)\n",
      "Loss: 1.744 | Acc: 38.123% (16396/43008)\n",
      "Loss: 1.744 | Acc: 38.132% (16424/43072)\n",
      "Loss: 1.744 | Acc: 38.121% (16444/43136)\n",
      "Loss: 1.743 | Acc: 38.116% (16466/43200)\n",
      "Loss: 1.743 | Acc: 38.131% (16497/43264)\n",
      "Loss: 1.742 | Acc: 38.160% (16534/43328)\n",
      "Loss: 1.742 | Acc: 38.164% (16560/43392)\n",
      "Loss: 1.741 | Acc: 38.186% (16594/43456)\n",
      "Loss: 1.742 | Acc: 38.166% (16610/43520)\n",
      "Loss: 1.741 | Acc: 38.181% (16641/43584)\n",
      "Loss: 1.741 | Acc: 38.187% (16668/43648)\n",
      "Loss: 1.741 | Acc: 38.191% (16694/43712)\n",
      "Loss: 1.741 | Acc: 38.194% (16720/43776)\n",
      "Loss: 1.740 | Acc: 38.205% (16749/43840)\n",
      "Loss: 1.740 | Acc: 38.229% (16784/43904)\n",
      "Loss: 1.740 | Acc: 38.253% (16819/43968)\n",
      "Loss: 1.739 | Acc: 38.279% (16855/44032)\n",
      "Loss: 1.739 | Acc: 38.289% (16884/44096)\n",
      "Loss: 1.738 | Acc: 38.288% (16908/44160)\n",
      "Loss: 1.738 | Acc: 38.294% (16935/44224)\n",
      "Loss: 1.738 | Acc: 38.320% (16971/44288)\n",
      "Loss: 1.737 | Acc: 38.325% (16998/44352)\n",
      "Loss: 1.737 | Acc: 38.347% (17032/44416)\n",
      "Loss: 1.737 | Acc: 38.341% (17054/44480)\n",
      "Loss: 1.737 | Acc: 38.337% (17077/44544)\n",
      "Loss: 1.736 | Acc: 38.347% (17106/44608)\n",
      "Loss: 1.736 | Acc: 38.362% (17137/44672)\n",
      "Loss: 1.735 | Acc: 38.374% (17167/44736)\n",
      "Loss: 1.735 | Acc: 38.391% (17199/44800)\n",
      "Loss: 1.734 | Acc: 38.412% (17233/44864)\n",
      "Loss: 1.734 | Acc: 38.413% (17258/44928)\n",
      "Loss: 1.734 | Acc: 38.422% (17287/44992)\n",
      "Loss: 1.734 | Acc: 38.426% (17313/45056)\n",
      "Loss: 1.733 | Acc: 38.435% (17342/45120)\n",
      "Loss: 1.733 | Acc: 38.452% (17374/45184)\n",
      "Loss: 1.733 | Acc: 38.459% (17402/45248)\n",
      "Loss: 1.732 | Acc: 38.473% (17433/45312)\n",
      "Loss: 1.732 | Acc: 38.487% (17464/45376)\n",
      "Loss: 1.731 | Acc: 38.506% (17497/45440)\n",
      "Loss: 1.731 | Acc: 38.509% (17523/45504)\n",
      "Loss: 1.731 | Acc: 38.525% (17555/45568)\n",
      "Loss: 1.731 | Acc: 38.532% (17583/45632)\n",
      "Loss: 1.731 | Acc: 38.533% (17608/45696)\n",
      "Loss: 1.730 | Acc: 38.542% (17637/45760)\n",
      "Loss: 1.730 | Acc: 38.556% (17668/45824)\n",
      "Loss: 1.730 | Acc: 38.561% (17695/45888)\n",
      "Loss: 1.730 | Acc: 38.564% (17721/45952)\n",
      "Loss: 1.729 | Acc: 38.576% (17751/46016)\n",
      "Loss: 1.729 | Acc: 38.592% (17783/46080)\n",
      "Loss: 1.728 | Acc: 38.584% (17804/46144)\n",
      "Loss: 1.729 | Acc: 38.573% (17824/46208)\n",
      "Loss: 1.728 | Acc: 38.578% (17851/46272)\n",
      "Loss: 1.728 | Acc: 38.575% (17874/46336)\n",
      "Loss: 1.728 | Acc: 38.575% (17899/46400)\n",
      "Loss: 1.727 | Acc: 38.602% (17936/46464)\n",
      "Loss: 1.727 | Acc: 38.588% (17954/46528)\n",
      "Loss: 1.727 | Acc: 38.599% (17984/46592)\n",
      "Loss: 1.727 | Acc: 38.619% (18018/46656)\n",
      "Loss: 1.726 | Acc: 38.630% (18048/46720)\n",
      "Loss: 1.726 | Acc: 38.641% (18078/46784)\n",
      "Loss: 1.726 | Acc: 38.633% (18099/46848)\n",
      "Loss: 1.726 | Acc: 38.632% (18123/46912)\n",
      "Loss: 1.725 | Acc: 38.639% (18151/46976)\n",
      "Loss: 1.725 | Acc: 38.659% (18185/47040)\n",
      "Loss: 1.725 | Acc: 38.676% (18218/47104)\n",
      "Loss: 1.725 | Acc: 38.660% (18235/47168)\n",
      "Loss: 1.724 | Acc: 38.667% (18263/47232)\n",
      "Loss: 1.724 | Acc: 38.669% (18289/47296)\n",
      "Loss: 1.724 | Acc: 38.674% (18316/47360)\n",
      "Loss: 1.724 | Acc: 38.681% (18344/47424)\n",
      "Loss: 1.723 | Acc: 38.686% (18371/47488)\n",
      "Loss: 1.723 | Acc: 38.686% (18396/47552)\n",
      "Loss: 1.723 | Acc: 38.691% (18423/47616)\n",
      "Loss: 1.723 | Acc: 38.706% (18455/47680)\n",
      "Loss: 1.722 | Acc: 38.729% (18491/47744)\n",
      "Loss: 1.722 | Acc: 38.747% (18524/47808)\n",
      "Loss: 1.721 | Acc: 38.753% (18552/47872)\n",
      "Loss: 1.721 | Acc: 38.764% (18582/47936)\n",
      "Loss: 1.721 | Acc: 38.781% (18615/48000)\n",
      "Loss: 1.721 | Acc: 38.786% (18642/48064)\n",
      "Loss: 1.721 | Acc: 38.782% (18665/48128)\n",
      "Loss: 1.720 | Acc: 38.784% (18691/48192)\n",
      "Loss: 1.720 | Acc: 38.803% (18725/48256)\n",
      "Loss: 1.719 | Acc: 38.802% (18749/48320)\n",
      "Loss: 1.719 | Acc: 38.819% (18782/48384)\n",
      "Loss: 1.719 | Acc: 38.833% (18814/48448)\n",
      "Loss: 1.719 | Acc: 38.842% (18843/48512)\n",
      "Loss: 1.719 | Acc: 38.840% (18867/48576)\n",
      "Loss: 1.718 | Acc: 38.849% (18896/48640)\n",
      "Loss: 1.718 | Acc: 38.861% (18927/48704)\n",
      "Loss: 1.718 | Acc: 38.870% (18956/48768)\n",
      "Loss: 1.717 | Acc: 38.874% (18983/48832)\n",
      "Loss: 1.717 | Acc: 38.891% (19016/48896)\n",
      "Loss: 1.717 | Acc: 38.905% (19048/48960)\n",
      "Loss: 1.716 | Acc: 38.918% (19070/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 38.91836734693877\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.562 | Acc: 45.312% (29/64)\n",
      "Loss: 1.525 | Acc: 46.094% (59/128)\n",
      "Loss: 1.535 | Acc: 40.104% (77/192)\n",
      "Loss: 1.574 | Acc: 40.625% (104/256)\n",
      "Loss: 1.534 | Acc: 41.250% (132/320)\n",
      "Loss: 1.583 | Acc: 40.365% (155/384)\n",
      "Loss: 1.613 | Acc: 40.848% (183/448)\n",
      "Loss: 1.583 | Acc: 41.992% (215/512)\n",
      "Loss: 1.561 | Acc: 43.750% (252/576)\n",
      "Loss: 1.556 | Acc: 43.594% (279/640)\n",
      "Loss: 1.564 | Acc: 43.608% (307/704)\n",
      "Loss: 1.563 | Acc: 43.750% (336/768)\n",
      "Loss: 1.562 | Acc: 43.750% (364/832)\n",
      "Loss: 1.556 | Acc: 43.638% (391/896)\n",
      "Loss: 1.543 | Acc: 43.854% (421/960)\n",
      "Loss: 1.523 | Acc: 44.727% (458/1024)\n",
      "Loss: 1.527 | Acc: 44.945% (489/1088)\n",
      "Loss: 1.529 | Acc: 44.792% (516/1152)\n",
      "Loss: 1.525 | Acc: 45.395% (552/1216)\n",
      "Loss: 1.532 | Acc: 44.609% (571/1280)\n",
      "Loss: 1.531 | Acc: 44.866% (603/1344)\n",
      "Loss: 1.533 | Acc: 45.170% (636/1408)\n",
      "Loss: 1.527 | Acc: 45.245% (666/1472)\n",
      "Loss: 1.533 | Acc: 44.922% (690/1536)\n",
      "Loss: 1.531 | Acc: 45.312% (725/1600)\n",
      "Loss: 1.531 | Acc: 45.373% (755/1664)\n",
      "Loss: 1.533 | Acc: 45.486% (786/1728)\n",
      "Loss: 1.534 | Acc: 45.424% (814/1792)\n",
      "Loss: 1.532 | Acc: 45.474% (844/1856)\n",
      "Loss: 1.533 | Acc: 45.677% (877/1920)\n",
      "Loss: 1.537 | Acc: 45.514% (903/1984)\n",
      "Loss: 1.536 | Acc: 45.605% (934/2048)\n",
      "Loss: 1.532 | Acc: 45.597% (963/2112)\n",
      "Loss: 1.532 | Acc: 45.726% (995/2176)\n",
      "Loss: 1.529 | Acc: 45.804% (1026/2240)\n",
      "Loss: 1.525 | Acc: 45.964% (1059/2304)\n",
      "Loss: 1.528 | Acc: 45.777% (1084/2368)\n",
      "Loss: 1.528 | Acc: 45.765% (1113/2432)\n",
      "Loss: 1.523 | Acc: 46.034% (1149/2496)\n",
      "Loss: 1.531 | Acc: 45.781% (1172/2560)\n",
      "Loss: 1.530 | Acc: 45.732% (1200/2624)\n",
      "Loss: 1.531 | Acc: 45.759% (1230/2688)\n",
      "Loss: 1.526 | Acc: 45.749% (1259/2752)\n",
      "Loss: 1.529 | Acc: 45.668% (1286/2816)\n",
      "Loss: 1.527 | Acc: 45.764% (1318/2880)\n",
      "Loss: 1.526 | Acc: 45.822% (1349/2944)\n",
      "Loss: 1.524 | Acc: 45.811% (1378/3008)\n",
      "Loss: 1.521 | Acc: 45.898% (1410/3072)\n",
      "Loss: 1.520 | Acc: 45.855% (1438/3136)\n",
      "Loss: 1.520 | Acc: 45.906% (1469/3200)\n",
      "Loss: 1.520 | Acc: 45.895% (1498/3264)\n",
      "Loss: 1.521 | Acc: 45.853% (1526/3328)\n",
      "Loss: 1.518 | Acc: 45.902% (1557/3392)\n",
      "Loss: 1.518 | Acc: 45.949% (1588/3456)\n",
      "Loss: 1.517 | Acc: 45.938% (1617/3520)\n",
      "Loss: 1.514 | Acc: 46.038% (1650/3584)\n",
      "Loss: 1.515 | Acc: 45.970% (1677/3648)\n",
      "Loss: 1.514 | Acc: 45.932% (1705/3712)\n",
      "Loss: 1.515 | Acc: 45.816% (1730/3776)\n",
      "Loss: 1.512 | Acc: 45.938% (1764/3840)\n",
      "Loss: 1.512 | Acc: 45.902% (1792/3904)\n",
      "Loss: 1.511 | Acc: 45.968% (1824/3968)\n",
      "Loss: 1.511 | Acc: 45.957% (1853/4032)\n",
      "Loss: 1.515 | Acc: 45.923% (1881/4096)\n",
      "Loss: 1.517 | Acc: 45.889% (1909/4160)\n",
      "Loss: 1.513 | Acc: 46.023% (1944/4224)\n",
      "Loss: 1.511 | Acc: 46.035% (1974/4288)\n",
      "Loss: 1.511 | Acc: 46.117% (2007/4352)\n",
      "Loss: 1.507 | Acc: 46.332% (2046/4416)\n",
      "Loss: 1.504 | Acc: 46.339% (2076/4480)\n",
      "Loss: 1.502 | Acc: 46.391% (2108/4544)\n",
      "Loss: 1.502 | Acc: 46.398% (2138/4608)\n",
      "Loss: 1.502 | Acc: 46.361% (2166/4672)\n",
      "Loss: 1.499 | Acc: 46.432% (2199/4736)\n",
      "Loss: 1.501 | Acc: 46.438% (2229/4800)\n",
      "Loss: 1.499 | Acc: 46.484% (2261/4864)\n",
      "Loss: 1.499 | Acc: 46.550% (2294/4928)\n",
      "Loss: 1.498 | Acc: 46.554% (2324/4992)\n",
      "Loss: 1.499 | Acc: 46.519% (2352/5056)\n",
      "Loss: 1.501 | Acc: 46.445% (2378/5120)\n",
      "Loss: 1.501 | Acc: 46.528% (2412/5184)\n",
      "Loss: 1.502 | Acc: 46.494% (2440/5248)\n",
      "Loss: 1.501 | Acc: 46.498% (2470/5312)\n",
      "Loss: 1.505 | Acc: 46.410% (2495/5376)\n",
      "Loss: 1.505 | Acc: 46.324% (2520/5440)\n",
      "Loss: 1.506 | Acc: 46.330% (2550/5504)\n",
      "Loss: 1.509 | Acc: 46.300% (2578/5568)\n",
      "Loss: 1.511 | Acc: 46.254% (2605/5632)\n",
      "Loss: 1.512 | Acc: 46.313% (2638/5696)\n",
      "Loss: 1.511 | Acc: 46.285% (2666/5760)\n",
      "Loss: 1.512 | Acc: 46.188% (2690/5824)\n",
      "Loss: 1.514 | Acc: 46.077% (2713/5888)\n",
      "Loss: 1.515 | Acc: 46.169% (2748/5952)\n",
      "Loss: 1.514 | Acc: 46.177% (2778/6016)\n",
      "Loss: 1.514 | Acc: 46.118% (2804/6080)\n",
      "Loss: 1.514 | Acc: 46.110% (2833/6144)\n",
      "Loss: 1.515 | Acc: 46.150% (2865/6208)\n",
      "Loss: 1.517 | Acc: 46.078% (2890/6272)\n",
      "Loss: 1.517 | Acc: 46.102% (2921/6336)\n",
      "Loss: 1.519 | Acc: 45.984% (2943/6400)\n",
      "Loss: 1.518 | Acc: 45.978% (2972/6464)\n",
      "Loss: 1.518 | Acc: 46.002% (3003/6528)\n",
      "Loss: 1.520 | Acc: 45.859% (3023/6592)\n",
      "Loss: 1.520 | Acc: 45.853% (3052/6656)\n",
      "Loss: 1.519 | Acc: 45.818% (3079/6720)\n",
      "Loss: 1.518 | Acc: 45.873% (3112/6784)\n",
      "Loss: 1.515 | Acc: 45.940% (3146/6848)\n",
      "Loss: 1.517 | Acc: 45.862% (3170/6912)\n",
      "Loss: 1.518 | Acc: 45.900% (3202/6976)\n",
      "Loss: 1.519 | Acc: 45.781% (3223/7040)\n",
      "Loss: 1.519 | Acc: 45.707% (3247/7104)\n",
      "Loss: 1.519 | Acc: 45.773% (3281/7168)\n",
      "Loss: 1.521 | Acc: 45.631% (3300/7232)\n",
      "Loss: 1.521 | Acc: 45.641% (3330/7296)\n",
      "Loss: 1.519 | Acc: 45.666% (3361/7360)\n",
      "Loss: 1.519 | Acc: 45.609% (3386/7424)\n",
      "Loss: 1.518 | Acc: 45.686% (3421/7488)\n",
      "Loss: 1.517 | Acc: 45.670% (3449/7552)\n",
      "Loss: 1.518 | Acc: 45.628% (3475/7616)\n",
      "Loss: 1.518 | Acc: 45.638% (3505/7680)\n",
      "Loss: 1.517 | Acc: 45.674% (3537/7744)\n",
      "Loss: 1.516 | Acc: 45.671% (3566/7808)\n",
      "Loss: 1.518 | Acc: 45.668% (3595/7872)\n",
      "Loss: 1.518 | Acc: 45.678% (3625/7936)\n",
      "Loss: 1.520 | Acc: 45.650% (3652/8000)\n",
      "Loss: 1.519 | Acc: 45.660% (3682/8064)\n",
      "Loss: 1.521 | Acc: 45.583% (3705/8128)\n",
      "Loss: 1.520 | Acc: 45.630% (3738/8192)\n",
      "Loss: 1.521 | Acc: 45.555% (3761/8256)\n",
      "Loss: 1.522 | Acc: 45.433% (3780/8320)\n",
      "Loss: 1.522 | Acc: 45.444% (3810/8384)\n",
      "Loss: 1.521 | Acc: 45.431% (3838/8448)\n",
      "Loss: 1.522 | Acc: 45.395% (3864/8512)\n",
      "Loss: 1.522 | Acc: 45.394% (3893/8576)\n",
      "Loss: 1.524 | Acc: 45.405% (3923/8640)\n",
      "Loss: 1.524 | Acc: 45.301% (3943/8704)\n",
      "Loss: 1.525 | Acc: 45.267% (3969/8768)\n",
      "Loss: 1.525 | Acc: 45.279% (3999/8832)\n",
      "Loss: 1.524 | Acc: 45.335% (4033/8896)\n",
      "Loss: 1.523 | Acc: 45.357% (4064/8960)\n",
      "Loss: 1.524 | Acc: 45.301% (4088/9024)\n",
      "Loss: 1.523 | Acc: 45.268% (4114/9088)\n",
      "Loss: 1.523 | Acc: 45.302% (4146/9152)\n",
      "Loss: 1.521 | Acc: 45.410% (4185/9216)\n",
      "Loss: 1.521 | Acc: 45.409% (4214/9280)\n",
      "Loss: 1.522 | Acc: 45.420% (4244/9344)\n",
      "Loss: 1.522 | Acc: 45.451% (4276/9408)\n",
      "Loss: 1.523 | Acc: 45.365% (4297/9472)\n",
      "Loss: 1.522 | Acc: 45.428% (4332/9536)\n",
      "Loss: 1.521 | Acc: 45.479% (4366/9600)\n",
      "Loss: 1.520 | Acc: 45.499% (4397/9664)\n",
      "Loss: 1.519 | Acc: 45.508% (4427/9728)\n",
      "Loss: 1.520 | Acc: 45.466% (4452/9792)\n",
      "Loss: 1.520 | Acc: 45.455% (4480/9856)\n",
      "Loss: 1.521 | Acc: 45.393% (4503/9920)\n",
      "Loss: 1.522 | Acc: 45.353% (4528/9984)\n",
      "Loss: 1.521 | Acc: 45.350% (4535/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 45.35\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.588 | Acc: 42.188% (27/64)\n",
      "Loss: 1.485 | Acc: 46.094% (59/128)\n",
      "Loss: 1.427 | Acc: 48.958% (94/192)\n",
      "Loss: 1.423 | Acc: 48.047% (123/256)\n",
      "Loss: 1.394 | Acc: 49.062% (157/320)\n",
      "Loss: 1.434 | Acc: 48.177% (185/384)\n",
      "Loss: 1.441 | Acc: 48.438% (217/448)\n",
      "Loss: 1.444 | Acc: 48.438% (248/512)\n",
      "Loss: 1.457 | Acc: 47.743% (275/576)\n",
      "Loss: 1.460 | Acc: 48.594% (311/640)\n",
      "Loss: 1.445 | Acc: 49.290% (347/704)\n",
      "Loss: 1.447 | Acc: 49.219% (378/768)\n",
      "Loss: 1.453 | Acc: 48.678% (405/832)\n",
      "Loss: 1.453 | Acc: 48.549% (435/896)\n",
      "Loss: 1.446 | Acc: 48.333% (464/960)\n",
      "Loss: 1.443 | Acc: 48.633% (498/1024)\n",
      "Loss: 1.438 | Acc: 48.346% (526/1088)\n",
      "Loss: 1.445 | Acc: 48.177% (555/1152)\n",
      "Loss: 1.448 | Acc: 48.109% (585/1216)\n",
      "Loss: 1.448 | Acc: 48.516% (621/1280)\n",
      "Loss: 1.458 | Acc: 48.214% (648/1344)\n",
      "Loss: 1.469 | Acc: 47.443% (668/1408)\n",
      "Loss: 1.480 | Acc: 46.739% (688/1472)\n",
      "Loss: 1.482 | Acc: 46.875% (720/1536)\n",
      "Loss: 1.476 | Acc: 46.812% (749/1600)\n",
      "Loss: 1.476 | Acc: 46.695% (777/1664)\n",
      "Loss: 1.478 | Acc: 46.817% (809/1728)\n",
      "Loss: 1.469 | Acc: 46.875% (840/1792)\n",
      "Loss: 1.473 | Acc: 46.552% (864/1856)\n",
      "Loss: 1.465 | Acc: 46.875% (900/1920)\n",
      "Loss: 1.463 | Acc: 46.925% (931/1984)\n",
      "Loss: 1.470 | Acc: 46.729% (957/2048)\n",
      "Loss: 1.471 | Acc: 46.780% (988/2112)\n",
      "Loss: 1.476 | Acc: 46.599% (1014/2176)\n",
      "Loss: 1.479 | Acc: 46.562% (1043/2240)\n",
      "Loss: 1.480 | Acc: 46.528% (1072/2304)\n",
      "Loss: 1.473 | Acc: 46.706% (1106/2368)\n",
      "Loss: 1.469 | Acc: 46.875% (1140/2432)\n",
      "Loss: 1.463 | Acc: 47.155% (1177/2496)\n",
      "Loss: 1.453 | Acc: 47.578% (1218/2560)\n",
      "Loss: 1.449 | Acc: 47.790% (1254/2624)\n",
      "Loss: 1.448 | Acc: 47.768% (1284/2688)\n",
      "Loss: 1.446 | Acc: 47.965% (1320/2752)\n",
      "Loss: 1.438 | Acc: 48.260% (1359/2816)\n",
      "Loss: 1.445 | Acc: 48.125% (1386/2880)\n",
      "Loss: 1.446 | Acc: 47.962% (1412/2944)\n",
      "Loss: 1.439 | Acc: 48.238% (1451/3008)\n",
      "Loss: 1.441 | Acc: 48.307% (1484/3072)\n",
      "Loss: 1.440 | Acc: 48.182% (1511/3136)\n",
      "Loss: 1.443 | Acc: 48.094% (1539/3200)\n",
      "Loss: 1.445 | Acc: 48.009% (1567/3264)\n",
      "Loss: 1.448 | Acc: 47.897% (1594/3328)\n",
      "Loss: 1.445 | Acc: 48.113% (1632/3392)\n",
      "Loss: 1.447 | Acc: 48.032% (1660/3456)\n",
      "Loss: 1.448 | Acc: 47.926% (1687/3520)\n",
      "Loss: 1.453 | Acc: 47.740% (1711/3584)\n",
      "Loss: 1.454 | Acc: 47.725% (1741/3648)\n",
      "Loss: 1.454 | Acc: 47.710% (1771/3712)\n",
      "Loss: 1.453 | Acc: 47.749% (1803/3776)\n",
      "Loss: 1.455 | Acc: 47.734% (1833/3840)\n",
      "Loss: 1.460 | Acc: 47.515% (1855/3904)\n",
      "Loss: 1.463 | Acc: 47.455% (1883/3968)\n",
      "Loss: 1.462 | Acc: 47.520% (1916/4032)\n",
      "Loss: 1.461 | Acc: 47.632% (1951/4096)\n",
      "Loss: 1.463 | Acc: 47.620% (1981/4160)\n",
      "Loss: 1.465 | Acc: 47.585% (2010/4224)\n",
      "Loss: 1.465 | Acc: 47.621% (2042/4288)\n",
      "Loss: 1.465 | Acc: 47.564% (2070/4352)\n",
      "Loss: 1.466 | Acc: 47.554% (2100/4416)\n",
      "Loss: 1.465 | Acc: 47.612% (2133/4480)\n",
      "Loss: 1.463 | Acc: 47.623% (2164/4544)\n",
      "Loss: 1.465 | Acc: 47.656% (2196/4608)\n",
      "Loss: 1.463 | Acc: 47.667% (2227/4672)\n",
      "Loss: 1.463 | Acc: 47.614% (2255/4736)\n",
      "Loss: 1.463 | Acc: 47.583% (2284/4800)\n",
      "Loss: 1.463 | Acc: 47.615% (2316/4864)\n",
      "Loss: 1.464 | Acc: 47.524% (2342/4928)\n",
      "Loss: 1.466 | Acc: 47.396% (2366/4992)\n",
      "Loss: 1.467 | Acc: 47.310% (2392/5056)\n",
      "Loss: 1.467 | Acc: 47.266% (2420/5120)\n",
      "Loss: 1.468 | Acc: 47.299% (2452/5184)\n",
      "Loss: 1.467 | Acc: 47.389% (2487/5248)\n",
      "Loss: 1.466 | Acc: 47.346% (2515/5312)\n",
      "Loss: 1.463 | Acc: 47.452% (2551/5376)\n",
      "Loss: 1.465 | Acc: 47.335% (2575/5440)\n",
      "Loss: 1.464 | Acc: 47.420% (2610/5504)\n",
      "Loss: 1.464 | Acc: 47.432% (2641/5568)\n",
      "Loss: 1.463 | Acc: 47.443% (2672/5632)\n",
      "Loss: 1.462 | Acc: 47.507% (2706/5696)\n",
      "Loss: 1.463 | Acc: 47.500% (2736/5760)\n",
      "Loss: 1.462 | Acc: 47.442% (2763/5824)\n",
      "Loss: 1.459 | Acc: 47.486% (2796/5888)\n",
      "Loss: 1.457 | Acc: 47.480% (2826/5952)\n",
      "Loss: 1.457 | Acc: 47.473% (2856/6016)\n",
      "Loss: 1.454 | Acc: 47.632% (2896/6080)\n",
      "Loss: 1.454 | Acc: 47.607% (2925/6144)\n",
      "Loss: 1.453 | Acc: 47.568% (2953/6208)\n",
      "Loss: 1.452 | Acc: 47.640% (2988/6272)\n",
      "Loss: 1.451 | Acc: 47.664% (3020/6336)\n",
      "Loss: 1.453 | Acc: 47.625% (3048/6400)\n",
      "Loss: 1.454 | Acc: 47.618% (3078/6464)\n",
      "Loss: 1.454 | Acc: 47.595% (3107/6528)\n",
      "Loss: 1.452 | Acc: 47.649% (3141/6592)\n",
      "Loss: 1.451 | Acc: 47.656% (3172/6656)\n",
      "Loss: 1.452 | Acc: 47.693% (3205/6720)\n",
      "Loss: 1.452 | Acc: 47.656% (3233/6784)\n",
      "Loss: 1.450 | Acc: 47.707% (3267/6848)\n",
      "Loss: 1.448 | Acc: 47.815% (3305/6912)\n",
      "Loss: 1.450 | Acc: 47.835% (3337/6976)\n",
      "Loss: 1.449 | Acc: 47.855% (3369/7040)\n",
      "Loss: 1.448 | Acc: 47.846% (3399/7104)\n",
      "Loss: 1.447 | Acc: 47.824% (3428/7168)\n",
      "Loss: 1.447 | Acc: 47.788% (3456/7232)\n",
      "Loss: 1.449 | Acc: 47.725% (3482/7296)\n",
      "Loss: 1.448 | Acc: 47.758% (3515/7360)\n",
      "Loss: 1.448 | Acc: 47.751% (3545/7424)\n",
      "Loss: 1.448 | Acc: 47.716% (3573/7488)\n",
      "Loss: 1.448 | Acc: 47.709% (3603/7552)\n",
      "Loss: 1.448 | Acc: 47.689% (3632/7616)\n",
      "Loss: 1.447 | Acc: 47.721% (3665/7680)\n",
      "Loss: 1.447 | Acc: 47.676% (3692/7744)\n",
      "Loss: 1.446 | Acc: 47.759% (3729/7808)\n",
      "Loss: 1.446 | Acc: 47.764% (3760/7872)\n",
      "Loss: 1.447 | Acc: 47.770% (3791/7936)\n",
      "Loss: 1.446 | Acc: 47.812% (3825/8000)\n",
      "Loss: 1.446 | Acc: 47.793% (3854/8064)\n",
      "Loss: 1.445 | Acc: 47.785% (3884/8128)\n",
      "Loss: 1.444 | Acc: 47.815% (3917/8192)\n",
      "Loss: 1.444 | Acc: 47.820% (3948/8256)\n",
      "Loss: 1.441 | Acc: 47.933% (3988/8320)\n",
      "Loss: 1.442 | Acc: 47.960% (4021/8384)\n",
      "Loss: 1.441 | Acc: 47.976% (4053/8448)\n",
      "Loss: 1.443 | Acc: 47.921% (4079/8512)\n",
      "Loss: 1.442 | Acc: 47.971% (4114/8576)\n",
      "Loss: 1.442 | Acc: 47.986% (4146/8640)\n",
      "Loss: 1.440 | Acc: 48.035% (4181/8704)\n",
      "Loss: 1.442 | Acc: 47.993% (4208/8768)\n",
      "Loss: 1.445 | Acc: 47.917% (4232/8832)\n",
      "Loss: 1.446 | Acc: 47.819% (4254/8896)\n",
      "Loss: 1.447 | Acc: 47.801% (4283/8960)\n",
      "Loss: 1.446 | Acc: 47.784% (4312/9024)\n",
      "Loss: 1.446 | Acc: 47.777% (4342/9088)\n",
      "Loss: 1.445 | Acc: 47.716% (4367/9152)\n",
      "Loss: 1.447 | Acc: 47.656% (4392/9216)\n",
      "Loss: 1.448 | Acc: 47.597% (4417/9280)\n",
      "Loss: 1.449 | Acc: 47.560% (4444/9344)\n",
      "Loss: 1.451 | Acc: 47.470% (4466/9408)\n",
      "Loss: 1.450 | Acc: 47.498% (4499/9472)\n",
      "Loss: 1.451 | Acc: 47.441% (4524/9536)\n",
      "Loss: 1.452 | Acc: 47.406% (4551/9600)\n",
      "Loss: 1.450 | Acc: 47.475% (4588/9664)\n",
      "Loss: 1.451 | Acc: 47.410% (4612/9728)\n",
      "Loss: 1.451 | Acc: 47.437% (4645/9792)\n",
      "Loss: 1.451 | Acc: 47.332% (4665/9856)\n",
      "Loss: 1.451 | Acc: 47.339% (4696/9920)\n",
      "Loss: 1.451 | Acc: 47.306% (4723/9984)\n",
      "Loss: 1.450 | Acc: 47.333% (4756/10048)\n",
      "Loss: 1.452 | Acc: 47.350% (4788/10112)\n",
      "Loss: 1.452 | Acc: 47.298% (4813/10176)\n",
      "Loss: 1.453 | Acc: 47.275% (4841/10240)\n",
      "Loss: 1.453 | Acc: 47.253% (4869/10304)\n",
      "Loss: 1.453 | Acc: 47.251% (4899/10368)\n",
      "Loss: 1.454 | Acc: 47.258% (4930/10432)\n",
      "Loss: 1.453 | Acc: 47.275% (4962/10496)\n",
      "Loss: 1.453 | Acc: 47.273% (4992/10560)\n",
      "Loss: 1.453 | Acc: 47.214% (5016/10624)\n",
      "Loss: 1.454 | Acc: 47.184% (5043/10688)\n",
      "Loss: 1.454 | Acc: 47.173% (5072/10752)\n",
      "Loss: 1.455 | Acc: 47.078% (5092/10816)\n",
      "Loss: 1.455 | Acc: 47.077% (5122/10880)\n",
      "Loss: 1.455 | Acc: 47.049% (5149/10944)\n",
      "Loss: 1.454 | Acc: 47.048% (5179/11008)\n",
      "Loss: 1.454 | Acc: 47.010% (5205/11072)\n",
      "Loss: 1.454 | Acc: 47.037% (5238/11136)\n",
      "Loss: 1.454 | Acc: 47.036% (5268/11200)\n",
      "Loss: 1.455 | Acc: 47.008% (5295/11264)\n",
      "Loss: 1.454 | Acc: 47.060% (5331/11328)\n",
      "Loss: 1.454 | Acc: 47.086% (5364/11392)\n",
      "Loss: 1.454 | Acc: 47.076% (5393/11456)\n",
      "Loss: 1.454 | Acc: 47.075% (5423/11520)\n",
      "Loss: 1.455 | Acc: 47.099% (5456/11584)\n",
      "Loss: 1.456 | Acc: 47.072% (5483/11648)\n",
      "Loss: 1.456 | Acc: 47.029% (5508/11712)\n",
      "Loss: 1.455 | Acc: 47.079% (5544/11776)\n",
      "Loss: 1.455 | Acc: 47.103% (5577/11840)\n",
      "Loss: 1.456 | Acc: 47.043% (5600/11904)\n",
      "Loss: 1.456 | Acc: 47.017% (5627/11968)\n",
      "Loss: 1.457 | Acc: 47.000% (5655/12032)\n",
      "Loss: 1.457 | Acc: 46.991% (5684/12096)\n",
      "Loss: 1.458 | Acc: 47.015% (5717/12160)\n",
      "Loss: 1.457 | Acc: 47.022% (5748/12224)\n",
      "Loss: 1.457 | Acc: 47.046% (5781/12288)\n",
      "Loss: 1.457 | Acc: 47.021% (5808/12352)\n",
      "Loss: 1.456 | Acc: 47.028% (5839/12416)\n",
      "Loss: 1.457 | Acc: 47.003% (5866/12480)\n",
      "Loss: 1.457 | Acc: 47.003% (5896/12544)\n",
      "Loss: 1.457 | Acc: 47.002% (5926/12608)\n",
      "Loss: 1.456 | Acc: 47.049% (5962/12672)\n",
      "Loss: 1.456 | Acc: 47.040% (5991/12736)\n",
      "Loss: 1.457 | Acc: 46.977% (6013/12800)\n",
      "Loss: 1.457 | Acc: 46.984% (6044/12864)\n",
      "Loss: 1.457 | Acc: 46.983% (6074/12928)\n",
      "Loss: 1.458 | Acc: 46.952% (6100/12992)\n",
      "Loss: 1.458 | Acc: 46.944% (6129/13056)\n",
      "Loss: 1.458 | Acc: 46.913% (6155/13120)\n",
      "Loss: 1.457 | Acc: 46.958% (6191/13184)\n",
      "Loss: 1.458 | Acc: 46.935% (6218/13248)\n",
      "Loss: 1.458 | Acc: 46.988% (6255/13312)\n",
      "Loss: 1.457 | Acc: 46.987% (6285/13376)\n",
      "Loss: 1.457 | Acc: 46.957% (6311/13440)\n",
      "Loss: 1.457 | Acc: 46.897% (6333/13504)\n",
      "Loss: 1.459 | Acc: 46.882% (6361/13568)\n",
      "Loss: 1.458 | Acc: 46.882% (6391/13632)\n",
      "Loss: 1.458 | Acc: 46.912% (6425/13696)\n",
      "Loss: 1.457 | Acc: 46.940% (6459/13760)\n",
      "Loss: 1.458 | Acc: 46.911% (6485/13824)\n",
      "Loss: 1.458 | Acc: 46.918% (6516/13888)\n",
      "Loss: 1.458 | Acc: 46.918% (6546/13952)\n",
      "Loss: 1.460 | Acc: 46.904% (6574/14016)\n",
      "Loss: 1.459 | Acc: 46.896% (6603/14080)\n",
      "Loss: 1.459 | Acc: 46.875% (6630/14144)\n",
      "Loss: 1.459 | Acc: 46.896% (6663/14208)\n",
      "Loss: 1.459 | Acc: 46.896% (6693/14272)\n",
      "Loss: 1.459 | Acc: 46.861% (6718/14336)\n",
      "Loss: 1.458 | Acc: 46.924% (6757/14400)\n",
      "Loss: 1.457 | Acc: 46.951% (6791/14464)\n",
      "Loss: 1.457 | Acc: 46.937% (6819/14528)\n",
      "Loss: 1.457 | Acc: 46.909% (6845/14592)\n",
      "Loss: 1.457 | Acc: 46.909% (6875/14656)\n",
      "Loss: 1.457 | Acc: 46.984% (6916/14720)\n",
      "Loss: 1.457 | Acc: 46.970% (6944/14784)\n",
      "Loss: 1.457 | Acc: 46.976% (6975/14848)\n",
      "Loss: 1.457 | Acc: 47.002% (7009/14912)\n",
      "Loss: 1.458 | Acc: 46.968% (7034/14976)\n",
      "Loss: 1.458 | Acc: 46.961% (7063/15040)\n",
      "Loss: 1.459 | Acc: 46.935% (7089/15104)\n",
      "Loss: 1.459 | Acc: 46.948% (7121/15168)\n",
      "Loss: 1.459 | Acc: 46.947% (7151/15232)\n",
      "Loss: 1.459 | Acc: 46.921% (7177/15296)\n",
      "Loss: 1.459 | Acc: 46.901% (7204/15360)\n",
      "Loss: 1.460 | Acc: 46.901% (7234/15424)\n",
      "Loss: 1.459 | Acc: 46.933% (7269/15488)\n",
      "Loss: 1.460 | Acc: 46.926% (7298/15552)\n",
      "Loss: 1.461 | Acc: 46.888% (7322/15616)\n",
      "Loss: 1.460 | Acc: 46.894% (7353/15680)\n",
      "Loss: 1.461 | Acc: 46.894% (7383/15744)\n",
      "Loss: 1.461 | Acc: 46.907% (7415/15808)\n",
      "Loss: 1.461 | Acc: 46.938% (7450/15872)\n",
      "Loss: 1.460 | Acc: 46.988% (7488/15936)\n",
      "Loss: 1.461 | Acc: 46.962% (7514/16000)\n",
      "Loss: 1.461 | Acc: 46.937% (7540/16064)\n",
      "Loss: 1.461 | Acc: 46.918% (7567/16128)\n",
      "Loss: 1.460 | Acc: 46.949% (7602/16192)\n",
      "Loss: 1.459 | Acc: 46.961% (7634/16256)\n",
      "Loss: 1.460 | Acc: 46.912% (7656/16320)\n",
      "Loss: 1.460 | Acc: 46.869% (7679/16384)\n",
      "Loss: 1.460 | Acc: 46.899% (7714/16448)\n",
      "Loss: 1.459 | Acc: 46.917% (7747/16512)\n",
      "Loss: 1.459 | Acc: 46.911% (7776/16576)\n",
      "Loss: 1.459 | Acc: 46.929% (7809/16640)\n",
      "Loss: 1.459 | Acc: 46.953% (7843/16704)\n",
      "Loss: 1.459 | Acc: 46.970% (7876/16768)\n",
      "Loss: 1.459 | Acc: 46.994% (7910/16832)\n",
      "Loss: 1.460 | Acc: 46.982% (7938/16896)\n",
      "Loss: 1.459 | Acc: 46.958% (7964/16960)\n",
      "Loss: 1.459 | Acc: 46.928% (7989/17024)\n",
      "Loss: 1.459 | Acc: 46.939% (8021/17088)\n",
      "Loss: 1.459 | Acc: 46.974% (8057/17152)\n",
      "Loss: 1.458 | Acc: 46.951% (8083/17216)\n",
      "Loss: 1.458 | Acc: 46.973% (8117/17280)\n",
      "Loss: 1.458 | Acc: 47.008% (8153/17344)\n",
      "Loss: 1.456 | Acc: 47.088% (8197/17408)\n",
      "Loss: 1.456 | Acc: 47.087% (8227/17472)\n",
      "Loss: 1.457 | Acc: 47.052% (8251/17536)\n",
      "Loss: 1.457 | Acc: 47.057% (8282/17600)\n",
      "Loss: 1.457 | Acc: 47.022% (8306/17664)\n",
      "Loss: 1.456 | Acc: 47.050% (8341/17728)\n",
      "Loss: 1.457 | Acc: 47.066% (8374/17792)\n",
      "Loss: 1.456 | Acc: 47.088% (8408/17856)\n",
      "Loss: 1.456 | Acc: 47.065% (8434/17920)\n",
      "Loss: 1.456 | Acc: 47.086% (8468/17984)\n",
      "Loss: 1.455 | Acc: 47.113% (8503/18048)\n",
      "Loss: 1.455 | Acc: 47.123% (8535/18112)\n",
      "Loss: 1.454 | Acc: 47.134% (8567/18176)\n",
      "Loss: 1.454 | Acc: 47.111% (8593/18240)\n",
      "Loss: 1.454 | Acc: 47.115% (8624/18304)\n",
      "Loss: 1.454 | Acc: 47.153% (8661/18368)\n",
      "Loss: 1.453 | Acc: 47.168% (8694/18432)\n",
      "Loss: 1.453 | Acc: 47.151% (8721/18496)\n",
      "Loss: 1.453 | Acc: 47.128% (8747/18560)\n",
      "Loss: 1.454 | Acc: 47.111% (8774/18624)\n",
      "Loss: 1.454 | Acc: 47.094% (8801/18688)\n",
      "Loss: 1.454 | Acc: 47.110% (8834/18752)\n",
      "Loss: 1.454 | Acc: 47.114% (8865/18816)\n",
      "Loss: 1.453 | Acc: 47.108% (8894/18880)\n",
      "Loss: 1.453 | Acc: 47.097% (8922/18944)\n",
      "Loss: 1.453 | Acc: 47.106% (8954/19008)\n",
      "Loss: 1.453 | Acc: 47.074% (8978/19072)\n",
      "Loss: 1.453 | Acc: 47.100% (9013/19136)\n",
      "Loss: 1.452 | Acc: 47.120% (9047/19200)\n",
      "Loss: 1.451 | Acc: 47.129% (9079/19264)\n",
      "Loss: 1.450 | Acc: 47.160% (9115/19328)\n",
      "Loss: 1.450 | Acc: 47.169% (9147/19392)\n",
      "Loss: 1.451 | Acc: 47.132% (9170/19456)\n",
      "Loss: 1.451 | Acc: 47.157% (9205/19520)\n",
      "Loss: 1.451 | Acc: 47.146% (9233/19584)\n",
      "Loss: 1.451 | Acc: 47.140% (9262/19648)\n",
      "Loss: 1.451 | Acc: 47.113% (9287/19712)\n",
      "Loss: 1.451 | Acc: 47.092% (9313/19776)\n",
      "Loss: 1.451 | Acc: 47.107% (9346/19840)\n",
      "Loss: 1.451 | Acc: 47.076% (9370/19904)\n",
      "Loss: 1.452 | Acc: 47.060% (9397/19968)\n",
      "Loss: 1.452 | Acc: 47.055% (9426/20032)\n",
      "Loss: 1.452 | Acc: 47.059% (9457/20096)\n",
      "Loss: 1.452 | Acc: 47.078% (9491/20160)\n",
      "Loss: 1.452 | Acc: 47.083% (9522/20224)\n",
      "Loss: 1.452 | Acc: 47.082% (9552/20288)\n",
      "Loss: 1.451 | Acc: 47.116% (9589/20352)\n",
      "Loss: 1.451 | Acc: 47.125% (9621/20416)\n",
      "Loss: 1.451 | Acc: 47.129% (9652/20480)\n",
      "Loss: 1.451 | Acc: 47.133% (9683/20544)\n",
      "Loss: 1.450 | Acc: 47.147% (9716/20608)\n",
      "Loss: 1.450 | Acc: 47.156% (9748/20672)\n",
      "Loss: 1.450 | Acc: 47.150% (9777/20736)\n",
      "Loss: 1.450 | Acc: 47.168% (9811/20800)\n",
      "Loss: 1.450 | Acc: 47.158% (9839/20864)\n",
      "Loss: 1.450 | Acc: 47.128% (9863/20928)\n",
      "Loss: 1.450 | Acc: 47.142% (9896/20992)\n",
      "Loss: 1.450 | Acc: 47.122% (9922/21056)\n",
      "Loss: 1.449 | Acc: 47.145% (9957/21120)\n",
      "Loss: 1.449 | Acc: 47.154% (9989/21184)\n",
      "Loss: 1.449 | Acc: 47.139% (10016/21248)\n",
      "Loss: 1.449 | Acc: 47.142% (10047/21312)\n",
      "Loss: 1.450 | Acc: 47.146% (10078/21376)\n",
      "Loss: 1.449 | Acc: 47.160% (10111/21440)\n",
      "Loss: 1.449 | Acc: 47.187% (10147/21504)\n",
      "Loss: 1.449 | Acc: 47.153% (10170/21568)\n",
      "Loss: 1.449 | Acc: 47.157% (10201/21632)\n",
      "Loss: 1.449 | Acc: 47.147% (10229/21696)\n",
      "Loss: 1.449 | Acc: 47.146% (10259/21760)\n",
      "Loss: 1.449 | Acc: 47.164% (10293/21824)\n",
      "Loss: 1.450 | Acc: 47.131% (10316/21888)\n",
      "Loss: 1.450 | Acc: 47.103% (10340/21952)\n",
      "Loss: 1.451 | Acc: 47.075% (10364/22016)\n",
      "Loss: 1.450 | Acc: 47.079% (10395/22080)\n",
      "Loss: 1.450 | Acc: 47.078% (10425/22144)\n",
      "Loss: 1.451 | Acc: 47.051% (10449/22208)\n",
      "Loss: 1.452 | Acc: 46.996% (10467/22272)\n",
      "Loss: 1.453 | Acc: 46.978% (10493/22336)\n",
      "Loss: 1.453 | Acc: 46.955% (10518/22400)\n",
      "Loss: 1.453 | Acc: 46.968% (10551/22464)\n",
      "Loss: 1.452 | Acc: 46.990% (10586/22528)\n",
      "Loss: 1.452 | Acc: 46.986% (10615/22592)\n",
      "Loss: 1.452 | Acc: 47.003% (10649/22656)\n",
      "Loss: 1.452 | Acc: 47.003% (10679/22720)\n",
      "Loss: 1.452 | Acc: 47.011% (10711/22784)\n",
      "Loss: 1.452 | Acc: 47.002% (10739/22848)\n",
      "Loss: 1.452 | Acc: 46.988% (10766/22912)\n",
      "Loss: 1.452 | Acc: 46.971% (10792/22976)\n",
      "Loss: 1.452 | Acc: 46.984% (10825/23040)\n",
      "Loss: 1.452 | Acc: 46.992% (10857/23104)\n",
      "Loss: 1.453 | Acc: 46.987% (10886/23168)\n",
      "Loss: 1.453 | Acc: 47.008% (10921/23232)\n",
      "Loss: 1.453 | Acc: 47.004% (10950/23296)\n",
      "Loss: 1.453 | Acc: 47.021% (10984/23360)\n",
      "Loss: 1.453 | Acc: 47.029% (11016/23424)\n",
      "Loss: 1.453 | Acc: 47.020% (11044/23488)\n",
      "Loss: 1.453 | Acc: 47.036% (11078/23552)\n",
      "Loss: 1.453 | Acc: 47.036% (11108/23616)\n",
      "Loss: 1.453 | Acc: 47.052% (11142/23680)\n",
      "Loss: 1.452 | Acc: 47.073% (11177/23744)\n",
      "Loss: 1.452 | Acc: 47.068% (11206/23808)\n",
      "Loss: 1.453 | Acc: 47.068% (11236/23872)\n",
      "Loss: 1.453 | Acc: 47.080% (11269/23936)\n",
      "Loss: 1.452 | Acc: 47.067% (11296/24000)\n",
      "Loss: 1.453 | Acc: 47.054% (11323/24064)\n",
      "Loss: 1.452 | Acc: 47.078% (11359/24128)\n",
      "Loss: 1.453 | Acc: 47.078% (11389/24192)\n",
      "Loss: 1.452 | Acc: 47.094% (11423/24256)\n",
      "Loss: 1.452 | Acc: 47.089% (11452/24320)\n",
      "Loss: 1.453 | Acc: 47.084% (11481/24384)\n",
      "Loss: 1.453 | Acc: 47.075% (11509/24448)\n",
      "Loss: 1.453 | Acc: 47.083% (11541/24512)\n",
      "Loss: 1.452 | Acc: 47.095% (11574/24576)\n",
      "Loss: 1.452 | Acc: 47.114% (11609/24640)\n",
      "Loss: 1.451 | Acc: 47.134% (11644/24704)\n",
      "Loss: 1.451 | Acc: 47.150% (11678/24768)\n",
      "Loss: 1.451 | Acc: 47.161% (11711/24832)\n",
      "Loss: 1.451 | Acc: 47.180% (11746/24896)\n",
      "Loss: 1.451 | Acc: 47.171% (11774/24960)\n",
      "Loss: 1.452 | Acc: 47.159% (11801/25024)\n",
      "Loss: 1.451 | Acc: 47.166% (11833/25088)\n",
      "Loss: 1.452 | Acc: 47.137% (11856/25152)\n",
      "Loss: 1.452 | Acc: 47.137% (11886/25216)\n",
      "Loss: 1.452 | Acc: 47.124% (11913/25280)\n",
      "Loss: 1.452 | Acc: 47.120% (11942/25344)\n",
      "Loss: 1.453 | Acc: 47.068% (11959/25408)\n",
      "Loss: 1.454 | Acc: 47.056% (11986/25472)\n",
      "Loss: 1.454 | Acc: 47.051% (12015/25536)\n",
      "Loss: 1.454 | Acc: 47.043% (12043/25600)\n",
      "Loss: 1.453 | Acc: 47.054% (12076/25664)\n",
      "Loss: 1.454 | Acc: 47.038% (12102/25728)\n",
      "Loss: 1.454 | Acc: 47.061% (12138/25792)\n",
      "Loss: 1.454 | Acc: 47.080% (12173/25856)\n",
      "Loss: 1.453 | Acc: 47.103% (12209/25920)\n",
      "Loss: 1.453 | Acc: 47.083% (12234/25984)\n",
      "Loss: 1.453 | Acc: 47.075% (12262/26048)\n",
      "Loss: 1.454 | Acc: 47.055% (12287/26112)\n",
      "Loss: 1.453 | Acc: 47.058% (12318/26176)\n",
      "Loss: 1.453 | Acc: 47.077% (12353/26240)\n",
      "Loss: 1.453 | Acc: 47.076% (12383/26304)\n",
      "Loss: 1.453 | Acc: 47.072% (12412/26368)\n",
      "Loss: 1.452 | Acc: 47.076% (12443/26432)\n",
      "Loss: 1.452 | Acc: 47.071% (12472/26496)\n",
      "Loss: 1.452 | Acc: 47.086% (12506/26560)\n",
      "Loss: 1.453 | Acc: 47.074% (12533/26624)\n",
      "Loss: 1.453 | Acc: 47.085% (12566/26688)\n",
      "Loss: 1.453 | Acc: 47.099% (12600/26752)\n",
      "Loss: 1.453 | Acc: 47.080% (12625/26816)\n",
      "Loss: 1.453 | Acc: 47.094% (12659/26880)\n",
      "Loss: 1.453 | Acc: 47.101% (12691/26944)\n",
      "Loss: 1.453 | Acc: 47.108% (12723/27008)\n",
      "Loss: 1.453 | Acc: 47.093% (12749/27072)\n",
      "Loss: 1.453 | Acc: 47.100% (12781/27136)\n",
      "Loss: 1.453 | Acc: 47.096% (12810/27200)\n",
      "Loss: 1.453 | Acc: 47.084% (12837/27264)\n",
      "Loss: 1.452 | Acc: 47.113% (12875/27328)\n",
      "Loss: 1.453 | Acc: 47.079% (12896/27392)\n",
      "Loss: 1.452 | Acc: 47.112% (12935/27456)\n",
      "Loss: 1.452 | Acc: 47.126% (12969/27520)\n",
      "Loss: 1.452 | Acc: 47.114% (12996/27584)\n",
      "Loss: 1.452 | Acc: 47.125% (13029/27648)\n",
      "Loss: 1.452 | Acc: 47.149% (13066/27712)\n",
      "Loss: 1.452 | Acc: 47.131% (13091/27776)\n",
      "Loss: 1.452 | Acc: 47.141% (13124/27840)\n",
      "Loss: 1.452 | Acc: 47.172% (13163/27904)\n",
      "Loss: 1.452 | Acc: 47.168% (13192/27968)\n",
      "Loss: 1.452 | Acc: 47.178% (13225/28032)\n",
      "Loss: 1.452 | Acc: 47.160% (13250/28096)\n",
      "Loss: 1.453 | Acc: 47.148% (13277/28160)\n",
      "Loss: 1.452 | Acc: 47.190% (13319/28224)\n",
      "Loss: 1.453 | Acc: 47.165% (13342/28288)\n",
      "Loss: 1.452 | Acc: 47.164% (13372/28352)\n",
      "Loss: 1.452 | Acc: 47.167% (13403/28416)\n",
      "Loss: 1.452 | Acc: 47.188% (13439/28480)\n",
      "Loss: 1.452 | Acc: 47.194% (13471/28544)\n",
      "Loss: 1.452 | Acc: 47.200% (13503/28608)\n",
      "Loss: 1.452 | Acc: 47.210% (13536/28672)\n",
      "Loss: 1.452 | Acc: 47.237% (13574/28736)\n",
      "Loss: 1.452 | Acc: 47.253% (13609/28800)\n",
      "Loss: 1.452 | Acc: 47.253% (13639/28864)\n",
      "Loss: 1.452 | Acc: 47.252% (13669/28928)\n",
      "Loss: 1.452 | Acc: 47.227% (13692/28992)\n",
      "Loss: 1.452 | Acc: 47.229% (13723/29056)\n",
      "Loss: 1.453 | Acc: 47.205% (13746/29120)\n",
      "Loss: 1.453 | Acc: 47.207% (13777/29184)\n",
      "Loss: 1.454 | Acc: 47.193% (13803/29248)\n",
      "Loss: 1.454 | Acc: 47.196% (13834/29312)\n",
      "Loss: 1.454 | Acc: 47.175% (13858/29376)\n",
      "Loss: 1.454 | Acc: 47.167% (13886/29440)\n",
      "Loss: 1.454 | Acc: 47.180% (13920/29504)\n",
      "Loss: 1.454 | Acc: 47.183% (13951/29568)\n",
      "Loss: 1.454 | Acc: 47.196% (13985/29632)\n",
      "Loss: 1.453 | Acc: 47.215% (14021/29696)\n",
      "Loss: 1.453 | Acc: 47.241% (14059/29760)\n",
      "Loss: 1.453 | Acc: 47.234% (14087/29824)\n",
      "Loss: 1.453 | Acc: 47.246% (14121/29888)\n",
      "Loss: 1.453 | Acc: 47.236% (14148/29952)\n",
      "Loss: 1.453 | Acc: 47.231% (14177/30016)\n",
      "Loss: 1.453 | Acc: 47.224% (14205/30080)\n",
      "Loss: 1.454 | Acc: 47.213% (14232/30144)\n",
      "Loss: 1.453 | Acc: 47.209% (14261/30208)\n",
      "Loss: 1.453 | Acc: 47.212% (14292/30272)\n",
      "Loss: 1.453 | Acc: 47.224% (14326/30336)\n",
      "Loss: 1.453 | Acc: 47.220% (14355/30400)\n",
      "Loss: 1.453 | Acc: 47.226% (14387/30464)\n",
      "Loss: 1.453 | Acc: 47.216% (14414/30528)\n",
      "Loss: 1.453 | Acc: 47.212% (14443/30592)\n",
      "Loss: 1.453 | Acc: 47.208% (14472/30656)\n",
      "Loss: 1.453 | Acc: 47.210% (14503/30720)\n",
      "Loss: 1.453 | Acc: 47.206% (14532/30784)\n",
      "Loss: 1.453 | Acc: 47.202% (14561/30848)\n",
      "Loss: 1.453 | Acc: 47.231% (14600/30912)\n",
      "Loss: 1.453 | Acc: 47.243% (14634/30976)\n",
      "Loss: 1.453 | Acc: 47.262% (14670/31040)\n",
      "Loss: 1.453 | Acc: 47.270% (14703/31104)\n",
      "Loss: 1.452 | Acc: 47.279% (14736/31168)\n",
      "Loss: 1.452 | Acc: 47.282% (14767/31232)\n",
      "Loss: 1.452 | Acc: 47.281% (14797/31296)\n",
      "Loss: 1.452 | Acc: 47.296% (14832/31360)\n",
      "Loss: 1.453 | Acc: 47.286% (14859/31424)\n",
      "Loss: 1.452 | Acc: 47.288% (14890/31488)\n",
      "Loss: 1.452 | Acc: 47.300% (14924/31552)\n",
      "Loss: 1.452 | Acc: 47.299% (14954/31616)\n",
      "Loss: 1.452 | Acc: 47.276% (14977/31680)\n",
      "Loss: 1.452 | Acc: 47.297% (15014/31744)\n",
      "Loss: 1.452 | Acc: 47.309% (15048/31808)\n",
      "Loss: 1.452 | Acc: 47.289% (15072/31872)\n",
      "Loss: 1.452 | Acc: 47.288% (15102/31936)\n",
      "Loss: 1.452 | Acc: 47.297% (15135/32000)\n",
      "Loss: 1.451 | Acc: 47.305% (15168/32064)\n",
      "Loss: 1.451 | Acc: 47.305% (15198/32128)\n",
      "Loss: 1.451 | Acc: 47.297% (15226/32192)\n",
      "Loss: 1.451 | Acc: 47.287% (15253/32256)\n",
      "Loss: 1.451 | Acc: 47.290% (15284/32320)\n",
      "Loss: 1.451 | Acc: 47.289% (15314/32384)\n",
      "Loss: 1.451 | Acc: 47.310% (15351/32448)\n",
      "Loss: 1.451 | Acc: 47.321% (15385/32512)\n",
      "Loss: 1.451 | Acc: 47.320% (15415/32576)\n",
      "Loss: 1.450 | Acc: 47.331% (15449/32640)\n",
      "Loss: 1.451 | Acc: 47.309% (15472/32704)\n",
      "Loss: 1.451 | Acc: 47.324% (15507/32768)\n",
      "Loss: 1.450 | Acc: 47.332% (15540/32832)\n",
      "Loss: 1.451 | Acc: 47.325% (15568/32896)\n",
      "Loss: 1.451 | Acc: 47.312% (15594/32960)\n",
      "Loss: 1.451 | Acc: 47.296% (15619/33024)\n",
      "Loss: 1.451 | Acc: 47.307% (15653/33088)\n",
      "Loss: 1.451 | Acc: 47.303% (15682/33152)\n",
      "Loss: 1.451 | Acc: 47.315% (15716/33216)\n",
      "Loss: 1.451 | Acc: 47.311% (15745/33280)\n",
      "Loss: 1.451 | Acc: 47.304% (15773/33344)\n",
      "Loss: 1.451 | Acc: 47.330% (15812/33408)\n",
      "Loss: 1.451 | Acc: 47.326% (15841/33472)\n",
      "Loss: 1.451 | Acc: 47.328% (15872/33536)\n",
      "Loss: 1.451 | Acc: 47.315% (15898/33600)\n",
      "Loss: 1.451 | Acc: 47.312% (15927/33664)\n",
      "Loss: 1.451 | Acc: 47.299% (15953/33728)\n",
      "Loss: 1.451 | Acc: 47.325% (15992/33792)\n",
      "Loss: 1.451 | Acc: 47.327% (16023/33856)\n",
      "Loss: 1.451 | Acc: 47.320% (16051/33920)\n",
      "Loss: 1.451 | Acc: 47.310% (16078/33984)\n",
      "Loss: 1.451 | Acc: 47.301% (16105/34048)\n",
      "Loss: 1.451 | Acc: 47.294% (16133/34112)\n",
      "Loss: 1.451 | Acc: 47.293% (16163/34176)\n",
      "Loss: 1.451 | Acc: 47.296% (16194/34240)\n",
      "Loss: 1.451 | Acc: 47.306% (16228/34304)\n",
      "Loss: 1.451 | Acc: 47.294% (16254/34368)\n",
      "Loss: 1.450 | Acc: 47.322% (16294/34432)\n",
      "Loss: 1.450 | Acc: 47.313% (16321/34496)\n",
      "Loss: 1.450 | Acc: 47.338% (16360/34560)\n",
      "Loss: 1.450 | Acc: 47.343% (16392/34624)\n",
      "Loss: 1.450 | Acc: 47.333% (16419/34688)\n",
      "Loss: 1.450 | Acc: 47.361% (16459/34752)\n",
      "Loss: 1.449 | Acc: 47.363% (16490/34816)\n",
      "Loss: 1.449 | Acc: 47.365% (16521/34880)\n",
      "Loss: 1.450 | Acc: 47.350% (16546/34944)\n",
      "Loss: 1.450 | Acc: 47.349% (16576/35008)\n",
      "Loss: 1.450 | Acc: 47.360% (16610/35072)\n",
      "Loss: 1.450 | Acc: 47.370% (16644/35136)\n",
      "Loss: 1.450 | Acc: 47.378% (16677/35200)\n",
      "Loss: 1.449 | Acc: 47.388% (16711/35264)\n",
      "Loss: 1.449 | Acc: 47.387% (16741/35328)\n",
      "Loss: 1.449 | Acc: 47.389% (16772/35392)\n",
      "Loss: 1.449 | Acc: 47.377% (16798/35456)\n",
      "Loss: 1.450 | Acc: 47.362% (16823/35520)\n",
      "Loss: 1.449 | Acc: 47.381% (16860/35584)\n",
      "Loss: 1.449 | Acc: 47.388% (16893/35648)\n",
      "Loss: 1.449 | Acc: 47.393% (16925/35712)\n",
      "Loss: 1.449 | Acc: 47.387% (16953/35776)\n",
      "Loss: 1.449 | Acc: 47.377% (16980/35840)\n",
      "Loss: 1.449 | Acc: 47.379% (17011/35904)\n",
      "Loss: 1.449 | Acc: 47.395% (17047/35968)\n",
      "Loss: 1.449 | Acc: 47.394% (17077/36032)\n",
      "Loss: 1.449 | Acc: 47.412% (17114/36096)\n",
      "Loss: 1.449 | Acc: 47.406% (17142/36160)\n",
      "Loss: 1.448 | Acc: 47.419% (17177/36224)\n",
      "Loss: 1.448 | Acc: 47.415% (17206/36288)\n",
      "Loss: 1.448 | Acc: 47.417% (17237/36352)\n",
      "Loss: 1.448 | Acc: 47.452% (17280/36416)\n",
      "Loss: 1.448 | Acc: 47.470% (17317/36480)\n",
      "Loss: 1.448 | Acc: 47.469% (17347/36544)\n",
      "Loss: 1.447 | Acc: 47.468% (17377/36608)\n",
      "Loss: 1.448 | Acc: 47.464% (17406/36672)\n",
      "Loss: 1.448 | Acc: 47.452% (17432/36736)\n",
      "Loss: 1.448 | Acc: 47.462% (17466/36800)\n",
      "Loss: 1.447 | Acc: 47.461% (17496/36864)\n",
      "Loss: 1.448 | Acc: 47.452% (17523/36928)\n",
      "Loss: 1.447 | Acc: 47.456% (17555/36992)\n",
      "Loss: 1.448 | Acc: 47.453% (17584/37056)\n",
      "Loss: 1.448 | Acc: 47.446% (17612/37120)\n",
      "Loss: 1.448 | Acc: 47.451% (17644/37184)\n",
      "Loss: 1.448 | Acc: 47.463% (17679/37248)\n",
      "Loss: 1.447 | Acc: 47.475% (17714/37312)\n",
      "Loss: 1.448 | Acc: 47.456% (17737/37376)\n",
      "Loss: 1.448 | Acc: 47.455% (17767/37440)\n",
      "Loss: 1.448 | Acc: 47.454% (17797/37504)\n",
      "Loss: 1.448 | Acc: 47.450% (17826/37568)\n",
      "Loss: 1.448 | Acc: 47.460% (17860/37632)\n",
      "Loss: 1.447 | Acc: 47.482% (17899/37696)\n",
      "Loss: 1.447 | Acc: 47.474% (17926/37760)\n",
      "Loss: 1.447 | Acc: 47.467% (17954/37824)\n",
      "Loss: 1.447 | Acc: 47.469% (17985/37888)\n",
      "Loss: 1.447 | Acc: 47.465% (18014/37952)\n",
      "Loss: 1.447 | Acc: 47.467% (18045/38016)\n",
      "Loss: 1.447 | Acc: 47.471% (18077/38080)\n",
      "Loss: 1.447 | Acc: 47.465% (18105/38144)\n",
      "Loss: 1.446 | Acc: 47.474% (18139/38208)\n",
      "Loss: 1.447 | Acc: 47.468% (18167/38272)\n",
      "Loss: 1.446 | Acc: 47.478% (18201/38336)\n",
      "Loss: 1.446 | Acc: 47.503% (18241/38400)\n",
      "Loss: 1.446 | Acc: 47.504% (18272/38464)\n",
      "Loss: 1.446 | Acc: 47.506% (18303/38528)\n",
      "Loss: 1.446 | Acc: 47.510% (18335/38592)\n",
      "Loss: 1.445 | Acc: 47.511% (18366/38656)\n",
      "Loss: 1.445 | Acc: 47.503% (18393/38720)\n",
      "Loss: 1.445 | Acc: 47.507% (18425/38784)\n",
      "Loss: 1.445 | Acc: 47.511% (18457/38848)\n",
      "Loss: 1.445 | Acc: 47.510% (18487/38912)\n",
      "Loss: 1.445 | Acc: 47.509% (18517/38976)\n",
      "Loss: 1.445 | Acc: 47.510% (18548/39040)\n",
      "Loss: 1.445 | Acc: 47.519% (18582/39104)\n",
      "Loss: 1.445 | Acc: 47.508% (18608/39168)\n",
      "Loss: 1.445 | Acc: 47.497% (18634/39232)\n",
      "Loss: 1.445 | Acc: 47.498% (18665/39296)\n",
      "Loss: 1.445 | Acc: 47.503% (18697/39360)\n",
      "Loss: 1.445 | Acc: 47.512% (18731/39424)\n",
      "Loss: 1.445 | Acc: 47.533% (18770/39488)\n",
      "Loss: 1.445 | Acc: 47.535% (18801/39552)\n",
      "Loss: 1.445 | Acc: 47.546% (18836/39616)\n",
      "Loss: 1.445 | Acc: 47.548% (18867/39680)\n",
      "Loss: 1.444 | Acc: 47.562% (18903/39744)\n",
      "Loss: 1.444 | Acc: 47.563% (18934/39808)\n",
      "Loss: 1.444 | Acc: 47.570% (18967/39872)\n",
      "Loss: 1.444 | Acc: 47.576% (19000/39936)\n",
      "Loss: 1.444 | Acc: 47.587% (19035/40000)\n",
      "Loss: 1.443 | Acc: 47.594% (19068/40064)\n",
      "Loss: 1.443 | Acc: 47.610% (19105/40128)\n",
      "Loss: 1.443 | Acc: 47.606% (19134/40192)\n",
      "Loss: 1.443 | Acc: 47.615% (19168/40256)\n",
      "Loss: 1.443 | Acc: 47.629% (19204/40320)\n",
      "Loss: 1.442 | Acc: 47.640% (19239/40384)\n",
      "Loss: 1.443 | Acc: 47.641% (19270/40448)\n",
      "Loss: 1.442 | Acc: 47.657% (19307/40512)\n",
      "Loss: 1.442 | Acc: 47.676% (19345/40576)\n",
      "Loss: 1.442 | Acc: 47.670% (19373/40640)\n",
      "Loss: 1.442 | Acc: 47.651% (19396/40704)\n",
      "Loss: 1.442 | Acc: 47.650% (19426/40768)\n",
      "Loss: 1.442 | Acc: 47.664% (19462/40832)\n",
      "Loss: 1.442 | Acc: 47.662% (19492/40896)\n",
      "Loss: 1.443 | Acc: 47.654% (19519/40960)\n",
      "Loss: 1.442 | Acc: 47.657% (19551/41024)\n",
      "Loss: 1.442 | Acc: 47.671% (19587/41088)\n",
      "Loss: 1.442 | Acc: 47.677% (19620/41152)\n",
      "Loss: 1.442 | Acc: 47.678% (19651/41216)\n",
      "Loss: 1.442 | Acc: 47.679% (19682/41280)\n",
      "Loss: 1.441 | Acc: 47.702% (19722/41344)\n",
      "Loss: 1.441 | Acc: 47.701% (19752/41408)\n",
      "Loss: 1.441 | Acc: 47.704% (19784/41472)\n",
      "Loss: 1.441 | Acc: 47.715% (19819/41536)\n",
      "Loss: 1.442 | Acc: 47.697% (19842/41600)\n",
      "Loss: 1.442 | Acc: 47.681% (19866/41664)\n",
      "Loss: 1.442 | Acc: 47.685% (19898/41728)\n",
      "Loss: 1.441 | Acc: 47.693% (19932/41792)\n",
      "Loss: 1.441 | Acc: 47.702% (19966/41856)\n",
      "Loss: 1.442 | Acc: 47.688% (19991/41920)\n",
      "Loss: 1.441 | Acc: 47.699% (20026/41984)\n",
      "Loss: 1.441 | Acc: 47.693% (20054/42048)\n",
      "Loss: 1.441 | Acc: 47.687% (20082/42112)\n",
      "Loss: 1.441 | Acc: 47.702% (20119/42176)\n",
      "Loss: 1.441 | Acc: 47.704% (20150/42240)\n",
      "Loss: 1.441 | Acc: 47.724% (20189/42304)\n",
      "Loss: 1.441 | Acc: 47.727% (20221/42368)\n",
      "Loss: 1.441 | Acc: 47.721% (20249/42432)\n",
      "Loss: 1.441 | Acc: 47.724% (20281/42496)\n",
      "Loss: 1.441 | Acc: 47.740% (20318/42560)\n",
      "Loss: 1.441 | Acc: 47.741% (20349/42624)\n",
      "Loss: 1.440 | Acc: 47.735% (20377/42688)\n",
      "Loss: 1.440 | Acc: 47.754% (20416/42752)\n",
      "Loss: 1.440 | Acc: 47.749% (20444/42816)\n",
      "Loss: 1.440 | Acc: 47.752% (20476/42880)\n",
      "Loss: 1.440 | Acc: 47.739% (20501/42944)\n",
      "Loss: 1.440 | Acc: 47.745% (20534/43008)\n",
      "Loss: 1.440 | Acc: 47.748% (20566/43072)\n",
      "Loss: 1.440 | Acc: 47.747% (20596/43136)\n",
      "Loss: 1.440 | Acc: 47.745% (20626/43200)\n",
      "Loss: 1.440 | Acc: 47.739% (20654/43264)\n",
      "Loss: 1.440 | Acc: 47.738% (20684/43328)\n",
      "Loss: 1.440 | Acc: 47.755% (20722/43392)\n",
      "Loss: 1.440 | Acc: 47.766% (20757/43456)\n",
      "Loss: 1.440 | Acc: 47.767% (20788/43520)\n",
      "Loss: 1.439 | Acc: 47.779% (20824/43584)\n",
      "Loss: 1.439 | Acc: 47.785% (20857/43648)\n",
      "Loss: 1.439 | Acc: 47.781% (20886/43712)\n",
      "Loss: 1.439 | Acc: 47.784% (20918/43776)\n",
      "Loss: 1.439 | Acc: 47.792% (20952/43840)\n",
      "Loss: 1.439 | Acc: 47.804% (20988/43904)\n",
      "Loss: 1.439 | Acc: 47.803% (21018/43968)\n",
      "Loss: 1.439 | Acc: 47.793% (21044/44032)\n",
      "Loss: 1.438 | Acc: 47.800% (21078/44096)\n",
      "Loss: 1.438 | Acc: 47.801% (21109/44160)\n",
      "Loss: 1.438 | Acc: 47.811% (21144/44224)\n",
      "Loss: 1.438 | Acc: 47.832% (21184/44288)\n",
      "Loss: 1.437 | Acc: 47.835% (21216/44352)\n",
      "Loss: 1.437 | Acc: 47.854% (21255/44416)\n",
      "Loss: 1.437 | Acc: 47.873% (21294/44480)\n",
      "Loss: 1.437 | Acc: 47.890% (21332/44544)\n",
      "Loss: 1.436 | Acc: 47.886% (21361/44608)\n",
      "Loss: 1.436 | Acc: 47.885% (21391/44672)\n",
      "Loss: 1.437 | Acc: 47.874% (21417/44736)\n",
      "Loss: 1.437 | Acc: 47.868% (21445/44800)\n",
      "Loss: 1.436 | Acc: 47.880% (21481/44864)\n",
      "Loss: 1.436 | Acc: 47.883% (21513/44928)\n",
      "Loss: 1.436 | Acc: 47.877% (21541/44992)\n",
      "Loss: 1.436 | Acc: 47.880% (21573/45056)\n",
      "Loss: 1.436 | Acc: 47.890% (21608/45120)\n",
      "Loss: 1.436 | Acc: 47.904% (21645/45184)\n",
      "Loss: 1.436 | Acc: 47.912% (21679/45248)\n",
      "Loss: 1.436 | Acc: 47.917% (21712/45312)\n",
      "Loss: 1.436 | Acc: 47.926% (21747/45376)\n",
      "Loss: 1.436 | Acc: 47.945% (21786/45440)\n",
      "Loss: 1.436 | Acc: 47.950% (21819/45504)\n",
      "Loss: 1.436 | Acc: 47.959% (21854/45568)\n",
      "Loss: 1.436 | Acc: 47.964% (21887/45632)\n",
      "Loss: 1.436 | Acc: 47.952% (21912/45696)\n",
      "Loss: 1.435 | Acc: 47.965% (21949/45760)\n",
      "Loss: 1.435 | Acc: 47.973% (21983/45824)\n",
      "Loss: 1.435 | Acc: 47.967% (22011/45888)\n",
      "Loss: 1.435 | Acc: 47.965% (22041/45952)\n",
      "Loss: 1.435 | Acc: 47.968% (22073/46016)\n",
      "Loss: 1.435 | Acc: 47.984% (22111/46080)\n",
      "Loss: 1.435 | Acc: 47.982% (22141/46144)\n",
      "Loss: 1.435 | Acc: 47.981% (22171/46208)\n",
      "Loss: 1.435 | Acc: 47.988% (22205/46272)\n",
      "Loss: 1.435 | Acc: 47.989% (22236/46336)\n",
      "Loss: 1.435 | Acc: 48.002% (22273/46400)\n",
      "Loss: 1.434 | Acc: 48.009% (22307/46464)\n",
      "Loss: 1.434 | Acc: 48.006% (22336/46528)\n",
      "Loss: 1.434 | Acc: 48.021% (22374/46592)\n",
      "Loss: 1.434 | Acc: 48.015% (22402/46656)\n",
      "Loss: 1.434 | Acc: 48.009% (22430/46720)\n",
      "Loss: 1.434 | Acc: 48.001% (22457/46784)\n",
      "Loss: 1.434 | Acc: 48.002% (22488/46848)\n",
      "Loss: 1.434 | Acc: 47.992% (22514/46912)\n",
      "Loss: 1.434 | Acc: 47.982% (22540/46976)\n",
      "Loss: 1.434 | Acc: 47.989% (22574/47040)\n",
      "Loss: 1.435 | Acc: 47.981% (22601/47104)\n",
      "Loss: 1.435 | Acc: 47.988% (22635/47168)\n",
      "Loss: 1.435 | Acc: 47.987% (22665/47232)\n",
      "Loss: 1.435 | Acc: 47.989% (22697/47296)\n",
      "Loss: 1.434 | Acc: 47.994% (22730/47360)\n",
      "Loss: 1.434 | Acc: 47.988% (22758/47424)\n",
      "Loss: 1.434 | Acc: 47.997% (22793/47488)\n",
      "Loss: 1.435 | Acc: 47.981% (22816/47552)\n",
      "Loss: 1.435 | Acc: 47.969% (22841/47616)\n",
      "Loss: 1.435 | Acc: 47.982% (22878/47680)\n",
      "Loss: 1.435 | Acc: 47.981% (22908/47744)\n",
      "Loss: 1.435 | Acc: 47.977% (22937/47808)\n",
      "Loss: 1.434 | Acc: 47.993% (22975/47872)\n",
      "Loss: 1.434 | Acc: 47.987% (23003/47936)\n",
      "Loss: 1.434 | Acc: 47.983% (23032/48000)\n",
      "Loss: 1.434 | Acc: 47.992% (23067/48064)\n",
      "Loss: 1.434 | Acc: 48.003% (23103/48128)\n",
      "Loss: 1.434 | Acc: 48.014% (23139/48192)\n",
      "Loss: 1.433 | Acc: 48.023% (23174/48256)\n",
      "Loss: 1.433 | Acc: 48.013% (23200/48320)\n",
      "Loss: 1.433 | Acc: 48.022% (23235/48384)\n",
      "Loss: 1.433 | Acc: 48.033% (23271/48448)\n",
      "Loss: 1.433 | Acc: 48.036% (23303/48512)\n",
      "Loss: 1.433 | Acc: 48.042% (23337/48576)\n",
      "Loss: 1.433 | Acc: 48.037% (23365/48640)\n",
      "Loss: 1.433 | Acc: 48.023% (23389/48704)\n",
      "Loss: 1.433 | Acc: 48.042% (23429/48768)\n",
      "Loss: 1.433 | Acc: 48.034% (23456/48832)\n",
      "Loss: 1.432 | Acc: 48.033% (23486/48896)\n",
      "Loss: 1.433 | Acc: 48.025% (23513/48960)\n",
      "Loss: 1.433 | Acc: 48.024% (23532/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 48.02448979591837\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.344 | Acc: 46.875% (30/64)\n",
      "Loss: 1.293 | Acc: 47.656% (61/128)\n",
      "Loss: 1.349 | Acc: 46.875% (90/192)\n",
      "Loss: 1.428 | Acc: 45.703% (117/256)\n",
      "Loss: 1.387 | Acc: 48.438% (155/320)\n",
      "Loss: 1.406 | Acc: 48.438% (186/384)\n",
      "Loss: 1.428 | Acc: 47.321% (212/448)\n",
      "Loss: 1.407 | Acc: 47.070% (241/512)\n",
      "Loss: 1.379 | Acc: 48.090% (277/576)\n",
      "Loss: 1.375 | Acc: 48.281% (309/640)\n",
      "Loss: 1.397 | Acc: 48.153% (339/704)\n",
      "Loss: 1.397 | Acc: 48.177% (370/768)\n",
      "Loss: 1.408 | Acc: 47.837% (398/832)\n",
      "Loss: 1.411 | Acc: 48.214% (432/896)\n",
      "Loss: 1.399 | Acc: 48.646% (467/960)\n",
      "Loss: 1.384 | Acc: 49.219% (504/1024)\n",
      "Loss: 1.375 | Acc: 49.449% (538/1088)\n",
      "Loss: 1.364 | Acc: 49.913% (575/1152)\n",
      "Loss: 1.360 | Acc: 50.329% (612/1216)\n",
      "Loss: 1.373 | Acc: 49.688% (636/1280)\n",
      "Loss: 1.375 | Acc: 49.851% (670/1344)\n",
      "Loss: 1.375 | Acc: 49.929% (703/1408)\n",
      "Loss: 1.370 | Acc: 49.932% (735/1472)\n",
      "Loss: 1.378 | Acc: 49.805% (765/1536)\n",
      "Loss: 1.380 | Acc: 49.812% (797/1600)\n",
      "Loss: 1.384 | Acc: 50.000% (832/1664)\n",
      "Loss: 1.383 | Acc: 49.942% (863/1728)\n",
      "Loss: 1.382 | Acc: 49.609% (889/1792)\n",
      "Loss: 1.379 | Acc: 49.838% (925/1856)\n",
      "Loss: 1.384 | Acc: 49.792% (956/1920)\n",
      "Loss: 1.385 | Acc: 49.899% (990/1984)\n",
      "Loss: 1.389 | Acc: 49.707% (1018/2048)\n",
      "Loss: 1.382 | Acc: 49.621% (1048/2112)\n",
      "Loss: 1.382 | Acc: 49.494% (1077/2176)\n",
      "Loss: 1.382 | Acc: 49.554% (1110/2240)\n",
      "Loss: 1.382 | Acc: 49.609% (1143/2304)\n",
      "Loss: 1.378 | Acc: 49.873% (1181/2368)\n",
      "Loss: 1.381 | Acc: 49.959% (1215/2432)\n",
      "Loss: 1.377 | Acc: 50.120% (1251/2496)\n",
      "Loss: 1.381 | Acc: 49.961% (1279/2560)\n",
      "Loss: 1.383 | Acc: 49.771% (1306/2624)\n",
      "Loss: 1.385 | Acc: 49.665% (1335/2688)\n",
      "Loss: 1.384 | Acc: 49.600% (1365/2752)\n",
      "Loss: 1.386 | Acc: 49.645% (1398/2816)\n",
      "Loss: 1.384 | Acc: 49.861% (1436/2880)\n",
      "Loss: 1.382 | Acc: 50.000% (1472/2944)\n",
      "Loss: 1.383 | Acc: 49.900% (1501/3008)\n",
      "Loss: 1.379 | Acc: 50.098% (1539/3072)\n",
      "Loss: 1.377 | Acc: 50.223% (1575/3136)\n",
      "Loss: 1.376 | Acc: 50.312% (1610/3200)\n",
      "Loss: 1.375 | Acc: 50.306% (1642/3264)\n",
      "Loss: 1.377 | Acc: 50.270% (1673/3328)\n",
      "Loss: 1.378 | Acc: 50.206% (1703/3392)\n",
      "Loss: 1.379 | Acc: 50.174% (1734/3456)\n",
      "Loss: 1.379 | Acc: 50.114% (1764/3520)\n",
      "Loss: 1.377 | Acc: 50.251% (1801/3584)\n",
      "Loss: 1.377 | Acc: 50.302% (1835/3648)\n",
      "Loss: 1.376 | Acc: 50.323% (1868/3712)\n",
      "Loss: 1.376 | Acc: 50.318% (1900/3776)\n",
      "Loss: 1.376 | Acc: 50.260% (1930/3840)\n",
      "Loss: 1.374 | Acc: 50.333% (1965/3904)\n",
      "Loss: 1.372 | Acc: 50.378% (1999/3968)\n",
      "Loss: 1.370 | Acc: 50.471% (2035/4032)\n",
      "Loss: 1.372 | Acc: 50.464% (2067/4096)\n",
      "Loss: 1.374 | Acc: 50.409% (2097/4160)\n",
      "Loss: 1.372 | Acc: 50.497% (2133/4224)\n",
      "Loss: 1.372 | Acc: 50.513% (2166/4288)\n",
      "Loss: 1.371 | Acc: 50.597% (2202/4352)\n",
      "Loss: 1.368 | Acc: 50.770% (2242/4416)\n",
      "Loss: 1.367 | Acc: 50.848% (2278/4480)\n",
      "Loss: 1.368 | Acc: 50.748% (2306/4544)\n",
      "Loss: 1.371 | Acc: 50.564% (2330/4608)\n",
      "Loss: 1.370 | Acc: 50.664% (2367/4672)\n",
      "Loss: 1.368 | Acc: 50.760% (2404/4736)\n",
      "Loss: 1.369 | Acc: 50.771% (2437/4800)\n",
      "Loss: 1.368 | Acc: 50.761% (2469/4864)\n",
      "Loss: 1.368 | Acc: 50.812% (2504/4928)\n",
      "Loss: 1.370 | Acc: 50.761% (2534/4992)\n",
      "Loss: 1.370 | Acc: 50.692% (2563/5056)\n",
      "Loss: 1.372 | Acc: 50.605% (2591/5120)\n",
      "Loss: 1.371 | Acc: 50.656% (2626/5184)\n",
      "Loss: 1.372 | Acc: 50.553% (2653/5248)\n",
      "Loss: 1.371 | Acc: 50.565% (2686/5312)\n",
      "Loss: 1.373 | Acc: 50.428% (2711/5376)\n",
      "Loss: 1.373 | Acc: 50.349% (2739/5440)\n",
      "Loss: 1.374 | Acc: 50.345% (2771/5504)\n",
      "Loss: 1.377 | Acc: 50.305% (2801/5568)\n",
      "Loss: 1.379 | Acc: 50.337% (2835/5632)\n",
      "Loss: 1.380 | Acc: 50.298% (2865/5696)\n",
      "Loss: 1.377 | Acc: 50.382% (2902/5760)\n",
      "Loss: 1.378 | Acc: 50.395% (2935/5824)\n",
      "Loss: 1.378 | Acc: 50.374% (2966/5888)\n",
      "Loss: 1.379 | Acc: 50.269% (2992/5952)\n",
      "Loss: 1.380 | Acc: 50.233% (3022/6016)\n",
      "Loss: 1.381 | Acc: 50.148% (3049/6080)\n",
      "Loss: 1.382 | Acc: 50.081% (3077/6144)\n",
      "Loss: 1.384 | Acc: 50.000% (3104/6208)\n",
      "Loss: 1.385 | Acc: 49.904% (3130/6272)\n",
      "Loss: 1.383 | Acc: 49.984% (3167/6336)\n",
      "Loss: 1.384 | Acc: 49.891% (3193/6400)\n",
      "Loss: 1.386 | Acc: 49.845% (3222/6464)\n",
      "Loss: 1.386 | Acc: 49.877% (3256/6528)\n",
      "Loss: 1.388 | Acc: 49.772% (3281/6592)\n",
      "Loss: 1.389 | Acc: 49.669% (3306/6656)\n",
      "Loss: 1.390 | Acc: 49.628% (3335/6720)\n",
      "Loss: 1.388 | Acc: 49.690% (3371/6784)\n",
      "Loss: 1.386 | Acc: 49.737% (3406/6848)\n",
      "Loss: 1.388 | Acc: 49.725% (3437/6912)\n",
      "Loss: 1.390 | Acc: 49.656% (3464/6976)\n",
      "Loss: 1.392 | Acc: 49.560% (3489/7040)\n",
      "Loss: 1.391 | Acc: 49.564% (3521/7104)\n",
      "Loss: 1.391 | Acc: 49.581% (3554/7168)\n",
      "Loss: 1.393 | Acc: 49.544% (3583/7232)\n",
      "Loss: 1.393 | Acc: 49.479% (3610/7296)\n",
      "Loss: 1.392 | Acc: 49.429% (3638/7360)\n",
      "Loss: 1.391 | Acc: 49.434% (3670/7424)\n",
      "Loss: 1.390 | Acc: 49.466% (3704/7488)\n",
      "Loss: 1.389 | Acc: 49.563% (3743/7552)\n",
      "Loss: 1.389 | Acc: 49.475% (3768/7616)\n",
      "Loss: 1.388 | Acc: 49.531% (3804/7680)\n",
      "Loss: 1.387 | Acc: 49.548% (3837/7744)\n",
      "Loss: 1.387 | Acc: 49.603% (3873/7808)\n",
      "Loss: 1.387 | Acc: 49.619% (3906/7872)\n",
      "Loss: 1.385 | Acc: 49.622% (3938/7936)\n",
      "Loss: 1.386 | Acc: 49.575% (3966/8000)\n",
      "Loss: 1.386 | Acc: 49.578% (3998/8064)\n",
      "Loss: 1.386 | Acc: 49.619% (4033/8128)\n",
      "Loss: 1.385 | Acc: 49.646% (4067/8192)\n",
      "Loss: 1.386 | Acc: 49.612% (4096/8256)\n",
      "Loss: 1.389 | Acc: 49.507% (4119/8320)\n",
      "Loss: 1.388 | Acc: 49.523% (4152/8384)\n",
      "Loss: 1.388 | Acc: 49.455% (4178/8448)\n",
      "Loss: 1.391 | Acc: 49.377% (4203/8512)\n",
      "Loss: 1.391 | Acc: 49.359% (4233/8576)\n",
      "Loss: 1.393 | Acc: 49.317% (4261/8640)\n",
      "Loss: 1.394 | Acc: 49.311% (4292/8704)\n",
      "Loss: 1.394 | Acc: 49.339% (4326/8768)\n",
      "Loss: 1.394 | Acc: 49.366% (4360/8832)\n",
      "Loss: 1.393 | Acc: 49.449% (4399/8896)\n",
      "Loss: 1.393 | Acc: 49.475% (4433/8960)\n",
      "Loss: 1.393 | Acc: 49.501% (4467/9024)\n",
      "Loss: 1.393 | Acc: 49.472% (4496/9088)\n",
      "Loss: 1.394 | Acc: 49.476% (4528/9152)\n",
      "Loss: 1.392 | Acc: 49.512% (4563/9216)\n",
      "Loss: 1.391 | Acc: 49.526% (4596/9280)\n",
      "Loss: 1.391 | Acc: 49.518% (4627/9344)\n",
      "Loss: 1.390 | Acc: 49.564% (4663/9408)\n",
      "Loss: 1.392 | Acc: 49.514% (4690/9472)\n",
      "Loss: 1.391 | Acc: 49.539% (4724/9536)\n",
      "Loss: 1.391 | Acc: 49.573% (4759/9600)\n",
      "Loss: 1.391 | Acc: 49.524% (4786/9664)\n",
      "Loss: 1.390 | Acc: 49.507% (4816/9728)\n",
      "Loss: 1.391 | Acc: 49.449% (4842/9792)\n",
      "Loss: 1.390 | Acc: 49.412% (4870/9856)\n",
      "Loss: 1.391 | Acc: 49.355% (4896/9920)\n",
      "Loss: 1.392 | Acc: 49.319% (4924/9984)\n",
      "Loss: 1.393 | Acc: 49.310% (4931/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 49.31\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.341 | Acc: 53.125% (34/64)\n",
      "Loss: 1.295 | Acc: 54.688% (70/128)\n",
      "Loss: 1.333 | Acc: 51.042% (98/192)\n",
      "Loss: 1.350 | Acc: 51.562% (132/256)\n",
      "Loss: 1.364 | Acc: 50.625% (162/320)\n",
      "Loss: 1.357 | Acc: 51.302% (197/384)\n",
      "Loss: 1.367 | Acc: 50.446% (226/448)\n",
      "Loss: 1.375 | Acc: 50.000% (256/512)\n",
      "Loss: 1.350 | Acc: 50.868% (293/576)\n",
      "Loss: 1.375 | Acc: 49.844% (319/640)\n",
      "Loss: 1.355 | Acc: 50.852% (358/704)\n",
      "Loss: 1.355 | Acc: 51.432% (395/768)\n",
      "Loss: 1.350 | Acc: 51.082% (425/832)\n",
      "Loss: 1.355 | Acc: 50.670% (454/896)\n",
      "Loss: 1.354 | Acc: 50.938% (489/960)\n",
      "Loss: 1.353 | Acc: 51.270% (525/1024)\n",
      "Loss: 1.353 | Acc: 51.195% (557/1088)\n",
      "Loss: 1.343 | Acc: 51.389% (592/1152)\n",
      "Loss: 1.350 | Acc: 50.987% (620/1216)\n",
      "Loss: 1.348 | Acc: 50.547% (647/1280)\n",
      "Loss: 1.356 | Acc: 50.744% (682/1344)\n",
      "Loss: 1.344 | Acc: 50.994% (718/1408)\n",
      "Loss: 1.341 | Acc: 51.223% (754/1472)\n",
      "Loss: 1.347 | Acc: 50.846% (781/1536)\n",
      "Loss: 1.354 | Acc: 50.688% (811/1600)\n",
      "Loss: 1.358 | Acc: 50.541% (841/1664)\n",
      "Loss: 1.354 | Acc: 50.637% (875/1728)\n",
      "Loss: 1.356 | Acc: 50.614% (907/1792)\n",
      "Loss: 1.356 | Acc: 50.593% (939/1856)\n",
      "Loss: 1.360 | Acc: 50.677% (973/1920)\n",
      "Loss: 1.372 | Acc: 50.101% (994/1984)\n",
      "Loss: 1.373 | Acc: 49.854% (1021/2048)\n",
      "Loss: 1.373 | Acc: 49.858% (1053/2112)\n",
      "Loss: 1.380 | Acc: 49.586% (1079/2176)\n",
      "Loss: 1.374 | Acc: 49.643% (1112/2240)\n",
      "Loss: 1.373 | Acc: 49.653% (1144/2304)\n",
      "Loss: 1.374 | Acc: 49.620% (1175/2368)\n",
      "Loss: 1.376 | Acc: 49.630% (1207/2432)\n",
      "Loss: 1.378 | Acc: 49.559% (1237/2496)\n",
      "Loss: 1.376 | Acc: 49.531% (1268/2560)\n",
      "Loss: 1.376 | Acc: 49.428% (1297/2624)\n",
      "Loss: 1.373 | Acc: 49.479% (1330/2688)\n",
      "Loss: 1.374 | Acc: 49.455% (1361/2752)\n",
      "Loss: 1.374 | Acc: 49.538% (1395/2816)\n",
      "Loss: 1.372 | Acc: 49.618% (1429/2880)\n",
      "Loss: 1.370 | Acc: 49.830% (1467/2944)\n",
      "Loss: 1.365 | Acc: 50.000% (1504/3008)\n",
      "Loss: 1.359 | Acc: 50.326% (1546/3072)\n",
      "Loss: 1.363 | Acc: 50.096% (1571/3136)\n",
      "Loss: 1.364 | Acc: 50.062% (1602/3200)\n",
      "Loss: 1.362 | Acc: 50.184% (1638/3264)\n",
      "Loss: 1.365 | Acc: 50.240% (1672/3328)\n",
      "Loss: 1.366 | Acc: 50.206% (1703/3392)\n",
      "Loss: 1.366 | Acc: 50.203% (1735/3456)\n",
      "Loss: 1.366 | Acc: 50.256% (1769/3520)\n",
      "Loss: 1.365 | Acc: 50.363% (1805/3584)\n",
      "Loss: 1.364 | Acc: 50.466% (1841/3648)\n",
      "Loss: 1.362 | Acc: 50.404% (1871/3712)\n",
      "Loss: 1.360 | Acc: 50.609% (1911/3776)\n",
      "Loss: 1.357 | Acc: 50.781% (1950/3840)\n",
      "Loss: 1.358 | Acc: 50.743% (1981/3904)\n",
      "Loss: 1.357 | Acc: 50.832% (2017/3968)\n",
      "Loss: 1.358 | Acc: 50.719% (2045/4032)\n",
      "Loss: 1.360 | Acc: 50.562% (2071/4096)\n",
      "Loss: 1.360 | Acc: 50.577% (2104/4160)\n",
      "Loss: 1.360 | Acc: 50.545% (2135/4224)\n",
      "Loss: 1.362 | Acc: 50.420% (2162/4288)\n",
      "Loss: 1.361 | Acc: 50.414% (2194/4352)\n",
      "Loss: 1.362 | Acc: 50.408% (2226/4416)\n",
      "Loss: 1.363 | Acc: 50.335% (2255/4480)\n",
      "Loss: 1.365 | Acc: 50.308% (2286/4544)\n",
      "Loss: 1.368 | Acc: 50.239% (2315/4608)\n",
      "Loss: 1.366 | Acc: 50.321% (2351/4672)\n",
      "Loss: 1.369 | Acc: 50.296% (2382/4736)\n",
      "Loss: 1.371 | Acc: 50.167% (2408/4800)\n",
      "Loss: 1.370 | Acc: 50.144% (2439/4864)\n",
      "Loss: 1.369 | Acc: 50.122% (2470/4928)\n",
      "Loss: 1.372 | Acc: 49.960% (2494/4992)\n",
      "Loss: 1.375 | Acc: 49.782% (2517/5056)\n",
      "Loss: 1.374 | Acc: 49.824% (2551/5120)\n",
      "Loss: 1.374 | Acc: 49.769% (2580/5184)\n",
      "Loss: 1.371 | Acc: 49.924% (2620/5248)\n",
      "Loss: 1.372 | Acc: 49.906% (2651/5312)\n",
      "Loss: 1.373 | Acc: 49.963% (2686/5376)\n",
      "Loss: 1.373 | Acc: 49.963% (2718/5440)\n",
      "Loss: 1.375 | Acc: 50.000% (2752/5504)\n",
      "Loss: 1.375 | Acc: 49.892% (2778/5568)\n",
      "Loss: 1.374 | Acc: 49.964% (2814/5632)\n",
      "Loss: 1.372 | Acc: 49.965% (2846/5696)\n",
      "Loss: 1.370 | Acc: 50.035% (2882/5760)\n",
      "Loss: 1.370 | Acc: 50.069% (2916/5824)\n",
      "Loss: 1.372 | Acc: 49.983% (2943/5888)\n",
      "Loss: 1.372 | Acc: 50.017% (2977/5952)\n",
      "Loss: 1.371 | Acc: 50.033% (3010/6016)\n",
      "Loss: 1.372 | Acc: 50.066% (3044/6080)\n",
      "Loss: 1.371 | Acc: 50.081% (3077/6144)\n",
      "Loss: 1.372 | Acc: 50.064% (3108/6208)\n",
      "Loss: 1.371 | Acc: 50.032% (3138/6272)\n",
      "Loss: 1.370 | Acc: 49.984% (3167/6336)\n",
      "Loss: 1.369 | Acc: 50.062% (3204/6400)\n",
      "Loss: 1.369 | Acc: 50.108% (3239/6464)\n",
      "Loss: 1.370 | Acc: 50.123% (3272/6528)\n",
      "Loss: 1.369 | Acc: 50.212% (3310/6592)\n",
      "Loss: 1.369 | Acc: 50.165% (3339/6656)\n",
      "Loss: 1.366 | Acc: 50.253% (3377/6720)\n",
      "Loss: 1.366 | Acc: 50.280% (3411/6784)\n",
      "Loss: 1.366 | Acc: 50.365% (3449/6848)\n",
      "Loss: 1.366 | Acc: 50.333% (3479/6912)\n",
      "Loss: 1.366 | Acc: 50.301% (3509/6976)\n",
      "Loss: 1.366 | Acc: 50.327% (3543/7040)\n",
      "Loss: 1.366 | Acc: 50.296% (3573/7104)\n",
      "Loss: 1.366 | Acc: 50.335% (3608/7168)\n",
      "Loss: 1.365 | Acc: 50.456% (3649/7232)\n",
      "Loss: 1.362 | Acc: 50.535% (3687/7296)\n",
      "Loss: 1.364 | Acc: 50.530% (3719/7360)\n",
      "Loss: 1.365 | Acc: 50.498% (3749/7424)\n",
      "Loss: 1.363 | Acc: 50.534% (3784/7488)\n",
      "Loss: 1.363 | Acc: 50.516% (3815/7552)\n",
      "Loss: 1.363 | Acc: 50.578% (3852/7616)\n",
      "Loss: 1.361 | Acc: 50.638% (3889/7680)\n",
      "Loss: 1.361 | Acc: 50.659% (3923/7744)\n",
      "Loss: 1.361 | Acc: 50.628% (3953/7808)\n",
      "Loss: 1.360 | Acc: 50.635% (3986/7872)\n",
      "Loss: 1.359 | Acc: 50.668% (4021/7936)\n",
      "Loss: 1.359 | Acc: 50.663% (4053/8000)\n",
      "Loss: 1.360 | Acc: 50.719% (4090/8064)\n",
      "Loss: 1.361 | Acc: 50.726% (4123/8128)\n",
      "Loss: 1.362 | Acc: 50.684% (4152/8192)\n",
      "Loss: 1.362 | Acc: 50.666% (4183/8256)\n",
      "Loss: 1.362 | Acc: 50.697% (4218/8320)\n",
      "Loss: 1.362 | Acc: 50.704% (4251/8384)\n",
      "Loss: 1.363 | Acc: 50.639% (4278/8448)\n",
      "Loss: 1.363 | Acc: 50.611% (4308/8512)\n",
      "Loss: 1.364 | Acc: 50.595% (4339/8576)\n",
      "Loss: 1.364 | Acc: 50.590% (4371/8640)\n",
      "Loss: 1.364 | Acc: 50.597% (4404/8704)\n",
      "Loss: 1.364 | Acc: 50.582% (4435/8768)\n",
      "Loss: 1.364 | Acc: 50.611% (4470/8832)\n",
      "Loss: 1.364 | Acc: 50.540% (4496/8896)\n",
      "Loss: 1.364 | Acc: 50.580% (4532/8960)\n",
      "Loss: 1.363 | Acc: 50.676% (4573/9024)\n",
      "Loss: 1.363 | Acc: 50.682% (4606/9088)\n",
      "Loss: 1.362 | Acc: 50.732% (4643/9152)\n",
      "Loss: 1.362 | Acc: 50.694% (4672/9216)\n",
      "Loss: 1.363 | Acc: 50.657% (4701/9280)\n",
      "Loss: 1.364 | Acc: 50.599% (4728/9344)\n",
      "Loss: 1.366 | Acc: 50.531% (4754/9408)\n",
      "Loss: 1.366 | Acc: 50.560% (4789/9472)\n",
      "Loss: 1.366 | Acc: 50.524% (4818/9536)\n",
      "Loss: 1.367 | Acc: 50.458% (4844/9600)\n",
      "Loss: 1.367 | Acc: 50.486% (4879/9664)\n",
      "Loss: 1.365 | Acc: 50.565% (4919/9728)\n",
      "Loss: 1.366 | Acc: 50.531% (4948/9792)\n",
      "Loss: 1.366 | Acc: 50.507% (4978/9856)\n",
      "Loss: 1.367 | Acc: 50.454% (5005/9920)\n",
      "Loss: 1.368 | Acc: 50.401% (5032/9984)\n",
      "Loss: 1.367 | Acc: 50.358% (5060/10048)\n",
      "Loss: 1.367 | Acc: 50.386% (5095/10112)\n",
      "Loss: 1.367 | Acc: 50.354% (5124/10176)\n",
      "Loss: 1.367 | Acc: 50.342% (5155/10240)\n",
      "Loss: 1.367 | Acc: 50.349% (5188/10304)\n",
      "Loss: 1.369 | Acc: 50.299% (5215/10368)\n",
      "Loss: 1.368 | Acc: 50.345% (5252/10432)\n",
      "Loss: 1.368 | Acc: 50.381% (5288/10496)\n",
      "Loss: 1.368 | Acc: 50.265% (5308/10560)\n",
      "Loss: 1.368 | Acc: 50.282% (5342/10624)\n",
      "Loss: 1.369 | Acc: 50.234% (5369/10688)\n",
      "Loss: 1.369 | Acc: 50.205% (5398/10752)\n",
      "Loss: 1.370 | Acc: 50.176% (5427/10816)\n",
      "Loss: 1.370 | Acc: 50.165% (5458/10880)\n",
      "Loss: 1.369 | Acc: 50.210% (5495/10944)\n",
      "Loss: 1.369 | Acc: 50.245% (5531/11008)\n",
      "Loss: 1.368 | Acc: 50.253% (5564/11072)\n",
      "Loss: 1.369 | Acc: 50.198% (5590/11136)\n",
      "Loss: 1.369 | Acc: 50.179% (5620/11200)\n",
      "Loss: 1.370 | Acc: 50.186% (5653/11264)\n",
      "Loss: 1.369 | Acc: 50.185% (5685/11328)\n",
      "Loss: 1.370 | Acc: 50.149% (5713/11392)\n",
      "Loss: 1.369 | Acc: 50.131% (5743/11456)\n",
      "Loss: 1.369 | Acc: 50.122% (5774/11520)\n",
      "Loss: 1.370 | Acc: 50.104% (5804/11584)\n",
      "Loss: 1.369 | Acc: 50.137% (5840/11648)\n",
      "Loss: 1.370 | Acc: 50.154% (5874/11712)\n",
      "Loss: 1.370 | Acc: 50.153% (5906/11776)\n",
      "Loss: 1.369 | Acc: 50.177% (5941/11840)\n",
      "Loss: 1.367 | Acc: 50.252% (5982/11904)\n",
      "Loss: 1.368 | Acc: 50.259% (6015/11968)\n",
      "Loss: 1.367 | Acc: 50.308% (6053/12032)\n",
      "Loss: 1.367 | Acc: 50.331% (6088/12096)\n",
      "Loss: 1.368 | Acc: 50.271% (6113/12160)\n",
      "Loss: 1.367 | Acc: 50.303% (6149/12224)\n",
      "Loss: 1.366 | Acc: 50.342% (6186/12288)\n",
      "Loss: 1.366 | Acc: 50.340% (6218/12352)\n",
      "Loss: 1.366 | Acc: 50.338% (6250/12416)\n",
      "Loss: 1.367 | Acc: 50.337% (6282/12480)\n",
      "Loss: 1.366 | Acc: 50.327% (6313/12544)\n",
      "Loss: 1.366 | Acc: 50.365% (6350/12608)\n",
      "Loss: 1.366 | Acc: 50.395% (6386/12672)\n",
      "Loss: 1.366 | Acc: 50.369% (6415/12736)\n",
      "Loss: 1.367 | Acc: 50.375% (6448/12800)\n",
      "Loss: 1.367 | Acc: 50.389% (6482/12864)\n",
      "Loss: 1.367 | Acc: 50.348% (6509/12928)\n",
      "Loss: 1.366 | Acc: 50.369% (6544/12992)\n",
      "Loss: 1.366 | Acc: 50.383% (6578/13056)\n",
      "Loss: 1.367 | Acc: 50.343% (6605/13120)\n",
      "Loss: 1.367 | Acc: 50.387% (6643/13184)\n",
      "Loss: 1.366 | Acc: 50.423% (6680/13248)\n",
      "Loss: 1.366 | Acc: 50.428% (6713/13312)\n",
      "Loss: 1.367 | Acc: 50.381% (6739/13376)\n",
      "Loss: 1.367 | Acc: 50.365% (6769/13440)\n",
      "Loss: 1.367 | Acc: 50.407% (6807/13504)\n",
      "Loss: 1.366 | Acc: 50.420% (6841/13568)\n",
      "Loss: 1.367 | Acc: 50.418% (6873/13632)\n",
      "Loss: 1.367 | Acc: 50.423% (6906/13696)\n",
      "Loss: 1.366 | Acc: 50.487% (6947/13760)\n",
      "Loss: 1.365 | Acc: 50.499% (6981/13824)\n",
      "Loss: 1.364 | Acc: 50.526% (7017/13888)\n",
      "Loss: 1.364 | Acc: 50.516% (7048/13952)\n",
      "Loss: 1.363 | Acc: 50.528% (7082/14016)\n",
      "Loss: 1.363 | Acc: 50.568% (7120/14080)\n",
      "Loss: 1.363 | Acc: 50.559% (7151/14144)\n",
      "Loss: 1.363 | Acc: 50.570% (7185/14208)\n",
      "Loss: 1.363 | Acc: 50.589% (7220/14272)\n",
      "Loss: 1.362 | Acc: 50.621% (7257/14336)\n",
      "Loss: 1.361 | Acc: 50.604% (7287/14400)\n",
      "Loss: 1.361 | Acc: 50.601% (7319/14464)\n",
      "Loss: 1.361 | Acc: 50.585% (7349/14528)\n",
      "Loss: 1.360 | Acc: 50.637% (7389/14592)\n",
      "Loss: 1.360 | Acc: 50.635% (7421/14656)\n",
      "Loss: 1.359 | Acc: 50.625% (7452/14720)\n",
      "Loss: 1.359 | Acc: 50.636% (7486/14784)\n",
      "Loss: 1.359 | Acc: 50.647% (7520/14848)\n",
      "Loss: 1.358 | Acc: 50.650% (7553/14912)\n",
      "Loss: 1.358 | Acc: 50.681% (7590/14976)\n",
      "Loss: 1.358 | Acc: 50.705% (7626/15040)\n",
      "Loss: 1.357 | Acc: 50.728% (7662/15104)\n",
      "Loss: 1.357 | Acc: 50.758% (7699/15168)\n",
      "Loss: 1.358 | Acc: 50.709% (7724/15232)\n",
      "Loss: 1.357 | Acc: 50.758% (7764/15296)\n",
      "Loss: 1.356 | Acc: 50.775% (7799/15360)\n",
      "Loss: 1.355 | Acc: 50.830% (7840/15424)\n",
      "Loss: 1.356 | Acc: 50.820% (7871/15488)\n",
      "Loss: 1.356 | Acc: 50.791% (7899/15552)\n",
      "Loss: 1.358 | Acc: 50.756% (7926/15616)\n",
      "Loss: 1.357 | Acc: 50.740% (7956/15680)\n",
      "Loss: 1.358 | Acc: 50.743% (7989/15744)\n",
      "Loss: 1.358 | Acc: 50.671% (8010/15808)\n",
      "Loss: 1.358 | Acc: 50.687% (8045/15872)\n",
      "Loss: 1.358 | Acc: 50.703% (8080/15936)\n",
      "Loss: 1.358 | Acc: 50.688% (8110/16000)\n",
      "Loss: 1.357 | Acc: 50.716% (8147/16064)\n",
      "Loss: 1.357 | Acc: 50.707% (8178/16128)\n",
      "Loss: 1.357 | Acc: 50.710% (8211/16192)\n",
      "Loss: 1.358 | Acc: 50.689% (8240/16256)\n",
      "Loss: 1.358 | Acc: 50.680% (8271/16320)\n",
      "Loss: 1.359 | Acc: 50.659% (8300/16384)\n",
      "Loss: 1.358 | Acc: 50.675% (8335/16448)\n",
      "Loss: 1.359 | Acc: 50.624% (8359/16512)\n",
      "Loss: 1.359 | Acc: 50.603% (8388/16576)\n",
      "Loss: 1.358 | Acc: 50.625% (8424/16640)\n",
      "Loss: 1.358 | Acc: 50.647% (8460/16704)\n",
      "Loss: 1.358 | Acc: 50.668% (8496/16768)\n",
      "Loss: 1.357 | Acc: 50.707% (8535/16832)\n",
      "Loss: 1.357 | Acc: 50.710% (8568/16896)\n",
      "Loss: 1.357 | Acc: 50.702% (8599/16960)\n",
      "Loss: 1.357 | Acc: 50.699% (8631/17024)\n",
      "Loss: 1.356 | Acc: 50.702% (8664/17088)\n",
      "Loss: 1.355 | Acc: 50.740% (8703/17152)\n",
      "Loss: 1.355 | Acc: 50.726% (8733/17216)\n",
      "Loss: 1.355 | Acc: 50.723% (8765/17280)\n",
      "Loss: 1.354 | Acc: 50.755% (8803/17344)\n",
      "Loss: 1.354 | Acc: 50.730% (8831/17408)\n",
      "Loss: 1.353 | Acc: 50.761% (8869/17472)\n",
      "Loss: 1.353 | Acc: 50.781% (8905/17536)\n",
      "Loss: 1.354 | Acc: 50.773% (8936/17600)\n",
      "Loss: 1.353 | Acc: 50.804% (8974/17664)\n",
      "Loss: 1.352 | Acc: 50.818% (9009/17728)\n",
      "Loss: 1.351 | Acc: 50.832% (9044/17792)\n",
      "Loss: 1.352 | Acc: 50.823% (9075/17856)\n",
      "Loss: 1.353 | Acc: 50.815% (9106/17920)\n",
      "Loss: 1.353 | Acc: 50.806% (9137/17984)\n",
      "Loss: 1.353 | Acc: 50.792% (9167/18048)\n",
      "Loss: 1.353 | Acc: 50.778% (9197/18112)\n",
      "Loss: 1.352 | Acc: 50.781% (9230/18176)\n",
      "Loss: 1.353 | Acc: 50.768% (9260/18240)\n",
      "Loss: 1.353 | Acc: 50.781% (9295/18304)\n",
      "Loss: 1.353 | Acc: 50.746% (9321/18368)\n",
      "Loss: 1.352 | Acc: 50.781% (9360/18432)\n",
      "Loss: 1.352 | Acc: 50.806% (9397/18496)\n",
      "Loss: 1.352 | Acc: 50.787% (9426/18560)\n",
      "Loss: 1.352 | Acc: 50.762% (9454/18624)\n",
      "Loss: 1.352 | Acc: 50.771% (9488/18688)\n",
      "Loss: 1.352 | Acc: 50.768% (9520/18752)\n",
      "Loss: 1.352 | Acc: 50.787% (9556/18816)\n",
      "Loss: 1.352 | Acc: 50.784% (9588/18880)\n",
      "Loss: 1.353 | Acc: 50.739% (9612/18944)\n",
      "Loss: 1.353 | Acc: 50.742% (9645/19008)\n",
      "Loss: 1.354 | Acc: 50.713% (9672/19072)\n",
      "Loss: 1.353 | Acc: 50.721% (9706/19136)\n",
      "Loss: 1.353 | Acc: 50.734% (9741/19200)\n",
      "Loss: 1.353 | Acc: 50.711% (9769/19264)\n",
      "Loss: 1.353 | Acc: 50.719% (9803/19328)\n",
      "Loss: 1.353 | Acc: 50.748% (9841/19392)\n",
      "Loss: 1.353 | Acc: 50.750% (9874/19456)\n",
      "Loss: 1.352 | Acc: 50.763% (9909/19520)\n",
      "Loss: 1.352 | Acc: 50.791% (9947/19584)\n",
      "Loss: 1.351 | Acc: 50.814% (9984/19648)\n",
      "Loss: 1.351 | Acc: 50.807% (10015/19712)\n",
      "Loss: 1.351 | Acc: 50.794% (10045/19776)\n",
      "Loss: 1.351 | Acc: 50.801% (10079/19840)\n",
      "Loss: 1.352 | Acc: 50.804% (10112/19904)\n",
      "Loss: 1.352 | Acc: 50.791% (10142/19968)\n",
      "Loss: 1.351 | Acc: 50.809% (10178/20032)\n",
      "Loss: 1.351 | Acc: 50.811% (10211/20096)\n",
      "Loss: 1.352 | Acc: 50.764% (10234/20160)\n",
      "Loss: 1.353 | Acc: 50.732% (10260/20224)\n",
      "Loss: 1.353 | Acc: 50.695% (10285/20288)\n",
      "Loss: 1.353 | Acc: 50.673% (10313/20352)\n",
      "Loss: 1.353 | Acc: 50.666% (10344/20416)\n",
      "Loss: 1.353 | Acc: 50.688% (10381/20480)\n",
      "Loss: 1.353 | Acc: 50.706% (10417/20544)\n",
      "Loss: 1.353 | Acc: 50.713% (10451/20608)\n",
      "Loss: 1.354 | Acc: 50.672% (10475/20672)\n",
      "Loss: 1.353 | Acc: 50.694% (10512/20736)\n",
      "Loss: 1.354 | Acc: 50.702% (10546/20800)\n",
      "Loss: 1.354 | Acc: 50.695% (10577/20864)\n",
      "Loss: 1.354 | Acc: 50.664% (10603/20928)\n",
      "Loss: 1.353 | Acc: 50.681% (10639/20992)\n",
      "Loss: 1.353 | Acc: 50.665% (10668/21056)\n",
      "Loss: 1.353 | Acc: 50.658% (10699/21120)\n",
      "Loss: 1.353 | Acc: 50.651% (10730/21184)\n",
      "Loss: 1.353 | Acc: 50.659% (10764/21248)\n",
      "Loss: 1.353 | Acc: 50.676% (10800/21312)\n",
      "Loss: 1.353 | Acc: 50.692% (10836/21376)\n",
      "Loss: 1.353 | Acc: 50.676% (10865/21440)\n",
      "Loss: 1.353 | Acc: 50.693% (10901/21504)\n",
      "Loss: 1.353 | Acc: 50.733% (10942/21568)\n",
      "Loss: 1.353 | Acc: 50.717% (10971/21632)\n",
      "Loss: 1.353 | Acc: 50.728% (11006/21696)\n",
      "Loss: 1.353 | Acc: 50.722% (11037/21760)\n",
      "Loss: 1.353 | Acc: 50.724% (11070/21824)\n",
      "Loss: 1.353 | Acc: 50.731% (11104/21888)\n",
      "Loss: 1.353 | Acc: 50.720% (11134/21952)\n",
      "Loss: 1.354 | Acc: 50.718% (11166/22016)\n",
      "Loss: 1.353 | Acc: 50.747% (11205/22080)\n",
      "Loss: 1.353 | Acc: 50.768% (11242/22144)\n",
      "Loss: 1.353 | Acc: 50.761% (11273/22208)\n",
      "Loss: 1.352 | Acc: 50.777% (11309/22272)\n",
      "Loss: 1.352 | Acc: 50.783% (11343/22336)\n",
      "Loss: 1.352 | Acc: 50.772% (11373/22400)\n",
      "Loss: 1.352 | Acc: 50.766% (11404/22464)\n",
      "Loss: 1.353 | Acc: 50.777% (11439/22528)\n",
      "Loss: 1.353 | Acc: 50.744% (11464/22592)\n",
      "Loss: 1.353 | Acc: 50.759% (11500/22656)\n",
      "Loss: 1.353 | Acc: 50.739% (11528/22720)\n",
      "Loss: 1.353 | Acc: 50.755% (11564/22784)\n",
      "Loss: 1.352 | Acc: 50.757% (11597/22848)\n",
      "Loss: 1.352 | Acc: 50.755% (11629/22912)\n",
      "Loss: 1.352 | Acc: 50.762% (11663/22976)\n",
      "Loss: 1.351 | Acc: 50.768% (11697/23040)\n",
      "Loss: 1.351 | Acc: 50.783% (11733/23104)\n",
      "Loss: 1.351 | Acc: 50.755% (11759/23168)\n",
      "Loss: 1.351 | Acc: 50.745% (11789/23232)\n",
      "Loss: 1.350 | Acc: 50.768% (11827/23296)\n",
      "Loss: 1.351 | Acc: 50.771% (11860/23360)\n",
      "Loss: 1.351 | Acc: 50.756% (11889/23424)\n",
      "Loss: 1.351 | Acc: 50.745% (11919/23488)\n",
      "Loss: 1.351 | Acc: 50.747% (11952/23552)\n",
      "Loss: 1.351 | Acc: 50.737% (11982/23616)\n",
      "Loss: 1.351 | Acc: 50.726% (12012/23680)\n",
      "Loss: 1.351 | Acc: 50.729% (12045/23744)\n",
      "Loss: 1.350 | Acc: 50.748% (12082/23808)\n",
      "Loss: 1.350 | Acc: 50.775% (12121/23872)\n",
      "Loss: 1.350 | Acc: 50.781% (12155/23936)\n",
      "Loss: 1.351 | Acc: 50.767% (12184/24000)\n",
      "Loss: 1.350 | Acc: 50.769% (12217/24064)\n",
      "Loss: 1.350 | Acc: 50.771% (12250/24128)\n",
      "Loss: 1.350 | Acc: 50.802% (12290/24192)\n",
      "Loss: 1.351 | Acc: 50.796% (12321/24256)\n",
      "Loss: 1.350 | Acc: 50.802% (12355/24320)\n",
      "Loss: 1.350 | Acc: 50.824% (12393/24384)\n",
      "Loss: 1.350 | Acc: 50.839% (12429/24448)\n",
      "Loss: 1.350 | Acc: 50.840% (12462/24512)\n",
      "Loss: 1.351 | Acc: 50.802% (12485/24576)\n",
      "Loss: 1.351 | Acc: 50.795% (12516/24640)\n",
      "Loss: 1.351 | Acc: 50.789% (12547/24704)\n",
      "Loss: 1.351 | Acc: 50.795% (12581/24768)\n",
      "Loss: 1.351 | Acc: 50.769% (12607/24832)\n",
      "Loss: 1.351 | Acc: 50.779% (12642/24896)\n",
      "Loss: 1.351 | Acc: 50.761% (12670/24960)\n",
      "Loss: 1.351 | Acc: 50.763% (12703/25024)\n",
      "Loss: 1.351 | Acc: 50.781% (12740/25088)\n",
      "Loss: 1.351 | Acc: 50.763% (12768/25152)\n",
      "Loss: 1.352 | Acc: 50.761% (12800/25216)\n",
      "Loss: 1.351 | Acc: 50.771% (12835/25280)\n",
      "Loss: 1.351 | Acc: 50.769% (12867/25344)\n",
      "Loss: 1.352 | Acc: 50.732% (12890/25408)\n",
      "Loss: 1.352 | Acc: 50.726% (12921/25472)\n",
      "Loss: 1.352 | Acc: 50.713% (12950/25536)\n",
      "Loss: 1.351 | Acc: 50.703% (12980/25600)\n",
      "Loss: 1.351 | Acc: 50.709% (13014/25664)\n",
      "Loss: 1.351 | Acc: 50.723% (13050/25728)\n",
      "Loss: 1.352 | Acc: 50.690% (13074/25792)\n",
      "Loss: 1.352 | Acc: 50.661% (13099/25856)\n",
      "Loss: 1.352 | Acc: 50.679% (13136/25920)\n",
      "Loss: 1.352 | Acc: 50.681% (13169/25984)\n",
      "Loss: 1.352 | Acc: 50.691% (13204/26048)\n",
      "Loss: 1.352 | Acc: 50.697% (13238/26112)\n",
      "Loss: 1.352 | Acc: 50.684% (13267/26176)\n",
      "Loss: 1.352 | Acc: 50.675% (13297/26240)\n",
      "Loss: 1.352 | Acc: 50.677% (13330/26304)\n",
      "Loss: 1.352 | Acc: 50.649% (13355/26368)\n",
      "Loss: 1.352 | Acc: 50.624% (13381/26432)\n",
      "Loss: 1.352 | Acc: 50.645% (13419/26496)\n",
      "Loss: 1.352 | Acc: 50.640% (13450/26560)\n",
      "Loss: 1.352 | Acc: 50.639% (13482/26624)\n",
      "Loss: 1.352 | Acc: 50.637% (13514/26688)\n",
      "Loss: 1.352 | Acc: 50.635% (13546/26752)\n",
      "Loss: 1.351 | Acc: 50.653% (13583/26816)\n",
      "Loss: 1.351 | Acc: 50.658% (13617/26880)\n",
      "Loss: 1.351 | Acc: 50.675% (13654/26944)\n",
      "Loss: 1.351 | Acc: 50.685% (13689/27008)\n",
      "Loss: 1.350 | Acc: 50.717% (13730/27072)\n",
      "Loss: 1.351 | Acc: 50.704% (13759/27136)\n",
      "Loss: 1.351 | Acc: 50.699% (13790/27200)\n",
      "Loss: 1.351 | Acc: 50.704% (13824/27264)\n",
      "Loss: 1.350 | Acc: 50.695% (13854/27328)\n",
      "Loss: 1.351 | Acc: 50.668% (13879/27392)\n",
      "Loss: 1.351 | Acc: 50.670% (13912/27456)\n",
      "Loss: 1.350 | Acc: 50.676% (13946/27520)\n",
      "Loss: 1.350 | Acc: 50.678% (13979/27584)\n",
      "Loss: 1.350 | Acc: 50.680% (14012/27648)\n",
      "Loss: 1.350 | Acc: 50.678% (14044/27712)\n",
      "Loss: 1.349 | Acc: 50.691% (14080/27776)\n",
      "Loss: 1.349 | Acc: 50.679% (14109/27840)\n",
      "Loss: 1.350 | Acc: 50.670% (14139/27904)\n",
      "Loss: 1.350 | Acc: 50.644% (14164/27968)\n",
      "Loss: 1.350 | Acc: 50.656% (14200/28032)\n",
      "Loss: 1.350 | Acc: 50.658% (14233/28096)\n",
      "Loss: 1.350 | Acc: 50.675% (14270/28160)\n",
      "Loss: 1.350 | Acc: 50.698% (14309/28224)\n",
      "Loss: 1.350 | Acc: 50.707% (14344/28288)\n",
      "Loss: 1.349 | Acc: 50.716% (14379/28352)\n",
      "Loss: 1.349 | Acc: 50.707% (14409/28416)\n",
      "Loss: 1.349 | Acc: 50.727% (14447/28480)\n",
      "Loss: 1.349 | Acc: 50.708% (14474/28544)\n",
      "Loss: 1.350 | Acc: 50.654% (14491/28608)\n",
      "Loss: 1.351 | Acc: 50.642% (14520/28672)\n",
      "Loss: 1.351 | Acc: 50.637% (14551/28736)\n",
      "Loss: 1.351 | Acc: 50.660% (14590/28800)\n",
      "Loss: 1.350 | Acc: 50.665% (14624/28864)\n",
      "Loss: 1.351 | Acc: 50.664% (14656/28928)\n",
      "Loss: 1.351 | Acc: 50.659% (14687/28992)\n",
      "Loss: 1.350 | Acc: 50.661% (14720/29056)\n",
      "Loss: 1.350 | Acc: 50.639% (14746/29120)\n",
      "Loss: 1.351 | Acc: 50.620% (14773/29184)\n",
      "Loss: 1.351 | Acc: 50.615% (14804/29248)\n",
      "Loss: 1.350 | Acc: 50.635% (14842/29312)\n",
      "Loss: 1.350 | Acc: 50.650% (14879/29376)\n",
      "Loss: 1.350 | Acc: 50.669% (14917/29440)\n",
      "Loss: 1.349 | Acc: 50.668% (14949/29504)\n",
      "Loss: 1.350 | Acc: 50.670% (14982/29568)\n",
      "Loss: 1.350 | Acc: 50.685% (15019/29632)\n",
      "Loss: 1.350 | Acc: 50.687% (15052/29696)\n",
      "Loss: 1.350 | Acc: 50.692% (15086/29760)\n",
      "Loss: 1.350 | Acc: 50.684% (15116/29824)\n",
      "Loss: 1.351 | Acc: 50.673% (15145/29888)\n",
      "Loss: 1.351 | Acc: 50.658% (15173/29952)\n",
      "Loss: 1.351 | Acc: 50.686% (15214/30016)\n",
      "Loss: 1.350 | Acc: 50.675% (15243/30080)\n",
      "Loss: 1.350 | Acc: 50.660% (15271/30144)\n",
      "Loss: 1.350 | Acc: 50.662% (15304/30208)\n",
      "Loss: 1.350 | Acc: 50.694% (15346/30272)\n",
      "Loss: 1.350 | Acc: 50.696% (15379/30336)\n",
      "Loss: 1.350 | Acc: 50.707% (15415/30400)\n",
      "Loss: 1.350 | Acc: 50.696% (15444/30464)\n",
      "Loss: 1.351 | Acc: 50.698% (15477/30528)\n",
      "Loss: 1.350 | Acc: 50.693% (15508/30592)\n",
      "Loss: 1.350 | Acc: 50.701% (15543/30656)\n",
      "Loss: 1.350 | Acc: 50.703% (15576/30720)\n",
      "Loss: 1.350 | Acc: 50.682% (15602/30784)\n",
      "Loss: 1.350 | Acc: 50.690% (15637/30848)\n",
      "Loss: 1.351 | Acc: 50.676% (15665/30912)\n",
      "Loss: 1.351 | Acc: 50.681% (15699/30976)\n",
      "Loss: 1.350 | Acc: 50.686% (15733/31040)\n",
      "Loss: 1.351 | Acc: 50.669% (15760/31104)\n",
      "Loss: 1.351 | Acc: 50.671% (15793/31168)\n",
      "Loss: 1.350 | Acc: 50.685% (15830/31232)\n",
      "Loss: 1.350 | Acc: 50.687% (15863/31296)\n",
      "Loss: 1.350 | Acc: 50.689% (15896/31360)\n",
      "Loss: 1.350 | Acc: 50.687% (15928/31424)\n",
      "Loss: 1.350 | Acc: 50.683% (15959/31488)\n",
      "Loss: 1.350 | Acc: 50.666% (15986/31552)\n",
      "Loss: 1.351 | Acc: 50.658% (16016/31616)\n",
      "Loss: 1.350 | Acc: 50.682% (16056/31680)\n",
      "Loss: 1.350 | Acc: 50.677% (16087/31744)\n",
      "Loss: 1.350 | Acc: 50.685% (16122/31808)\n",
      "Loss: 1.351 | Acc: 50.687% (16155/31872)\n",
      "Loss: 1.350 | Acc: 50.695% (16190/31936)\n",
      "Loss: 1.350 | Acc: 50.712% (16228/32000)\n",
      "Loss: 1.350 | Acc: 50.702% (16257/32064)\n",
      "Loss: 1.350 | Acc: 50.710% (16292/32128)\n",
      "Loss: 1.350 | Acc: 50.711% (16325/32192)\n",
      "Loss: 1.350 | Acc: 50.701% (16354/32256)\n",
      "Loss: 1.350 | Acc: 50.705% (16388/32320)\n",
      "Loss: 1.350 | Acc: 50.710% (16422/32384)\n",
      "Loss: 1.350 | Acc: 50.724% (16459/32448)\n",
      "Loss: 1.350 | Acc: 50.735% (16495/32512)\n",
      "Loss: 1.350 | Acc: 50.740% (16529/32576)\n",
      "Loss: 1.350 | Acc: 50.735% (16560/32640)\n",
      "Loss: 1.350 | Acc: 50.716% (16586/32704)\n",
      "Loss: 1.350 | Acc: 50.711% (16617/32768)\n",
      "Loss: 1.350 | Acc: 50.722% (16653/32832)\n",
      "Loss: 1.350 | Acc: 50.742% (16692/32896)\n",
      "Loss: 1.349 | Acc: 50.740% (16724/32960)\n",
      "Loss: 1.349 | Acc: 50.736% (16755/33024)\n",
      "Loss: 1.349 | Acc: 50.740% (16789/33088)\n",
      "Loss: 1.349 | Acc: 50.748% (16824/33152)\n",
      "Loss: 1.349 | Acc: 50.741% (16854/33216)\n",
      "Loss: 1.349 | Acc: 50.745% (16888/33280)\n",
      "Loss: 1.349 | Acc: 50.738% (16918/33344)\n",
      "Loss: 1.349 | Acc: 50.733% (16949/33408)\n",
      "Loss: 1.349 | Acc: 50.729% (16980/33472)\n",
      "Loss: 1.350 | Acc: 50.719% (17009/33536)\n",
      "Loss: 1.350 | Acc: 50.708% (17038/33600)\n",
      "Loss: 1.350 | Acc: 50.689% (17064/33664)\n",
      "Loss: 1.351 | Acc: 50.670% (17090/33728)\n",
      "Loss: 1.351 | Acc: 50.687% (17128/33792)\n",
      "Loss: 1.351 | Acc: 50.676% (17157/33856)\n",
      "Loss: 1.351 | Acc: 50.675% (17189/33920)\n",
      "Loss: 1.351 | Acc: 50.665% (17218/33984)\n",
      "Loss: 1.351 | Acc: 50.661% (17249/34048)\n",
      "Loss: 1.351 | Acc: 50.671% (17285/34112)\n",
      "Loss: 1.351 | Acc: 50.667% (17316/34176)\n",
      "Loss: 1.351 | Acc: 50.654% (17344/34240)\n",
      "Loss: 1.352 | Acc: 50.630% (17368/34304)\n",
      "Loss: 1.352 | Acc: 50.631% (17401/34368)\n",
      "Loss: 1.352 | Acc: 50.616% (17428/34432)\n",
      "Loss: 1.351 | Acc: 50.632% (17466/34496)\n",
      "Loss: 1.351 | Acc: 50.639% (17501/34560)\n",
      "Loss: 1.352 | Acc: 50.618% (17526/34624)\n",
      "Loss: 1.352 | Acc: 50.608% (17555/34688)\n",
      "Loss: 1.352 | Acc: 50.599% (17584/34752)\n",
      "Loss: 1.352 | Acc: 50.600% (17617/34816)\n",
      "Loss: 1.352 | Acc: 50.593% (17647/34880)\n",
      "Loss: 1.352 | Acc: 50.592% (17679/34944)\n",
      "Loss: 1.352 | Acc: 50.594% (17712/35008)\n",
      "Loss: 1.351 | Acc: 50.613% (17751/35072)\n",
      "Loss: 1.352 | Acc: 50.612% (17783/35136)\n",
      "Loss: 1.352 | Acc: 50.614% (17816/35200)\n",
      "Loss: 1.352 | Acc: 50.624% (17852/35264)\n",
      "Loss: 1.352 | Acc: 50.614% (17881/35328)\n",
      "Loss: 1.352 | Acc: 50.627% (17918/35392)\n",
      "Loss: 1.352 | Acc: 50.620% (17948/35456)\n",
      "Loss: 1.352 | Acc: 50.628% (17983/35520)\n",
      "Loss: 1.352 | Acc: 50.629% (18016/35584)\n",
      "Loss: 1.352 | Acc: 50.620% (18045/35648)\n",
      "Loss: 1.352 | Acc: 50.622% (18078/35712)\n",
      "Loss: 1.352 | Acc: 50.635% (18115/35776)\n",
      "Loss: 1.352 | Acc: 50.625% (18144/35840)\n",
      "Loss: 1.352 | Acc: 50.632% (18179/35904)\n",
      "Loss: 1.353 | Acc: 50.614% (18205/35968)\n",
      "Loss: 1.352 | Acc: 50.638% (18246/36032)\n",
      "Loss: 1.352 | Acc: 50.632% (18276/36096)\n",
      "Loss: 1.352 | Acc: 50.636% (18310/36160)\n",
      "Loss: 1.352 | Acc: 50.621% (18337/36224)\n",
      "Loss: 1.352 | Acc: 50.604% (18363/36288)\n",
      "Loss: 1.352 | Acc: 50.597% (18393/36352)\n",
      "Loss: 1.352 | Acc: 50.596% (18425/36416)\n",
      "Loss: 1.352 | Acc: 50.600% (18459/36480)\n",
      "Loss: 1.352 | Acc: 50.602% (18492/36544)\n",
      "Loss: 1.352 | Acc: 50.604% (18525/36608)\n",
      "Loss: 1.351 | Acc: 50.619% (18563/36672)\n",
      "Loss: 1.351 | Acc: 50.621% (18596/36736)\n",
      "Loss: 1.351 | Acc: 50.630% (18632/36800)\n",
      "Loss: 1.352 | Acc: 50.624% (18662/36864)\n",
      "Loss: 1.352 | Acc: 50.626% (18695/36928)\n",
      "Loss: 1.352 | Acc: 50.616% (18724/36992)\n",
      "Loss: 1.352 | Acc: 50.623% (18759/37056)\n",
      "Loss: 1.352 | Acc: 50.628% (18793/37120)\n",
      "Loss: 1.351 | Acc: 50.651% (18834/37184)\n",
      "Loss: 1.351 | Acc: 50.647% (18865/37248)\n",
      "Loss: 1.351 | Acc: 50.662% (18903/37312)\n",
      "Loss: 1.352 | Acc: 50.650% (18931/37376)\n",
      "Loss: 1.351 | Acc: 50.670% (18971/37440)\n",
      "Loss: 1.351 | Acc: 50.664% (19001/37504)\n",
      "Loss: 1.352 | Acc: 50.649% (19028/37568)\n",
      "Loss: 1.352 | Acc: 50.651% (19061/37632)\n",
      "Loss: 1.352 | Acc: 50.658% (19096/37696)\n",
      "Loss: 1.352 | Acc: 50.667% (19132/37760)\n",
      "Loss: 1.352 | Acc: 50.672% (19166/37824)\n",
      "Loss: 1.352 | Acc: 50.662% (19195/37888)\n",
      "Loss: 1.352 | Acc: 50.651% (19223/37952)\n",
      "Loss: 1.352 | Acc: 50.655% (19257/38016)\n",
      "Loss: 1.352 | Acc: 50.649% (19287/38080)\n",
      "Loss: 1.352 | Acc: 50.653% (19321/38144)\n",
      "Loss: 1.352 | Acc: 50.641% (19349/38208)\n",
      "Loss: 1.353 | Acc: 50.645% (19383/38272)\n",
      "Loss: 1.353 | Acc: 50.655% (19419/38336)\n",
      "Loss: 1.352 | Acc: 50.672% (19458/38400)\n",
      "Loss: 1.352 | Acc: 50.694% (19499/38464)\n",
      "Loss: 1.352 | Acc: 50.701% (19534/38528)\n",
      "Loss: 1.352 | Acc: 50.707% (19569/38592)\n",
      "Loss: 1.352 | Acc: 50.711% (19603/38656)\n",
      "Loss: 1.352 | Acc: 50.700% (19631/38720)\n",
      "Loss: 1.352 | Acc: 50.696% (19662/38784)\n",
      "Loss: 1.352 | Acc: 50.705% (19698/38848)\n",
      "Loss: 1.352 | Acc: 50.707% (19731/38912)\n",
      "Loss: 1.352 | Acc: 50.700% (19761/38976)\n",
      "Loss: 1.352 | Acc: 50.694% (19791/39040)\n",
      "Loss: 1.352 | Acc: 50.701% (19826/39104)\n",
      "Loss: 1.352 | Acc: 50.707% (19861/39168)\n",
      "Loss: 1.352 | Acc: 50.714% (19896/39232)\n",
      "Loss: 1.352 | Acc: 50.715% (19929/39296)\n",
      "Loss: 1.352 | Acc: 50.709% (19959/39360)\n",
      "Loss: 1.351 | Acc: 50.720% (19996/39424)\n",
      "Loss: 1.352 | Acc: 50.717% (20027/39488)\n",
      "Loss: 1.352 | Acc: 50.728% (20064/39552)\n",
      "Loss: 1.352 | Acc: 50.735% (20099/39616)\n",
      "Loss: 1.352 | Acc: 50.728% (20129/39680)\n",
      "Loss: 1.352 | Acc: 50.710% (20154/39744)\n",
      "Loss: 1.352 | Acc: 50.708% (20186/39808)\n",
      "Loss: 1.352 | Acc: 50.717% (20222/39872)\n",
      "Loss: 1.352 | Acc: 50.724% (20257/39936)\n",
      "Loss: 1.352 | Acc: 50.715% (20286/40000)\n",
      "Loss: 1.352 | Acc: 50.724% (20322/40064)\n",
      "Loss: 1.352 | Acc: 50.723% (20354/40128)\n",
      "Loss: 1.352 | Acc: 50.722% (20386/40192)\n",
      "Loss: 1.352 | Acc: 50.715% (20416/40256)\n",
      "Loss: 1.352 | Acc: 50.727% (20453/40320)\n",
      "Loss: 1.352 | Acc: 50.738% (20490/40384)\n",
      "Loss: 1.352 | Acc: 50.707% (20510/40448)\n",
      "Loss: 1.352 | Acc: 50.701% (20540/40512)\n",
      "Loss: 1.352 | Acc: 50.715% (20578/40576)\n",
      "Loss: 1.352 | Acc: 50.716% (20611/40640)\n",
      "Loss: 1.352 | Acc: 50.720% (20645/40704)\n",
      "Loss: 1.352 | Acc: 50.719% (20677/40768)\n",
      "Loss: 1.352 | Acc: 50.715% (20708/40832)\n",
      "Loss: 1.352 | Acc: 50.714% (20740/40896)\n",
      "Loss: 1.352 | Acc: 50.723% (20776/40960)\n",
      "Loss: 1.352 | Acc: 50.707% (20802/41024)\n",
      "Loss: 1.352 | Acc: 50.708% (20835/41088)\n",
      "Loss: 1.351 | Acc: 50.710% (20868/41152)\n",
      "Loss: 1.352 | Acc: 50.699% (20896/41216)\n",
      "Loss: 1.352 | Acc: 50.700% (20929/41280)\n",
      "Loss: 1.352 | Acc: 50.699% (20961/41344)\n",
      "Loss: 1.352 | Acc: 50.700% (20994/41408)\n",
      "Loss: 1.352 | Acc: 50.702% (21027/41472)\n",
      "Loss: 1.352 | Acc: 50.698% (21058/41536)\n",
      "Loss: 1.352 | Acc: 50.685% (21085/41600)\n",
      "Loss: 1.351 | Acc: 50.715% (21130/41664)\n",
      "Loss: 1.351 | Acc: 50.714% (21162/41728)\n",
      "Loss: 1.351 | Acc: 50.715% (21195/41792)\n",
      "Loss: 1.351 | Acc: 50.724% (21231/41856)\n",
      "Loss: 1.352 | Acc: 50.720% (21262/41920)\n",
      "Loss: 1.352 | Acc: 50.719% (21294/41984)\n",
      "Loss: 1.352 | Acc: 50.709% (21322/42048)\n",
      "Loss: 1.352 | Acc: 50.698% (21350/42112)\n",
      "Loss: 1.352 | Acc: 50.697% (21382/42176)\n",
      "Loss: 1.352 | Acc: 50.694% (21413/42240)\n",
      "Loss: 1.351 | Acc: 50.704% (21450/42304)\n",
      "Loss: 1.351 | Acc: 50.703% (21482/42368)\n",
      "Loss: 1.351 | Acc: 50.707% (21516/42432)\n",
      "Loss: 1.351 | Acc: 50.715% (21552/42496)\n",
      "Loss: 1.351 | Acc: 50.710% (21582/42560)\n",
      "Loss: 1.351 | Acc: 50.709% (21614/42624)\n",
      "Loss: 1.351 | Acc: 50.712% (21648/42688)\n",
      "Loss: 1.352 | Acc: 50.713% (21681/42752)\n",
      "Loss: 1.352 | Acc: 50.705% (21710/42816)\n",
      "Loss: 1.352 | Acc: 50.700% (21740/42880)\n",
      "Loss: 1.352 | Acc: 50.694% (21770/42944)\n",
      "Loss: 1.352 | Acc: 50.700% (21805/43008)\n",
      "Loss: 1.351 | Acc: 50.706% (21840/43072)\n",
      "Loss: 1.351 | Acc: 50.698% (21869/43136)\n",
      "Loss: 1.352 | Acc: 50.694% (21900/43200)\n",
      "Loss: 1.352 | Acc: 50.691% (21931/43264)\n",
      "Loss: 1.351 | Acc: 50.690% (21963/43328)\n",
      "Loss: 1.351 | Acc: 50.708% (22003/43392)\n",
      "Loss: 1.351 | Acc: 50.720% (22041/43456)\n",
      "Loss: 1.351 | Acc: 50.717% (22072/43520)\n",
      "Loss: 1.351 | Acc: 50.709% (22101/43584)\n",
      "Loss: 1.351 | Acc: 50.696% (22128/43648)\n",
      "Loss: 1.352 | Acc: 50.686% (22156/43712)\n",
      "Loss: 1.352 | Acc: 50.688% (22189/43776)\n",
      "Loss: 1.352 | Acc: 50.677% (22217/43840)\n",
      "Loss: 1.352 | Acc: 50.656% (22240/43904)\n",
      "Loss: 1.352 | Acc: 50.653% (22271/43968)\n",
      "Loss: 1.352 | Acc: 50.661% (22307/44032)\n",
      "Loss: 1.352 | Acc: 50.649% (22334/44096)\n",
      "Loss: 1.351 | Acc: 50.663% (22373/44160)\n",
      "Loss: 1.351 | Acc: 50.660% (22404/44224)\n",
      "Loss: 1.351 | Acc: 50.684% (22447/44288)\n",
      "Loss: 1.351 | Acc: 50.670% (22473/44352)\n",
      "Loss: 1.351 | Acc: 50.673% (22507/44416)\n",
      "Loss: 1.351 | Acc: 50.659% (22533/44480)\n",
      "Loss: 1.351 | Acc: 50.656% (22564/44544)\n",
      "Loss: 1.351 | Acc: 50.648% (22593/44608)\n",
      "Loss: 1.351 | Acc: 50.669% (22635/44672)\n",
      "Loss: 1.351 | Acc: 50.680% (22672/44736)\n",
      "Loss: 1.351 | Acc: 50.676% (22703/44800)\n",
      "Loss: 1.351 | Acc: 50.671% (22733/44864)\n",
      "Loss: 1.351 | Acc: 50.666% (22763/44928)\n",
      "Loss: 1.351 | Acc: 50.660% (22793/44992)\n",
      "Loss: 1.351 | Acc: 50.655% (22823/45056)\n",
      "Loss: 1.351 | Acc: 50.672% (22863/45120)\n",
      "Loss: 1.351 | Acc: 50.671% (22895/45184)\n",
      "Loss: 1.351 | Acc: 50.676% (22930/45248)\n",
      "Loss: 1.351 | Acc: 50.684% (22966/45312)\n",
      "Loss: 1.351 | Acc: 50.683% (22998/45376)\n",
      "Loss: 1.350 | Acc: 50.682% (23030/45440)\n",
      "Loss: 1.350 | Acc: 50.686% (23064/45504)\n",
      "Loss: 1.350 | Acc: 50.680% (23094/45568)\n",
      "Loss: 1.350 | Acc: 50.686% (23129/45632)\n",
      "Loss: 1.350 | Acc: 50.687% (23162/45696)\n",
      "Loss: 1.350 | Acc: 50.699% (23200/45760)\n",
      "Loss: 1.350 | Acc: 50.701% (23233/45824)\n",
      "Loss: 1.350 | Acc: 50.700% (23265/45888)\n",
      "Loss: 1.350 | Acc: 50.699% (23297/45952)\n",
      "Loss: 1.350 | Acc: 50.706% (23333/46016)\n",
      "Loss: 1.350 | Acc: 50.699% (23362/46080)\n",
      "Loss: 1.350 | Acc: 50.700% (23395/46144)\n",
      "Loss: 1.350 | Acc: 50.688% (23422/46208)\n",
      "Loss: 1.350 | Acc: 50.681% (23451/46272)\n",
      "Loss: 1.350 | Acc: 50.680% (23483/46336)\n",
      "Loss: 1.351 | Acc: 50.668% (23510/46400)\n",
      "Loss: 1.350 | Acc: 50.667% (23542/46464)\n",
      "Loss: 1.350 | Acc: 50.675% (23578/46528)\n",
      "Loss: 1.350 | Acc: 50.680% (23613/46592)\n",
      "Loss: 1.350 | Acc: 50.703% (23656/46656)\n",
      "Loss: 1.350 | Acc: 50.715% (23694/46720)\n",
      "Loss: 1.350 | Acc: 50.731% (23734/46784)\n",
      "Loss: 1.349 | Acc: 50.734% (23768/46848)\n",
      "Loss: 1.349 | Acc: 50.744% (23805/46912)\n",
      "Loss: 1.349 | Acc: 50.734% (23833/46976)\n",
      "Loss: 1.349 | Acc: 50.733% (23865/47040)\n",
      "Loss: 1.349 | Acc: 50.741% (23901/47104)\n",
      "Loss: 1.350 | Acc: 50.736% (23931/47168)\n",
      "Loss: 1.349 | Acc: 50.743% (23967/47232)\n",
      "Loss: 1.349 | Acc: 50.753% (24004/47296)\n",
      "Loss: 1.349 | Acc: 50.735% (24028/47360)\n",
      "Loss: 1.350 | Acc: 50.723% (24055/47424)\n",
      "Loss: 1.350 | Acc: 50.712% (24082/47488)\n",
      "Loss: 1.350 | Acc: 50.709% (24113/47552)\n",
      "Loss: 1.350 | Acc: 50.697% (24140/47616)\n",
      "Loss: 1.350 | Acc: 50.688% (24168/47680)\n",
      "Loss: 1.350 | Acc: 50.691% (24202/47744)\n",
      "Loss: 1.350 | Acc: 50.701% (24239/47808)\n",
      "Loss: 1.350 | Acc: 50.700% (24271/47872)\n",
      "Loss: 1.350 | Acc: 50.709% (24308/47936)\n",
      "Loss: 1.350 | Acc: 50.712% (24342/48000)\n",
      "Loss: 1.349 | Acc: 50.714% (24375/48064)\n",
      "Loss: 1.349 | Acc: 50.717% (24409/48128)\n",
      "Loss: 1.349 | Acc: 50.720% (24443/48192)\n",
      "Loss: 1.349 | Acc: 50.723% (24477/48256)\n",
      "Loss: 1.349 | Acc: 50.728% (24512/48320)\n",
      "Loss: 1.350 | Acc: 50.721% (24541/48384)\n",
      "Loss: 1.349 | Acc: 50.727% (24576/48448)\n",
      "Loss: 1.349 | Acc: 50.719% (24605/48512)\n",
      "Loss: 1.349 | Acc: 50.710% (24633/48576)\n",
      "Loss: 1.349 | Acc: 50.718% (24669/48640)\n",
      "Loss: 1.349 | Acc: 50.717% (24701/48704)\n",
      "Loss: 1.349 | Acc: 50.722% (24736/48768)\n",
      "Loss: 1.349 | Acc: 50.715% (24765/48832)\n",
      "Loss: 1.349 | Acc: 50.714% (24797/48896)\n",
      "Loss: 1.349 | Acc: 50.715% (24830/48960)\n",
      "Loss: 1.349 | Acc: 50.720% (24853/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 50.720408163265304\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.245 | Acc: 48.438% (31/64)\n",
      "Loss: 1.211 | Acc: 52.344% (67/128)\n",
      "Loss: 1.298 | Acc: 50.000% (96/192)\n",
      "Loss: 1.365 | Acc: 49.219% (126/256)\n",
      "Loss: 1.354 | Acc: 48.750% (156/320)\n",
      "Loss: 1.392 | Acc: 48.177% (185/384)\n",
      "Loss: 1.410 | Acc: 47.991% (215/448)\n",
      "Loss: 1.397 | Acc: 47.852% (245/512)\n",
      "Loss: 1.379 | Acc: 49.306% (284/576)\n",
      "Loss: 1.358 | Acc: 50.469% (323/640)\n",
      "Loss: 1.378 | Acc: 49.858% (351/704)\n",
      "Loss: 1.362 | Acc: 50.521% (388/768)\n",
      "Loss: 1.363 | Acc: 50.481% (420/832)\n",
      "Loss: 1.358 | Acc: 50.000% (448/896)\n",
      "Loss: 1.348 | Acc: 50.625% (486/960)\n",
      "Loss: 1.344 | Acc: 51.367% (526/1024)\n",
      "Loss: 1.345 | Acc: 51.103% (556/1088)\n",
      "Loss: 1.335 | Acc: 51.128% (589/1152)\n",
      "Loss: 1.328 | Acc: 51.480% (626/1216)\n",
      "Loss: 1.335 | Acc: 50.938% (652/1280)\n",
      "Loss: 1.333 | Acc: 50.967% (685/1344)\n",
      "Loss: 1.338 | Acc: 50.923% (717/1408)\n",
      "Loss: 1.337 | Acc: 50.747% (747/1472)\n",
      "Loss: 1.343 | Acc: 50.846% (781/1536)\n",
      "Loss: 1.344 | Acc: 51.062% (817/1600)\n",
      "Loss: 1.353 | Acc: 50.962% (848/1664)\n",
      "Loss: 1.350 | Acc: 50.984% (881/1728)\n",
      "Loss: 1.353 | Acc: 50.837% (911/1792)\n",
      "Loss: 1.354 | Acc: 50.808% (943/1856)\n",
      "Loss: 1.355 | Acc: 51.042% (980/1920)\n",
      "Loss: 1.354 | Acc: 51.159% (1015/1984)\n",
      "Loss: 1.355 | Acc: 51.074% (1046/2048)\n",
      "Loss: 1.346 | Acc: 51.326% (1084/2112)\n",
      "Loss: 1.351 | Acc: 51.195% (1114/2176)\n",
      "Loss: 1.348 | Acc: 51.473% (1153/2240)\n",
      "Loss: 1.346 | Acc: 51.432% (1185/2304)\n",
      "Loss: 1.345 | Acc: 51.478% (1219/2368)\n",
      "Loss: 1.345 | Acc: 51.480% (1252/2432)\n",
      "Loss: 1.345 | Acc: 51.482% (1285/2496)\n",
      "Loss: 1.349 | Acc: 51.406% (1316/2560)\n",
      "Loss: 1.348 | Acc: 51.220% (1344/2624)\n",
      "Loss: 1.353 | Acc: 51.153% (1375/2688)\n",
      "Loss: 1.349 | Acc: 51.090% (1406/2752)\n",
      "Loss: 1.351 | Acc: 50.959% (1435/2816)\n",
      "Loss: 1.352 | Acc: 51.111% (1472/2880)\n",
      "Loss: 1.350 | Acc: 51.223% (1508/2944)\n",
      "Loss: 1.351 | Acc: 51.164% (1539/3008)\n",
      "Loss: 1.351 | Acc: 51.042% (1568/3072)\n",
      "Loss: 1.351 | Acc: 50.925% (1597/3136)\n",
      "Loss: 1.351 | Acc: 50.906% (1629/3200)\n",
      "Loss: 1.350 | Acc: 50.827% (1659/3264)\n",
      "Loss: 1.350 | Acc: 50.781% (1690/3328)\n",
      "Loss: 1.349 | Acc: 50.737% (1721/3392)\n",
      "Loss: 1.349 | Acc: 50.810% (1756/3456)\n",
      "Loss: 1.347 | Acc: 50.710% (1785/3520)\n",
      "Loss: 1.345 | Acc: 50.893% (1824/3584)\n",
      "Loss: 1.346 | Acc: 50.987% (1860/3648)\n",
      "Loss: 1.344 | Acc: 51.185% (1900/3712)\n",
      "Loss: 1.345 | Acc: 51.165% (1932/3776)\n",
      "Loss: 1.344 | Acc: 51.016% (1959/3840)\n",
      "Loss: 1.345 | Acc: 50.973% (1990/3904)\n",
      "Loss: 1.344 | Acc: 51.008% (2024/3968)\n",
      "Loss: 1.345 | Acc: 50.942% (2054/4032)\n",
      "Loss: 1.347 | Acc: 50.952% (2087/4096)\n",
      "Loss: 1.347 | Acc: 51.058% (2124/4160)\n",
      "Loss: 1.346 | Acc: 51.113% (2159/4224)\n",
      "Loss: 1.346 | Acc: 51.189% (2195/4288)\n",
      "Loss: 1.345 | Acc: 51.195% (2228/4352)\n",
      "Loss: 1.345 | Acc: 51.245% (2263/4416)\n",
      "Loss: 1.344 | Acc: 51.272% (2297/4480)\n",
      "Loss: 1.343 | Acc: 51.166% (2325/4544)\n",
      "Loss: 1.347 | Acc: 50.998% (2350/4608)\n",
      "Loss: 1.347 | Acc: 50.985% (2382/4672)\n",
      "Loss: 1.345 | Acc: 51.014% (2416/4736)\n",
      "Loss: 1.348 | Acc: 51.062% (2451/4800)\n",
      "Loss: 1.347 | Acc: 51.049% (2483/4864)\n",
      "Loss: 1.348 | Acc: 51.075% (2517/4928)\n",
      "Loss: 1.350 | Acc: 50.942% (2543/4992)\n",
      "Loss: 1.350 | Acc: 50.850% (2571/5056)\n",
      "Loss: 1.351 | Acc: 50.840% (2603/5120)\n",
      "Loss: 1.351 | Acc: 50.868% (2637/5184)\n",
      "Loss: 1.351 | Acc: 50.896% (2671/5248)\n",
      "Loss: 1.350 | Acc: 50.941% (2706/5312)\n",
      "Loss: 1.353 | Acc: 50.893% (2736/5376)\n",
      "Loss: 1.353 | Acc: 50.790% (2763/5440)\n",
      "Loss: 1.354 | Acc: 50.799% (2796/5504)\n",
      "Loss: 1.358 | Acc: 50.754% (2826/5568)\n",
      "Loss: 1.358 | Acc: 50.763% (2859/5632)\n",
      "Loss: 1.358 | Acc: 50.737% (2890/5696)\n",
      "Loss: 1.355 | Acc: 50.764% (2924/5760)\n",
      "Loss: 1.355 | Acc: 50.721% (2954/5824)\n",
      "Loss: 1.358 | Acc: 50.645% (2982/5888)\n",
      "Loss: 1.359 | Acc: 50.521% (3007/5952)\n",
      "Loss: 1.359 | Acc: 50.465% (3036/6016)\n",
      "Loss: 1.359 | Acc: 50.461% (3068/6080)\n",
      "Loss: 1.359 | Acc: 50.456% (3100/6144)\n",
      "Loss: 1.361 | Acc: 50.419% (3130/6208)\n",
      "Loss: 1.362 | Acc: 50.367% (3159/6272)\n",
      "Loss: 1.362 | Acc: 50.426% (3195/6336)\n",
      "Loss: 1.361 | Acc: 50.438% (3228/6400)\n",
      "Loss: 1.360 | Acc: 50.495% (3264/6464)\n",
      "Loss: 1.360 | Acc: 50.551% (3300/6528)\n",
      "Loss: 1.362 | Acc: 50.425% (3324/6592)\n",
      "Loss: 1.362 | Acc: 50.466% (3359/6656)\n",
      "Loss: 1.362 | Acc: 50.461% (3391/6720)\n",
      "Loss: 1.361 | Acc: 50.545% (3429/6784)\n",
      "Loss: 1.358 | Acc: 50.672% (3470/6848)\n",
      "Loss: 1.361 | Acc: 50.579% (3496/6912)\n",
      "Loss: 1.362 | Acc: 50.573% (3528/6976)\n",
      "Loss: 1.363 | Acc: 50.497% (3555/7040)\n",
      "Loss: 1.362 | Acc: 50.493% (3587/7104)\n",
      "Loss: 1.364 | Acc: 50.446% (3616/7168)\n",
      "Loss: 1.365 | Acc: 50.401% (3645/7232)\n",
      "Loss: 1.364 | Acc: 50.439% (3680/7296)\n",
      "Loss: 1.362 | Acc: 50.476% (3715/7360)\n",
      "Loss: 1.363 | Acc: 50.418% (3743/7424)\n",
      "Loss: 1.361 | Acc: 50.467% (3779/7488)\n",
      "Loss: 1.360 | Acc: 50.371% (3804/7552)\n",
      "Loss: 1.362 | Acc: 50.249% (3827/7616)\n",
      "Loss: 1.362 | Acc: 50.260% (3860/7680)\n",
      "Loss: 1.360 | Acc: 50.336% (3898/7744)\n",
      "Loss: 1.360 | Acc: 50.410% (3936/7808)\n",
      "Loss: 1.360 | Acc: 50.394% (3967/7872)\n",
      "Loss: 1.360 | Acc: 50.328% (3994/7936)\n",
      "Loss: 1.362 | Acc: 50.288% (4023/8000)\n",
      "Loss: 1.361 | Acc: 50.360% (4061/8064)\n",
      "Loss: 1.361 | Acc: 50.308% (4089/8128)\n",
      "Loss: 1.360 | Acc: 50.317% (4122/8192)\n",
      "Loss: 1.361 | Acc: 50.315% (4154/8256)\n",
      "Loss: 1.362 | Acc: 50.264% (4182/8320)\n",
      "Loss: 1.362 | Acc: 50.250% (4213/8384)\n",
      "Loss: 1.361 | Acc: 50.331% (4252/8448)\n",
      "Loss: 1.364 | Acc: 50.305% (4282/8512)\n",
      "Loss: 1.364 | Acc: 50.292% (4313/8576)\n",
      "Loss: 1.365 | Acc: 50.312% (4347/8640)\n",
      "Loss: 1.365 | Acc: 50.207% (4370/8704)\n",
      "Loss: 1.365 | Acc: 50.217% (4403/8768)\n",
      "Loss: 1.365 | Acc: 50.249% (4438/8832)\n",
      "Loss: 1.364 | Acc: 50.304% (4475/8896)\n",
      "Loss: 1.365 | Acc: 50.301% (4507/8960)\n",
      "Loss: 1.364 | Acc: 50.344% (4543/9024)\n",
      "Loss: 1.364 | Acc: 50.374% (4578/9088)\n",
      "Loss: 1.365 | Acc: 50.295% (4603/9152)\n",
      "Loss: 1.363 | Acc: 50.347% (4640/9216)\n",
      "Loss: 1.362 | Acc: 50.366% (4674/9280)\n",
      "Loss: 1.362 | Acc: 50.375% (4707/9344)\n",
      "Loss: 1.363 | Acc: 50.361% (4738/9408)\n",
      "Loss: 1.364 | Acc: 50.327% (4767/9472)\n",
      "Loss: 1.364 | Acc: 50.378% (4804/9536)\n",
      "Loss: 1.363 | Acc: 50.417% (4840/9600)\n",
      "Loss: 1.363 | Acc: 50.414% (4872/9664)\n",
      "Loss: 1.364 | Acc: 50.370% (4900/9728)\n",
      "Loss: 1.364 | Acc: 50.398% (4935/9792)\n",
      "Loss: 1.364 | Acc: 50.365% (4964/9856)\n",
      "Loss: 1.364 | Acc: 50.323% (4992/9920)\n",
      "Loss: 1.366 | Acc: 50.260% (5018/9984)\n",
      "Loss: 1.365 | Acc: 50.270% (5027/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 50.27\n",
      "\n",
      "Final train set accuracy is 50.720408163265304\n",
      "Final test set accuracy is 50.27\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims=hidden_dims, input_dims=input_dims, output_dims=output_dims, num_trans_layers = num_trans_layers, num_heads=num_heads, image_k=image_k, patch_k=patch_k)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec757796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp411_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e5eda89d23070afdb7722fb9f668633be4586ec4912bbf9b234825cc100d6be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
